{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2367eb",
   "metadata": {},
   "source": [
    "# [Setup]\n",
    "Block 1 (light preprocessing)\n",
    "Block 2 (gazetteer + spacy/stanza)\n",
    "Block 3 (NER functions)\n",
    "Block 7 (WordNet vague terms)\n",
    "Block 8 (motion/transport terms)\n",
    "Block 9 (sentence scoring prep)\n",
    "\n",
    "# [Pipeline]\n",
    "Block 1 (continue with PDF preprocessing)\n",
    "Block 4 (load cleaned sentences)\n",
    "Block 5 (NER + ML filtering)\n",
    "Block 6 (boosting small towns)\n",
    "Block 10 (GeoNames + Wikidata)\n",
    "Block 11 (Wikidata enrichment)\n",
    "Block 12 (clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfdb186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Pre-Block: Downloads & Setup ===\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"framenet_v17\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56ccd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 12:16:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf868182710d4b4a8489e8512067098a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 12:16:17 INFO: Downloaded file to /Users/alicja/stanza_resources/resources.json\n",
      "2025-08-01 12:16:17 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-08-01 12:16:18 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-08-01 12:16:18 WARNING: GPU requested, but is not available!\n",
      "2025-08-01 12:16:18 INFO: Using device: cpu\n",
      "2025-08-01 12:16:18 INFO: Loading: tokenize\n",
      "2025-08-01 12:16:18 INFO: Loading: mwt\n",
      "2025-08-01 12:16:18 INFO: Loading: ner\n",
      "2025-08-01 12:16:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# === Block 1: Imports ===\n",
    "\n",
    "# Standard\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import yaml\n",
    "import json\n",
    "# Third-party\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "from fuzzywuzzy import fuzz\n",
    "import contractions\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords, wordnet as wn, framenet as fn\n",
    "\n",
    "# Helpers\n",
    "from nlp_helpers import (\n",
    "    init_nlp,\n",
    "    get_stopwords,\n",
    "    tag_named_entities,\n",
    "    extract_text_from_pdf,\n",
    "    load_config,\n",
    "    normalize_punctuation,  \n",
    "    clean_light,\n",
    "    preprocess_text,\n",
    "    segment_sentences,\n",
    "    clean_heavy\n",
    ")\n",
    "from gazetteer_helpers import build_gazetteer\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "  # Preview output\n",
    "from IPython.display import display\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# === NLP Initialization ===\n",
    "nlp, stanza_pipeline = init_nlp()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d76d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 2: Extract text from PDF using PyMuPDF.  pages 28-148\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str, start_page: int, end_page: int) -> str:\n",
    "    \"\"\"Extracts and returns text from a PDF given a path and page range.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = doc[start_page:end_page]\n",
    "    return \"\\n\".join(page.get_text() for page in pages)\n",
    "\n",
    "\n",
    "\n",
    "#Load configuration from YAML file\n",
    "def load_config(config_path: str = \"config.yaml\") -> dict:\n",
    "    \"\"\"Loads YAML configuration file and returns it as a dictionary.\"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        \n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "raw_text = extract_text_from_pdf(pdf_conf[\"path\"], pdf_conf[\"start_page\"], pdf_conf[\"end_page\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e33e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 3: Text Preprocessing Functions ===\n",
    "\n",
    "from nlp_helpers import (\n",
    "    init_nlp,\n",
    "    get_stopwords,\n",
    "    tag_named_entities,\n",
    "    extract_text_from_pdf,\n",
    "    load_config,\n",
    "    normalize_punctuation,\n",
    "    clean_light,\n",
    "    preprocess_text,\n",
    "    segment_sentences,\n",
    "    clean_heavy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2737feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned outputs saved:\n",
      "- Geoparsing (light): outputs/cleaned_MotorcycleDiaries_geoparsing.txt\n",
      "- NLP prep (heavy): outputs/cleaned_MotorcycleDiaries_nlp.txt\n",
      "‚úÖ sentence_data prepared with 2407 narrative sentences.\n"
     ]
    }
   ],
   "source": [
    "# === Block 4: Clean and Save Tagged + NLP Versions ===\n",
    "\n",
    "# Load config\n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "gaz_conf = config[\"gazetteer\"]\n",
    "\n",
    "# Output filenames based on input PDF\n",
    "base_name = Path(pdf_conf[\"path\"]).stem\n",
    "tagged_path = Path(\"outputs\") / f\"cleaned_{base_name}_geoparsing.txt\"\n",
    "heavy_path = Path(\"outputs\") / f\"cleaned_{base_name}_nlp.txt\"\n",
    "\n",
    "# Create outputs dir if needed\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# üßπ Clean raw text (light and heavy)\n",
    "light_cleaned = clean_light(raw_text)\n",
    "sentences = segment_sentences(light_cleaned, nlp)\n",
    "sentences_with_tags = [f\"[SENT {i+1}] {s}\" for i, s in enumerate(sentences)]\n",
    "\n",
    "all_stops = get_stopwords(nlp)\n",
    "heavy_cleaned = clean_heavy(light_cleaned, nlp, all_stops)\n",
    "\n",
    "# üíæ Save both cleaned versions\n",
    "with open(tagged_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sentences_with_tags))\n",
    "\n",
    "with open(heavy_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(heavy_cleaned)\n",
    "\n",
    "print(\"‚úÖ Cleaned outputs saved:\")\n",
    "print(f\"- Geoparsing (light): {tagged_path}\")\n",
    "print(f\"- NLP prep (heavy): {heavy_path}\")\n",
    "\n",
    "# ‚úÖ Prepare sentence_data for Block 7\n",
    "sentence_data = [(i, sent.strip()) for i, sent in enumerate(sentences)]\n",
    "print(f\"‚úÖ sentence_data prepared with {len(sentence_data)} narrative sentences.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8272556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loaded 159 symbolic verb forms from WordNet.\n",
      "üõ£Ô∏è Loaded 205 movement verb forms from WordNet.\n"
     ]
    }
   ],
   "source": [
    "# === Build symbolic verb lexicon from WordNet (only once)\n",
    "\n",
    "\n",
    "def get_symbolic_verb_synonyms():\n",
    "    base_words = [\n",
    "        \"dream\", \"hope\", \"struggle\", \"escape\", \"resist\", \"believe\", \"follow\", \n",
    "        \"ride\", \"rebel\", \"fight\", \"flee\", \"live\", \"return\"\n",
    "    ]\n",
    "    synonyms = set()\n",
    "    for word in base_words:\n",
    "        for syn in wordnet.synsets(word, pos=wordnet.VERB):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "    return synonyms\n",
    "\n",
    "# Save the expanded verb set to reuse\n",
    "SYMBOLIC_VERBS = get_symbolic_verb_synonyms()\n",
    "print(f\"üß† Loaded {len(SYMBOLIC_VERBS)} symbolic verb forms from WordNet.\")\n",
    "\n",
    "# === Build movement verb list using WordNet\n",
    "\n",
    "\n",
    "def get_movement_verbs():\n",
    "    base = [\"go\", \"move\", \"travel\", \"walk\", \"drive\", \"ride\", \"arrive\", \"depart\", \"leave\", \"return\", \"cross\", \"fly\", \"sail\"]\n",
    "    move_verbs = set()\n",
    "    for word in base:\n",
    "        for syn in wordnet.synsets(word, pos=wordnet.VERB):\n",
    "            for lemma in syn.lemmas():\n",
    "                move_verbs.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "    return move_verbs\n",
    "\n",
    "MOVEMENT_VERBS = get_movement_verbs()\n",
    "print(f\"üõ£Ô∏è Loaded {len(MOVEMENT_VERBS)} movement verb forms from WordNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41623c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found existing gazetteer file, loading...\n"
     ]
    }
   ],
   "source": [
    "# === Block 6: Build Gazetteer ===\n",
    "\n",
    "\n",
    "# üîê JSON safety patch\n",
    "def make_json_safe(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    raise TypeError(f\"‚ùå Not JSON serializable: {type(obj)}\")\n",
    "\n",
    "# üì• Load config.yaml\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "gaz_conf = config[\"gazetteer\"]\n",
    "\n",
    "# üìÅ Gazetteer path\n",
    "gazetteer_path = Path(\"outputs/gazetteer_cities.json\")\n",
    "\n",
    "# ‚öôÔ∏è Build or load gazetteer\n",
    "if gazetteer_path.exists():\n",
    "    print(\"üìÇ Found existing gazetteer file, loading...\")\n",
    "    with open(gazetteer_path, \"r\") as f:\n",
    "        gazetteer = json.load(f)\n",
    "else:\n",
    "    print(\"üåç No gazetteer file found, building...\")\n",
    "    gazetteer = build_gazetteer(\n",
    "        username=gaz_conf[\"username\"],\n",
    "        countries=gaz_conf[\"countries\"],\n",
    "        max_rows=gaz_conf[\"max_rows\"]\n",
    "    )\n",
    "    with open(gazetteer_path, \"w\") as f:\n",
    "        json.dump(gazetteer, f, indent=2, default=make_json_safe)  # üîê here\n",
    "    print(\"‚úÖ Gazetteer built and saved with coordinates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d70be0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Running ensemble NER + gazetteer matching (with metonymy awareness)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER + Gazetteer:   0%|          | 2/2407 [00:02<52:53,  1.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# === Run full pipeline ===\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîé Running ensemble NER + gazetteer matching (with metonymy awareness)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m entities_combined \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_ner_gazetteer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgazetteer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(entities_combined)\n\u001b[1;32m    151\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m remove_overlapping_shorter(df_combined)\n",
      "Cell \u001b[0;32mIn[32], line 81\u001b[0m, in \u001b[0;36mcombine_ner_gazetteer\u001b[0;34m(sentences, gazetteer)\u001b[0m\n\u001b[1;32m     79\u001b[0m ents_spacy \u001b[38;5;241m=\u001b[39m extract_entities_spacy(text)\n\u001b[1;32m     80\u001b[0m ents_stanza \u001b[38;5;241m=\u001b[39m extract_entities_stanza(text) \u001b[38;5;28;01mif\u001b[39;00m USE_STANZA \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m---> 81\u001b[0m ents_gazetteer \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_gazetteer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgazetteer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m all_ents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent_text, label, start, end \u001b[38;5;129;01min\u001b[39;00m ents_spacy \u001b[38;5;241m+\u001b[39m ents_stanza \u001b[38;5;241m+\u001b[39m ents_gazetteer:\n",
      "Cell \u001b[0;32mIn[32], line 28\u001b[0m, in \u001b[0;36mmatch_gazetteer\u001b[0;34m(text, known_places)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m place \u001b[38;5;129;01min\u001b[39;00m known_places_sorted:\n\u001b[1;32m     27\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(re\u001b[38;5;241m.\u001b[39mescape(place\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_lower\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     29\u001b[0m         start, end \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mstart(), m\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m     30\u001b[0m         match_text \u001b[38;5;241m=\u001b[39m text[start:end]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.,;:!?()[]\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m0123456789 \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/re.py:248\u001b[0m, in \u001b[0;36mfinditer\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfinditer\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over all non-overlapping matches in the\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    string.  For each match, the iterator returns a Match object.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfinditer(string)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/sre_compile.py:804\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, i \u001b[38;5;129;01min\u001b[39;00m groupindex\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    802\u001b[0m     indexgroup[i] \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m--> 804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroupindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindexgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Block 7: NER + Gazetteer with Metonymy Filtering (Thesis-Ready) ===\n",
    "# üìö Metonymy-aware NER inspired by Gritta et al. (2018)\n",
    "# GitHub: https://github.com/milangritta/WhatsMissingInGeoparsing\n",
    "\n",
    "USE_STANZA = True  # Toggle Stanza NER support\n",
    "\n",
    "stop_words = get_stopwords(nlp)\n",
    "\n",
    "def extract_entities_spacy(text: str) -> List[Tuple[str, str, int, int]]:\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "def extract_entities_stanza(text: str) -> List[Tuple[str, str, int, int]]:\n",
    "    doc = stanza_pipeline(text)\n",
    "    results = []\n",
    "    for sent in doc.sentences:\n",
    "        for ent in sent.ents:\n",
    "            results.append((ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "    return results\n",
    "\n",
    "def match_gazetteer(text: str, known_places: set[str]) -> List[Tuple[str, str, int, int]]:\n",
    "    text_lower = text.lower()\n",
    "    matches = []\n",
    "    known_places_sorted = sorted([p for p in known_places if len(p) > 3 and p not in stop_words], key=len, reverse=True)\n",
    "\n",
    "    for place in known_places_sorted:\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(place.lower()))\n",
    "        for m in re.finditer(pattern, text_lower):\n",
    "            start, end = m.start(), m.end()\n",
    "            match_text = text[start:end].strip(\".,;:!?()[]{}0123456789 \")\n",
    "            if match_text:\n",
    "                matches.append((match_text, \"GAZETTEER\", start, end))\n",
    "    return matches\n",
    "\n",
    "# === Enhanced Metonymy Detection Function ===\n",
    "def is_probable_metonymy(entity_text: str, sentence: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect whether a location entity is used metonymically (non-literal).\n",
    "    - Looks for cue words near the entity in the sentence\n",
    "    - Checks that cue is a noun (e.g. 'government', 'industry')\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    entity_tokens = [t for t in doc if entity_text.lower() in t.text.lower()]\n",
    "    cue_words = {\n",
    "        \"government\", \"policy\", \"military\", \"regime\", \"parliament\", \"industry\",\n",
    "        \"media\", \"revolution\", \"capital\", \"press\", \"organization\", \"power\"\n",
    "    }\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text.lower() in cue_words and token.pos_ == \"NOUN\":\n",
    "            # Check distance to entity\n",
    "            for ent_token in entity_tokens:\n",
    "                if abs(token.i - ent_token.i) <= 10:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "# === Main Pipeline ===\n",
    "def combine_ner_gazetteer(\n",
    "    sentences: List[Tuple[int, str]], \n",
    "    gazetteer: set[str]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combines entities from spaCy, optional Stanza, and gazetteer pattern matching.\n",
    "    Filters to location-type entities only: GPE, LOC, and GAZETTEER.\n",
    "    Applies metonymy-aware filtering. Stores all useful metadata per entity.\n",
    "    \"\"\"\n",
    "    allowed_labels = {\"GPE\", \"LOC\", \"GAZETTEER\"}\n",
    "    results = []\n",
    "    metonymy_filtered = []\n",
    "\n",
    "    # Pre-tag persons per sentence\n",
    "    persons_by_sentence = {\n",
    "        sid: [ent.text for ent in nlp(text).ents if ent.label_ == \"PERSON\"]\n",
    "        for sid, text in sentences\n",
    "    }\n",
    "\n",
    "    for sent_id, text in tqdm(sentences, desc=\"NER + Gazetteer\"):\n",
    "        try:\n",
    "            ents_spacy = extract_entities_spacy(text)\n",
    "            ents_stanza = extract_entities_stanza(text) if USE_STANZA else []\n",
    "            ents_gazetteer = match_gazetteer(text, gazetteer)\n",
    "\n",
    "            all_ents = []\n",
    "            for ent_text, label, start, end in ents_spacy + ents_stanza + ents_gazetteer:\n",
    "                if label in allowed_labels:\n",
    "                    if not is_probable_metonymy(ent_text, text):\n",
    "                        all_ents.append((ent_text, label, start, end))\n",
    "                    else:\n",
    "                        metonymy_filtered.append({\n",
    "                            \"sentence_id\": sent_id,\n",
    "                            \"entity\": ent_text,\n",
    "                            \"label\": label,\n",
    "                            \"sentence\": text\n",
    "                        })\n",
    "\n",
    "            # Deduplicate person list\n",
    "            raw_persons = persons_by_sentence.get(sent_id, [])\n",
    "            cleaned_persons = []\n",
    "            seen_persons = set()\n",
    "            for p in raw_persons:\n",
    "                p_clean = re.sub(r\"\\d+$\", \"\", p).strip()\n",
    "                norm = p_clean.lower()\n",
    "                if not any(fuzz.ratio(norm, s) > 90 for s in seen_persons):\n",
    "                    seen_persons.add(norm)\n",
    "                    cleaned_persons.append(p_clean)\n",
    "\n",
    "            # Store results\n",
    "            seen = set()\n",
    "            for ent_text, label, start, end in all_ents:\n",
    "                norm = ent_text.lower()\n",
    "                if not any(fuzz.ratio(norm, s) > 90 for s in seen):\n",
    "                    seen.add(norm)\n",
    "                    results.append({\n",
    "                        \"sentence_id\": sent_id,\n",
    "                        \"entity\": ent_text,\n",
    "                        \"entity_norm\": norm,\n",
    "                        \"label\": label,\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end,\n",
    "                        \"sentence\": text,\n",
    "                        \"persons_in_sentence\": cleaned_persons\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in sentence {sent_id}: {e}\")\n",
    "\n",
    "    # Save metonymy-flagged entities for analysis\n",
    "    if metonymy_filtered:\n",
    "        pd.DataFrame(metonymy_filtered).to_csv(\"outputs/metonymy_filtered.csv\", index=False)\n",
    "        print(\"üì§ Logged metonymy-filtered entities to: outputs/metonymy_filtered.csv\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def remove_overlapping_shorter(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean = []\n",
    "    for sid in df[\"sentence_id\"].unique():\n",
    "        sent_df = df[df[\"sentence_id\"] == sid].sort_values(\"start_char\")\n",
    "        to_keep = []\n",
    "        last_end = -1\n",
    "        for _, row in sent_df.iterrows():\n",
    "            if row[\"start_char\"] >= last_end:\n",
    "                to_keep.append(row)\n",
    "                last_end = row[\"end_char\"]\n",
    "        clean.append(pd.DataFrame(to_keep))\n",
    "    return pd.concat(clean, ignore_index=True)\n",
    "\n",
    "# === Run full pipeline ===\n",
    "print(\"üîé Running ensemble NER + gazetteer matching (with metonymy awareness)...\")\n",
    "entities_combined = combine_ner_gazetteer(sentence_data, gazetteer)\n",
    "\n",
    "df_combined = pd.DataFrame(entities_combined)\n",
    "df_combined = remove_overlapping_shorter(df_combined)\n",
    "\n",
    "df_combined.to_csv(\"outputs/geoparsing_ner_ensemble.csv\", index=False)\n",
    "print(\"‚úÖ Saved: outputs/geoparsing_ner_ensemble.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8692b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing on first 100 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER + Gazetteer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:00<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Logged metonymy-filtered entities to: outputs/metonymy_filtered.csv\n",
      "‚úÖ Test results saved to: outputs/geoparsing_ner_sample_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === TEST BLOCK: Sample NER + Gazetteer Run ===\n",
    "\n",
    "def test_ner_pipeline(n: int = 100):\n",
    "    \"\"\"\n",
    "    Runs NER + gazetteer matching on a sample of n sentences.\n",
    "    \"\"\"\n",
    "    sample_sentences = sentence_data[:n]\n",
    "    print(f\"üß™ Testing on first {n} sentences...\")\n",
    "\n",
    "    results = combine_ner_gazetteer(sample_sentences, gazetteer)\n",
    "    df_test = pd.DataFrame(results)\n",
    "\n",
    "    output_path = Path(\"outputs\") / \"geoparsing_ner_sample_test.csv\"\n",
    "    df_test.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Test results saved to: {output_path}\")\n",
    "\n",
    "# ‚úÖ Run this\n",
    "test_ner_pipeline(n=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f112e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final enriched symbolic data saved to: outputs/geoparsing_final_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "#actual blok 8 rule-based filtering \n",
    "\n",
    "# Filter only relevant, valid entities using rule-based filtering\n",
    "# === Block 8: Manual Gazetteer Enrichment ===\n",
    "# ‚úÖ Rule-Based Filtering of Enriched Gazetteer Output\n",
    "\n",
    "\n",
    "## === Block 8: Rule-Based Filtering + Symbolic Enrichment (Final Thesis-Ready) ===\n",
    "\n",
    "# Load gazetteer\n",
    "with open(\"outputs/gazetteer_cities.json\", \"r\") as f:\n",
    "    gazetteer = json.load(f)\n",
    "gazetteer_set = set(gazetteer.keys())\n",
    "\n",
    "# Load Block 7 output\n",
    "df_combined = pd.read_csv(\"outputs/geoparsing_ner_sample_test.csv\")\n",
    "df_combined[\"persons_in_sentence\"] = df_combined[\"persons_in_sentence\"].apply(eval)\n",
    "\n",
    "# === Gazetteer info ===\n",
    "def get_lat(entity): return gazetteer.get(entity.lower(), {}).get(\"lat\")\n",
    "def get_lon(entity): return gazetteer.get(entity.lower(), {}).get(\"lon\")\n",
    "def country_valid(entity): return entity.lower() in gazetteer_set\n",
    "def symbolic_flagged(entity, persons):\n",
    "    return any(fuzz.token_set_ratio(entity.lower(), p.lower()) > 85 for p in persons)\n",
    "\n",
    "# === Label Fix (safe relabel)\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "def relabel_as_person(row):\n",
    "    if row[\"label\"] not in {\"GPE\", \"LOC\", \"GAZETTEER\"}:\n",
    "        return row[\"label\"]\n",
    "    ent = normalize(row[\"entity\"])\n",
    "    if ent.lower() in gazetteer_set:\n",
    "        return row[\"label\"]\n",
    "    for p in row[\"persons_in_sentence\"]:\n",
    "        if fuzz.token_set_ratio(ent, normalize(p)) >= 85:\n",
    "            return \"PERSON\"\n",
    "    return row[\"label\"]\n",
    "\n",
    "\n",
    "def movement_verb_present(row):\n",
    "    doc = nlp(row[\"sentence\"])\n",
    "    ent = row[\"entity\"].lower()\n",
    "    persons = [p.lower() for p in row[\"persons_in_sentence\"]]\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in MOVEMENT_VERBS:\n",
    "            context = [token] + list(token.children) + [token.head]\n",
    "            for t in context:\n",
    "                t_text = t.text.lower()\n",
    "                if ent in t_text or any(p in t_text for p in persons):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "# === Symbolic Context via WordNet-enhanced Verbs\n",
    "def advanced_symbolic_context(row):\n",
    "    doc = nlp(row[\"sentence\"])\n",
    "    ent_text = row[\"entity\"].lower()\n",
    "    persons = [p.lower() for p in row[\"persons_in_sentence\"]]\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in SYMBOLIC_VERBS:\n",
    "            related_tokens = [token] + list(token.children) + [token.head]\n",
    "            for t in related_tokens:\n",
    "                t_lower = t.text.lower()\n",
    "                if ent_text in t_lower:\n",
    "                    return True\n",
    "                for p in persons:\n",
    "                    if p in t_lower:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "# === Metonymy filter\n",
    "def is_probable_metonymy(entity_text: str, sentence: str) -> bool:\n",
    "    cue_words = {\n",
    "        \"government\", \"policy\", \"military\", \"regime\", \"parliament\", \"industry\",\n",
    "        \"media\", \"revolution\", \"capital\", \"press\", \"organization\", \"power\"\n",
    "    }\n",
    "    doc = nlp(sentence)\n",
    "    entity_tokens = [t for t in doc if entity_text.lower() in t.text.lower()]\n",
    "    for token in doc:\n",
    "        if token.text.lower() in cue_words and token.pos_ == \"NOUN\":\n",
    "            for ent_token in entity_tokens:\n",
    "                if abs(token.i - ent_token.i) <= 10:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "# === Apply enrichments\n",
    "df_combined[\"lat\"] = df_combined[\"entity\"].apply(get_lat)\n",
    "df_combined[\"lon\"] = df_combined[\"entity\"].apply(get_lon)\n",
    "df_combined[\"country_valid\"] = df_combined[\"entity\"].apply(country_valid)\n",
    "df_combined[\"symbolic_flagged\"] = df_combined.apply(\n",
    "    lambda row: symbolic_flagged(row[\"entity\"], row[\"persons_in_sentence\"]), axis=1\n",
    ")\n",
    "df_combined[\"label\"] = df_combined.apply(relabel_as_person, axis=1)\n",
    "df_combined[\"symbolic_context\"] = df_combined.apply(advanced_symbolic_context, axis=1)\n",
    "df_combined[\"metonymy_flagged\"] = df_combined.apply(\n",
    "    lambda row: is_probable_metonymy(row[\"entity\"], row[\"sentence\"]), axis=1\n",
    ")\n",
    "df_combined[\"movement_verb_present\"] = df_combined.apply(movement_verb_present, axis=1)\n",
    "\n",
    "# === Regional Consistency Tracking (3-sentence memory)\n",
    "\n",
    "def is_regionally_consistent(current_row, history, max_distance_km=500):\n",
    "    if not pd.notnull(current_row[\"lat\"]) or not pd.notnull(current_row[\"lon\"]):\n",
    "        return False\n",
    "    current_coords = (current_row[\"lat\"], current_row[\"lon\"])\n",
    "\n",
    "    for past in history:\n",
    "        if not pd.notnull(past[\"lat\"]) or not pd.notnull(past[\"lon\"]):\n",
    "            continue\n",
    "        past_coords = (past[\"lat\"], past[\"lon\"])\n",
    "        try:\n",
    "            if geodesic(current_coords, past_coords).km <= max_distance_km:\n",
    "                return True\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "\n",
    "region_history = []\n",
    "consistency_flags = []\n",
    "\n",
    "for _, row in df_combined.iterrows():\n",
    "    flag = is_regionally_consistent(row, region_history)\n",
    "    consistency_flags.append(flag)\n",
    "    if row[\"label\"] in {\"GPE\", \"LOC\", \"GAZETTEER\"} and row[\"country_valid\"]:\n",
    "        region_history.append(row)\n",
    "        if len(region_history) > 3:\n",
    "            region_history.pop(0)\n",
    "\n",
    "df_combined[\"regional_consistency_flag\"] = consistency_flags\n",
    "\n",
    "# === Final symbolic score\n",
    "df_combined[\"symbolic_score\"] = (\n",
    "    df_combined[\"symbolic_flagged\"].astype(int)\n",
    "    + df_combined[\"symbolic_context\"].astype(int)\n",
    "    + df_combined[\"movement_verb_present\"].astype(int)\n",
    "    + df_combined[\"regional_consistency_flag\"].astype(int)\n",
    "    - df_combined[\"metonymy_flagged\"].astype(int)\n",
    ")\n",
    "\n",
    "df_combined[\"symbolic_confidence\"] = df_combined[\"symbolic_score\"].apply(\n",
    "    lambda x: \"high\" if x >= 2 else (\"low\" if x > 0 else \"none\")\n",
    ")\n",
    "\n",
    "# === Final label (for ML)\n",
    "def final_label(row):\n",
    "    if row[\"symbolic_score\"] >= 1:\n",
    "        return \"SYMBOLIC\"\n",
    "    elif row[\"label\"] in {\"GPE\", \"LOC\", \"GAZETTEER\"} and row[\"country_valid\"]:\n",
    "        return \"LITERAL\"\n",
    "    else:\n",
    "        return \"NOISE\"\n",
    "\n",
    "df_combined[\"final_label\"] = df_combined.apply(final_label, axis=1)\n",
    "\n",
    "# === Save result\n",
    "df_combined.to_csv(\"outputs/geoparsing_final_enriched.csv\", index=False)\n",
    "print(\"‚úÖ Final enriched symbolic data saved to: outputs/geoparsing_final_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       198\n",
      "           1       1.00      1.00      1.00        35\n",
      "\n",
      "    accuracy                           1.00       233\n",
      "   macro avg       1.00      1.00      1.00       233\n",
      "weighted avg       1.00      1.00      1.00       233\n",
      "\n",
      " ML filtering complete\n"
     ]
    }
   ],
   "source": [
    "## !!! Ignore this for now \n",
    "# Block x: Train ML model to filter real geographic entities\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load ensemble NER output\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble.csv\")\n",
    "\n",
    "# Create features\n",
    "df[\"label_GPE\"] = (df[\"label\"] == \"GPE\").astype(int)\n",
    "df[\"label_LOC\"] = (df[\"label\"] == \"LOC\").astype(int)\n",
    "\n",
    "symbolic_keywords = [\"freedom\", \"struggle\", \"liberation\", \"future\", \"dream\", \"cause\", \"revolution\", \"hope\", \"people\"]\n",
    "df[\"symbolic_flagged\"] = df[\"sentence\"].str.contains(\"|\".join(symbolic_keywords), flags=re.IGNORECASE, na=False)\n",
    "\n",
    "expected_countries = [\"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\", \"venezuela\"]\n",
    "df[\"country_valid\"] = df[\"sentence\"].str.lower().apply(\n",
    "    lambda x: int(any(country in x for country in expected_countries))\n",
    ")\n",
    "\n",
    "df[\"fuzzy_score\"] = df.apply(\n",
    "    lambda row: fuzz.ratio(str(row[\"entity\"]).lower(), str(row[\"entity_norm\"]).lower()), axis=1\n",
    ")\n",
    "df[\"fuzzy_score_scaled\"] = df[\"fuzzy_score\"] / 100.0\n",
    "\n",
    "df[\"auto_label\"] = ((df[\"country_valid\"] == 1) & (~df[\"symbolic_flagged\"])).astype(int)\n",
    "\n",
    "# Train/test split\n",
    "features = df[[\"label_GPE\", \"label_LOC\", \"symbolic_flagged\", \"country_valid\", \"fuzzy_score_scaled\"]]\n",
    "target = df[\"auto_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, stratify=target, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(\" Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict on all\n",
    "X_all_scaled = scaler.transform(features)\n",
    "df[\"geo_confidence\"] = clf.predict_proba(X_all_scaled)[:, 1]\n",
    "df[\"filtered_out_ml\"] = df[\"geo_confidence\"] < 0.5\n",
    "\n",
    "# Save outputs\n",
    "df_filtered = df[~df[\"filtered_out_ml\"]].copy()\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_flagged_with_ml.csv\", index=False)\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\", index=False)\n",
    "\n",
    "print(\" ML filtering complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Could not load enriched data: \"['country'] not in index\"\n",
      "Saved: geoparsing_ensemble_final_ml_boosted.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 9: Boost confidence for small towns in South America\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\")\n",
    "\n",
    "# Define target countries\n",
    "target_countries = [\n",
    "    \"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\",\n",
    "    \"venezuela\", \"ecuador\", \"brazil\", \"uruguay\", \"paraguay\"\n",
    "]\n",
    "\n",
    "# Prepare for matching\n",
    "df[\"entity_norm_lower\"] = df[\"entity_norm\"].str.lower()\n",
    "\n",
    "# Try to load population-enriched data\n",
    "try:\n",
    "    enriched = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "    enriched[\"entity_norm_lower\"] = enriched[\"entity_norm\"].str.lower()\n",
    "\n",
    "    df = df.merge(\n",
    "        enriched[[\"entity_norm_lower\", \"country\", \"population\"]],\n",
    "        on=\"entity_norm_lower\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Mark small towns\n",
    "    df[\"boost_small_town\"] = (\n",
    "        df[\"population\"].fillna(0).lt(50000) &\n",
    "        df[\"country\"].str.lower().isin(target_countries)\n",
    "    )\n",
    "    print(f\" Boosted {df['boost_small_town'].sum()} small towns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Could not load enriched data: {e}\")\n",
    "    df[\"boost_small_town\"] = False\n",
    "\n",
    "# Apply confidence boost\n",
    "df[\"geo_confidence_boosted\"] = df[\"geo_confidence\"]\n",
    "df.loc[df[\"boost_small_town\"], \"geo_confidence_boosted\"] = 0.9\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\", index=False)\n",
    "print(\"Saved: geoparsing_ensemble_final_ml_boosted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b88ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ML-learned vague terms: 0\n",
      " WordNet vague terms: 1786\n",
      " Removed 0 entities (ML or WordNet flagged)\n",
      " 'country' column not found. Cannot apply region filter.\n",
      " Saved: geoparsing_ner_ensemble_filtered_southamerica.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Block 10: vague term filtering (ML + WordNet) + South America region restriction\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\")\n",
    "\n",
    "#  Normalize entity name \n",
    "df[\"entity_norm\"] = df[\"entity_norm\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# === STEP 1: ML-learned vague terms ===\n",
    "term_stats = df.groupby(\"entity_norm\").agg({\n",
    "    \"geo_confidence_boosted\": \"mean\",\n",
    "    \"entity\": \"count\"\n",
    "}).rename(columns={\n",
    "    \"entity\": \"freq\",\n",
    "    \"geo_confidence_boosted\": \"avg_conf\"\n",
    "}).reset_index()\n",
    "\n",
    "learned_vague = term_stats[\n",
    "    (term_stats[\"freq\"] >= 3) &\n",
    "    (term_stats[\"avg_conf\"] < 0.35)\n",
    "][\"entity_norm\"].tolist()\n",
    "print(f\" ML-learned vague terms: {len(learned_vague)}\")\n",
    "\n",
    "# === STEP 2: WordNet vague terms ===\n",
    "location_synsets = [\n",
    "    wn.synset(\"location.n.01\"),\n",
    "    wn.synset(\"region.n.01\"),\n",
    "    wn.synset(\"area.n.01\"),\n",
    "    wn.synset(\"place.n.01\"),\n",
    "    wn.synset(\"territory.n.01\")\n",
    "]\n",
    "\n",
    "vague_terms_wordnet = set()\n",
    "for syn in location_synsets:\n",
    "    for hypo in syn.closure(lambda s: s.hyponyms()):\n",
    "        for lemma in hypo.lemmas():\n",
    "            vague_terms_wordnet.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "print(f\" WordNet vague terms: {len(vague_terms_wordnet)}\")\n",
    "\n",
    "# === STEP 3: Combined vague term filtering ===\n",
    "df[\"is_vague_combined\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"entity_norm\"] in learned_vague or\n",
    "        (row[\"label\"] == \"LOC\" and row[\"entity_norm\"] in vague_terms_wordnet)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_filtered = df[~df[\"is_vague_combined\"]].copy()\n",
    "print(f\" Removed {df['is_vague_combined'].sum()} entities (ML or WordNet flagged)\")\n",
    "\n",
    "# === STEP 4: South America region restriction ===\n",
    "sa_countries = {\n",
    "    \"argentina\", \"bolivia\", \"brazil\", \"chile\", \"colombia\",\n",
    "    \"ecuador\", \"guyana\", \"paraguay\", \"peru\", \"suriname\", \"uruguay\", \"venezuela\"\n",
    "}\n",
    "\n",
    "if \"country\" in df_filtered.columns:\n",
    "    df_filtered[\"country\"] = df_filtered[\"country\"].astype(str).str.lower()\n",
    "    df_filtered = df_filtered[df_filtered[\"country\"].isin(sa_countries)].copy()\n",
    "    print(f\"üåé After SA region filter: {len(df_filtered)} rows\")\n",
    "else:\n",
    "    print(\" 'country' column not found. Cannot apply region filter.\")\n",
    "\n",
    "# === Final Save ===\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ner_ensemble_filtered_southamerica.csv\", index=False)\n",
    "print(\" Saved: geoparsing_ner_ensemble_filtered_southamerica.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('personnel_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_vehicle.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('weapons_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warplane.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warship.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('tank.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('half_track.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('bomber.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('fighter.n.02') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_plane.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('technical.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('minivan.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "Scoring entities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:15<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Saved: outputs/geoparsing_scored_candidates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 11 : Contextual Scoring with enhanced inputs\n",
    "import spacy\n",
    "import re\n",
    "import dateparser.search\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn, framenet as fn\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === Load motion verbs ===\n",
    "motion_frames = ['Motion', 'Travel', 'Self_motion', 'Arriving', 'Departing']\n",
    "motion_verbs = set()\n",
    "for frame in motion_frames:\n",
    "    try:\n",
    "        for lu in fn.frame_by_name(frame).lexUnit.values():\n",
    "            if lu['name'].endswith('.v'):\n",
    "                motion_verbs.add(lu['name'].split('.')[0].lower())\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# === Load transport terms ===\n",
    "vehicle_syn = wn.synset('vehicle.n.01')\n",
    "transport_terms = set()\n",
    "for syn in vehicle_syn.closure(lambda s: s.hyponyms()):\n",
    "    for lemma in syn.lemmas():\n",
    "        transport_terms.add(lemma.name().lower().replace('_', ' '))\n",
    "\n",
    "# === Load filtered file after SA restriction ===\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble_filtered_southamerica.csv\")\n",
    "\n",
    "# === Load sentence map ===\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "sentence_map = {}\n",
    "for line in lines:\n",
    "    if line.strip().startswith(\"[SENT\"):\n",
    "        sid = int(line.split(\"]\")[0].split()[1])\n",
    "        sentence_map[sid] = line.split(\"]\")[1].strip()\n",
    "\n",
    "# === Scoring logic ===\n",
    "scored_rows = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring entities\"):\n",
    "    sid = row[\"sentence_id\"]\n",
    "    entity = row[\"entity\"]\n",
    "    norm = row[\"entity_norm\"]\n",
    "    label = row[\"label\"]\n",
    "    sentence = sentence_map.get(sid, \"\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    score = 0\n",
    "    entity_token = None\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"} and ent.text.lower().strip() == entity.lower().strip():\n",
    "            entity_token = ent.root\n",
    "            break\n",
    "\n",
    "    if not entity_token:\n",
    "        continue\n",
    "\n",
    "    sentence_lower = sentence.lower()\n",
    "    entity_lower = entity.lower()\n",
    "\n",
    "    if any(verb in sentence_lower for verb in motion_verbs):\n",
    "        score += 1\n",
    "    if any(term in sentence_lower for term in transport_terms):\n",
    "        score += 1\n",
    "    if re.search(r'\\b(in|to|at)\\s+' + re.escape(entity_lower) + r'\\b', sentence_lower):\n",
    "        score += 1\n",
    "    if re.search(r\"\\b\" + re.escape(entity_lower) + r\"['‚Äô]s\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bof\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bfor\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if dateparser.search.search_dates(sentence):\n",
    "        score += 1\n",
    "\n",
    "    scored_rows.append({\n",
    "        \"sentence_id\": sid,\n",
    "        \"entity\": entity,\n",
    "        \"entity_norm\": norm,\n",
    "        \"label\": label,\n",
    "        \"sentence\": sentence,\n",
    "        \"score\": score,\n",
    "        \"latitude\": row.get(\"latitude\"),\n",
    "        \"longitude\": row.get(\"longitude\"),\n",
    "        \"country\": row.get(\"country\")\n",
    "    })\n",
    "\n",
    "scored_df = pd.DataFrame(scored_rows)\n",
    "scored_df = scored_df.sort_values(by=[\"score\", \"sentence_id\"], ascending=[False, True])\n",
    "scored_df.to_csv(\"outputs/geoparsing_scored_candidates.csv\", index=False)\n",
    "print(\"üìÑ Saved: outputs/geoparsing_scored_candidates.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ea256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Querying GeoNames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GeoNames queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:55<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Querying Wikidata for missing coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wikidata queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:35<00:00,  1.22it/s]\n",
      "/var/folders/62/b5nr017s2w11vxfg0s2020wh0000gn/T/ipykernel_92610/1796646064.py:69: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"wikidata_lat\"])\n",
      "/var/folders/62/b5nr017s2w11vxfg0s2020wh0000gn/T/ipykernel_92610/1796646064.py:70: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"wikidata_lon\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß≠ Querying OpenStreetMap for unresolved locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OSM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:11<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 12: Enrich entities with coordinates via GeoNames ‚Üí Wikidata ‚Üí OSM\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load scored candidates ===\n",
    "df = pd.read_csv(\"outputs/geoparsing_scored_candidates.csv\")\n",
    "places = df[\"entity_norm\"].dropna().unique()\n",
    "geonames_username = \"alicjab\"\n",
    "\n",
    "geo_data = {}\n",
    "\n",
    "# === Step 1: GeoNames ===\n",
    "print(\"üåç Querying GeoNames...\")\n",
    "for place in tqdm(places, desc=\"GeoNames queries\"):\n",
    "    try:\n",
    "        params = {\"q\": place, \"maxRows\": 1, \"username\": geonames_username}\n",
    "        r = requests.get(\"http://api.geonames.org/searchJSON\", params=params, timeout=10).json()\n",
    "        if not r.get(\"geonames\"):\n",
    "            raise ValueError(\"No result\")\n",
    "        g = r[\"geonames\"][0]\n",
    "        geo_data[place] = {\n",
    "            \"latitude\": float(g[\"lat\"]),\n",
    "            \"longitude\": float(g[\"lng\"]),\n",
    "            \"country\": g.get(\"countryName\"),\n",
    "            \"population\": int(g.get(\"population\", 0))\n",
    "        }\n",
    "    except Exception:\n",
    "        geo_data[place] = {\"latitude\": None, \"longitude\": None, \"country\": None, \"population\": None}\n",
    "    time.sleep(1)  # GeoNames rate limit\n",
    "\n",
    "geo_df = pd.DataFrame.from_dict(geo_data, orient=\"index\")\n",
    "geo_df.index.name = \"entity_norm\"\n",
    "geo_df.reset_index(inplace=True)\n",
    "\n",
    "# === Merge GeoNames results ===\n",
    "df_enriched = df.merge(geo_df, on=\"entity_norm\", how=\"left\")\n",
    "\n",
    "# === Step 2: Wikidata fallback ===\n",
    "print(\"üîÅ Querying Wikidata for missing coordinates...\")\n",
    "if \"latitude\" not in df_enriched.columns:\n",
    "    df_enriched[\"latitude\"] = None\n",
    "    df_enriched[\"longitude\"] = None\n",
    "\n",
    "missing_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_wikidata_coords(place):\n",
    "    try:\n",
    "        search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"search\": place,\n",
    "            \"language\": \"en\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        r = requests.get(search_url, params=search_params, timeout=10).json()\n",
    "        if not r[\"search\"]:\n",
    "            return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "        qid = r[\"search\"][0][\"id\"]\n",
    "        entity = requests.get(f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\").json()\n",
    "        coords = entity[\"entities\"][qid][\"claims\"].get(\"P625\", [{}])[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {})\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": coords.get(\"latitude\"), \"wikidata_lon\": coords.get(\"longitude\")}\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "\n",
    "wikidata_results = pd.DataFrame([query_wikidata_coords(p) for p in tqdm(missing_places, desc=\"Wikidata queries\")])\n",
    "df_enriched = df_enriched.merge(wikidata_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"wikidata_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"wikidata_lon\"])\n",
    "\n",
    "# === Step 3: OSM fallback ===\n",
    "print(\"üß≠ Querying OpenStreetMap for unresolved locations...\")\n",
    "nominatim_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_osm(place):\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://nominatim.openstreetmap.org/search\",\n",
    "            params={\"q\": place, \"format\": \"json\", \"limit\": 1},\n",
    "            headers={\"User-Agent\": \"Geoparser/1.0\"},\n",
    "            timeout=10\n",
    "        ).json()\n",
    "        if not r:\n",
    "            return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "        return {\n",
    "            \"entity_norm\": place,\n",
    "            \"osm_lat\": float(r[0][\"lat\"]),\n",
    "            \"osm_lon\": float(r[0][\"lon\"])\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "\n",
    "osm_results = pd.DataFrame([query_osm(p) for p in tqdm(nominatim_places, desc=\"OSM queries\")])\n",
    "df_enriched = df_enriched.merge(osm_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"osm_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"osm_lon\"])\n",
    "\n",
    "\n",
    "df_enriched.to_csv(\"outputs/geoparsing_final_enriched.csv\", index=False)\n",
    "print(\"‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚Ü©Ô∏è Reweighting outliers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 2712.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final post-reweighting size: 107 rows\n",
      "üåé Filtering by continent...\n",
      "Loading formatted geocoded file...\n",
      "üåç Removed 11 non-South American entries\n",
      "Saved final cleaned + reweighted + region-filtered version: geoparsing_final_scored_clustered.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 13: Geographic Outlier Filtering using DBSCAN (corrected input + centroid-aware reweighting + continent filtering)\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "\n",
    "# Drop rows without coordinates\n",
    "df_geo = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "coords_rad = np.radians(df_geo[[\"latitude\", \"longitude\"]].values)\n",
    "\n",
    "# Cluster with DBSCAN using haversine distance (in radians)\n",
    "clustering = DBSCAN(eps=0.5, min_samples=2, metric='haversine')\n",
    "df_geo[\"geo_cluster\"] = clustering.fit_predict(coords_rad)\n",
    "\n",
    "# === Compute centroids of each valid cluster ===\n",
    "centroids = (\n",
    "    df_geo[df_geo[\"geo_cluster\"] != -1]\n",
    "    .groupby(\"geo_cluster\")[[\"latitude\", \"longitude\"]]\n",
    "    .mean()\n",
    "    .to_dict(\"index\")\n",
    ")\n",
    "\n",
    "# === Reweight scores for outliers ===\n",
    "outliers = df_geo[df_geo[\"geo_cluster\"] == -1].copy()\n",
    "non_outliers = df_geo[df_geo[\"geo_cluster\"] != -1].copy()\n",
    "\n",
    "reweighted = []\n",
    "for _, row in tqdm(outliers.iterrows(), total=len(outliers), desc=\"‚Ü©Ô∏è Reweighting outliers\"):\n",
    "    min_dist_km = float(\"inf\")\n",
    "    entity_point = (row[\"latitude\"], row[\"longitude\"])\n",
    "\n",
    "    for c in centroids.values():\n",
    "        centroid_point = (c[\"latitude\"], c[\"longitude\"])\n",
    "        dist = geodesic(entity_point, centroid_point).km\n",
    "        if dist < min_dist_km:\n",
    "            min_dist_km = dist\n",
    "\n",
    "    # If outlier is <500 km from any cluster, keep it with downgraded score\n",
    "    if min_dist_km < 500:\n",
    "        row[\"geo_cluster\"] = -2  # kept but marked as downgraded outlier\n",
    "        row[\"geo_score_adjusted\"] = row.get(\"score\", 0) - 1\n",
    "        reweighted.append(row)\n",
    "\n",
    "# Combine cleaned + downgraded outliers\n",
    "final_df = pd.concat([non_outliers, pd.DataFrame(reweighted)], ignore_index=True)\n",
    "print(f\" Final post-reweighting size: {len(final_df)} rows\")\n",
    "\n",
    "# === Filter by continent (keep only South America) ===\n",
    "def get_continent(lat, lon):\n",
    "    try:\n",
    "        results = rg.search((lat, lon), mode=1)\n",
    "        cc = results[0]['cc']\n",
    "        # ISO country codes in South America\n",
    "        south_america = {\n",
    "            \"AR\", \"BO\", \"BR\", \"CL\", \"CO\", \"EC\", \"GY\", \"PY\", \"PE\", \"SR\", \"UY\", \"VE\",  # South America\n",
    "             \"MX\", \"GT\", \"HN\", \"SV\", \"NI\", \"CR\", \"PA\"  # Central America / Mesoamerica\n",
    "        }\n",
    "        return 'South America' if cc in south_america else 'Other'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "print(\"üåé Filtering by continent...\")\n",
    "final_df[\"continent\"] = final_df.apply(\n",
    "    lambda row: get_continent(row[\"latitude\"], row[\"longitude\"]), axis=1\n",
    ")\n",
    "\n",
    "before_filter = len(final_df)\n",
    "final_df = final_df[final_df[\"continent\"] == \"South America\"].copy()\n",
    "print(f\"üåç Removed {before_filter - len(final_df)} non-South American entries\")\n",
    "\n",
    "# Save final output\n",
    "final_df.to_csv(\"outputs/geoparsing_final_scored_clustered.csv\", index=False)\n",
    "print(\"Saved final cleaned + reweighted + region-filtered version: geoparsing_final_scored_clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e8424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "geo": "geo",
         "hovertemplate": "<b>%{hovertext}</b><br><br>entity=%{text}<br>latitude=%{lat}<br>longitude=%{lon}<br>cluster=%{marker.color}<extra></extra>",
         "hovertext": [
          "Antofagasta (cluster 0)",
          "Chile (cluster 0)",
          "Bogot√° (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Osorno (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Peru (cluster 0)",
          "Venezuela (cluster 0)",
          "Venezuela (cluster 0)",
          "Bogot√° (cluster 0)",
          "Colombia (cluster 0)",
          "Caracas (cluster 0)",
          "Chile (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Granado (cluster 0)",
          "Chile (cluster 0)",
          "San Carlos de Bariloche (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Peru (cluster 0)",
          "Chile (cluster 0)",
          "Iquique (cluster 0)",
          "Arica (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Santiago de Chile (cluster 0)",
          "Puno (cluster 0)",
          "Lima (cluster 0)",
          "Peru (cluster 0)",
          "peru (cluster 0)",
          "Argentina (cluster 0)",
          "Manaos (cluster 0)",
          "Peru (cluster 0)",
          "Bogot√° (cluster 0)",
          "Colombia (cluster 0)",
          "Venezuela (cluster 0)",
          "Colombia (cluster 0)",
          "C√≥rdoba (cluster 0)",
          "Baquedano (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Valpara√≠so (cluster 0)",
          "Chile (cluster 0)",
          "Americas (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Arica (cluster 0)",
          "Peru (cluster 0)",
          "Chile (cluster 0)",
          "Chuquicamata (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Andes (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "C√≥rdoba (cluster 0)",
          "Santa Luc√≠a (cluster 0)",
          "Tarat√° (cluster 0)",
          "Peru (cluster 0)",
          "Pucallpa (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Lima (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Lima (cluster 0)",
          "Chile (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "South America (cluster 0)",
          "Bolivia (cluster 0)",
          "Suqu√≠a (cluster 0)",
          "Peru (cluster 0)",
          "Quechua (cluster 0)",
          "Colombia (cluster 0)",
          "Colombia (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)"
         ],
         "lat": {
          "bdata": "i0YCvnmlN8AAAAAAAIBAwDgteNFXcBJAAAAAAAAAQcAAAAAAAABBwHyqqqqqSkTAAAAAAACAQMAAAAAAAIBAwAAAAAAAAEHAzczMzMzMIsAAAAAAAAAgQAAAAAAAACBAOC140VdwEkAAAAAAAAAQQAAAAAAAACVAAAAAAACAQMDNzMzMzMwiwM3MzMzMzCLAYdtHE5TTJ0AAAAAAAIBAwDMzMzMzk0TAAAAAAACAQMAAAAAAAABBwM3MzMzMzCLAAAAAAACAQMDXEvJBzzY0wLIxCt2AejLAzczMzMzMIsAAAAAAAABBwAAAAAAAgEDAAAAAAACAQMAAAAAAAABBwAAAAAAAuEDAdJX8YsmvL8DxvFRszB8owM3MzMzMzCLAzczMzMzMIsAAAAAAAABBwDaEdMKVDQnAzczMzMzMIsA4LXjRV3ASQAAAAAAAABBAAAAAAAAAIEAAAAAAAAAQQAirqqqqaj/A4A55cBzJRsAAAAAAAABBwAAAAAAAgEDAKxoJ+OaFQMAAAAAAAIBAwAAAAAAAADNAAAAAAACAQMAAAAAAAIBAwLIxCt2AejLAzczMzMzMIsAAAAAAAIBAwPjeY9lhSjbAAAAAAACAQMAAAAAAAABBwAXKARZUnxZAAAAAAACAQMAAAAAAAABBwAAAAAAAAEHAAAAAAAAAQcAAAAAAAABBwAAAAAAAgEDACKuqqqpqP8CydlDYpxApQKrXLQJjeTHAzczMzMzMIsDMUvwpn8MgwM3MzMzMzCLAzczMzMzMIsDNzMzMzMwiwAAAAAAAAEHA8bxUbMwfKMDNzMzMzMwiwM3MzMzMzCLA8bxUbMwfKMAAAAAAAIBAwM3MzMzMzCLAAAAAAAAAQcAAAAAAAABBwAAAAAAAgEDAAAAAAACAQMAAAAAAAAA1wKXbvwGPDjHAFyr/Wl5jP8DNzMzMzMwiwDjUlAlkZzvAAAAAAAAAEEAAAAAAAAAQQAAAAAAAAEHAAAAAAAAAQcAAAAAAAIBAwAAAAAAAgEDA",
          "dtype": "f8"
         },
         "legendgroup": "",
         "lon": {
          "bdata": "i0YCvnmZUcAAAAAAAMBRwKwcWmQ7hVLAAAAAAAAAUMAAAAAAAABQwMKqqqqqSlLAAAAAAADAUcAAAAAAAMBRwAAAAAAAAFDAAAAAAAAAU8AAAAAAAMBQwAAAAAAAwFDArBxaZDuFUsAAAAAAAFBSwKS7u7u7u1DAAAAAAADAUcAAAAAAAABTwAAAAAAAAFPApR5YRIN4VcAAAAAAAMBRwDMzMzMz01HAAAAAAADAUcAAAAAAAABQwAAAAAAAAFPAAAAAAADAUcBE+u3rwIlRwEEdRYyNlFHAAAAAAAAAU8AAAAAAAABQwAAAAAAAwFHAAAAAAADAUcAAAAAAAABQwJqZmZmZqVHA0IIt2IKBUcDgXXhvVkJTwAAAAAAAAFPAAAAAAAAAU8AAAAAAAABQwMzXwLHC/U3AAAAAAAAAU8CsHFpkO4VSwAAAAAAAUFLAAAAAAADAUMAAAAAAAFBSwKS7u7u7C1DAV65cFGIEUsAAAAAAAABQwAAAAAAAwFHAIUNlh6nnUcAAAAAAAMBRwAAAAAAAAFjAAAAAAADAUcAAAAAAAMBRwEEdRYyNlFHAAAAAAAAAU8AAAAAAAMBRwPomLHW5OVHAAAAAAADAUcAAAAAAAABQwK299Jsu+FLAAAAAAADAUcAAAAAAAABQwAAAAAAAAFDAAAAAAAAAUMAAAAAAAABQwAAAAAAAwFHApLu7u7sLUMBrc9jMg21VwBEdAkcCglHAAAAAAAAAU8CdTDfkeqJSwAAAAAAAAFPAAAAAAAAAU8AAAAAAAABTwAAAAAAAAFDA4F14b1ZCU8AAAAAAAABTwAAAAAAAAFPA4F14b1ZCU8AAAAAAAMBRwAAAAAAAAFPAAAAAAAAAUMAAAAAAAABQwAAAAAAAwFHAAAAAAADAUcAAAAAAAIBNwDPlIEpwP1DANO5ytqUMUMAAAAAAAABTwD0q/u8IkVHAAAAAAABQUsAAAAAAAFBSwAAAAAAAAFDAAAAAAAAAUMAAAAAAAMBRwAAAAAAAwFHA",
          "dtype": "f8"
         },
         "marker": {
          "color": {
           "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "",
         "showlegend": false,
         "text": [
          "Antofagasta",
          "Chile",
          "Bogot√°",
          "Argentina",
          "Argentina",
          "Osorno",
          "Chile",
          "Chile",
          "Argentina",
          "Peru",
          "Venezuela",
          "Venezuela",
          "Bogot√°",
          "Colombia",
          "Caracas",
          "Chile",
          "Peru",
          "Peru",
          "Granado",
          "Chile",
          "San Carlos de Bariloche",
          "Chile",
          "Argentina",
          "Peru",
          "Chile",
          "Iquique",
          "Arica",
          "Peru",
          "Argentina",
          "Chile",
          "Chile",
          "Argentina",
          "Santiago de Chile",
          "Puno",
          "Lima",
          "Peru",
          "peru",
          "Argentina",
          "Manaos",
          "Peru",
          "Bogot√°",
          "Colombia",
          "Venezuela",
          "Colombia",
          "C√≥rdoba",
          "Baquedano",
          "Argentina",
          "Chile",
          "Valpara√≠so",
          "Chile",
          "Americas",
          "Chile",
          "Chile",
          "Arica",
          "Peru",
          "Chile",
          "Chuquicamata",
          "Chile",
          "Argentina",
          "Andes",
          "Chile",
          "Argentina",
          "Argentina",
          "Argentina",
          "Argentina",
          "Chile",
          "C√≥rdoba",
          "Santa Luc√≠a",
          "Tarat√°",
          "Peru",
          "Pucallpa",
          "Peru",
          "Peru",
          "Peru",
          "Argentina",
          "Lima",
          "Peru",
          "Peru",
          "Lima",
          "Chile",
          "Peru",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile",
          "South America",
          "Bolivia",
          "Suqu√≠a",
          "Peru",
          "Quechua",
          "Colombia",
          "Colombia",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile"
         ],
         "type": "scattergeo"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "geo": {
         "center": {},
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "projection": {
          "type": "natural earth"
         }
        },
        "legend": {
         "title": {
          "text": "Cluster ID"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Clustered Location Mentions from Text"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Block 14: Visualize clustered results using Plotly\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load clustered data\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_scored_clustered.csv\")\n",
    "\n",
    "# Drop NaNs (should already be clean)\n",
    "df = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# Rename cluster column for consistency with Plotly\n",
    "df[\"cluster\"] = df[\"geo_cluster\"]\n",
    "\n",
    "# Create hover label\n",
    "df[\"hover\"] = df[\"entity\"] + \" (cluster \" + df[\"cluster\"].astype(str) + \")\"\n",
    "\n",
    "# Basic scatter geo map\n",
    "fig = px.scatter_geo(\n",
    "    df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    text=\"entity\",\n",
    "    hover_name=\"hover\",\n",
    "    color=\"cluster\",\n",
    "    title=\"Clustered Location Mentions from Text\",\n",
    "    projection=\"natural earth\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.update_layout(legend_title_text='Cluster ID')\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
