{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdb186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package names to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in ./.venv/lib/python3.13/site-packages (from geopandas) (2.1.2)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.11.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from geopandas) (25.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in ./.venv/lib/python3.13/site-packages (from geopandas) (2.2.3)\n",
      "Collecting pyproj>=3.5.0 (from geopandas)\n",
      "  Downloading pyproj-3.7.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (31 kB)\n",
      "Collecting shapely>=2.0.0 (from geopandas)\n",
      "  Downloading shapely-2.1.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=2.0.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=2.0.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from pyogrio>=0.7.2->geopandas) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.16.0)\n",
      "Downloading geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
      "Downloading pyogrio-0.11.1-cp313-cp313-macosx_12_0_arm64.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/19.4 MB\u001b[0m \u001b[31m89.3 kB/s\u001b[0m eta \u001b[36m0:01:44\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ~~~~~~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/commands/install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        partially_downloaded_reqs,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        parallel_builds=parallel_builds,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ~~~~~~~~~~~~~~~~~~~^\n",
      "        chunk_size,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<22 lines>...\n",
      "        decode_content=False,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: geopy in ./.venv/lib/python3.13/site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in ./.venv/lib/python3.13/site-packages (from geopy) (2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting stanza\n",
      "  Using cached stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Using cached emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from stanza) (2.1.2)\n",
      "Collecting protobuf>=3.15.0 (from stanza)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from stanza) (2.32.4)\n",
      "Collecting networkx (from stanza)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting torch>=1.3.0 (from stanza)\n",
      "  Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from stanza) (4.67.1)\n",
      "Collecting filelock (from torch>=1.3.0->stanza)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch>=1.3.0->stanza)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (75.3.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.3.0->stanza)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Collecting fsspec (from torch>=1.3.0->stanza)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (2025.7.14)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.3.0->stanza)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Using cached stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/73.6 MB\u001b[0m \u001b[31m85.3 kB/s\u001b[0m eta \u001b[36m0:09:47\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ~~~~~~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/commands/install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        partially_downloaded_reqs,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        parallel_builds=parallel_builds,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ~~~~~~~~~~~~~~~~~~~^\n",
      "        chunk_size,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<22 lines>...\n",
      "        decode_content=False,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting geodatasets\n",
      "  Using cached geodatasets-2024.8.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pooch (from geodatasets)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from pooch->geodatasets) (4.3.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from pooch->geodatasets) (25.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.13/site-packages (from pooch->geodatasets) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (2025.7.14)\n",
      "Using cached geodatasets-2024.8.0-py3-none-any.whl (20 kB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: pooch, geodatasets\n",
      "Successfully installed geodatasets-2024.8.0 pooch-1.8.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: folium in ./.venv/lib/python3.13/site-packages (0.20.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in ./.venv/lib/python3.13/site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in ./.venv/lib/python3.13/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from folium) (2.1.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from folium) (2.32.4)\n",
      "Requirement already satisfied: xyzservices in ./.venv/lib/python3.13/site-packages (from folium) (2025.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->folium) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->folium) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->folium) (2025.7.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.3-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from xgboost) (2.1.2)\n",
      "Collecting scipy (from xgboost)\n",
      "  Using cached scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Downloading xgboost-3.0.3-py3-none-macosx_12_0_arm64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:04\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/20.8 MB\u001b[0m \u001b[31m90.1 kB/s\u001b[0m eta \u001b[36m0:01:35\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ~~~~~~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/commands/install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        partially_downloaded_reqs,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        parallel_builds=parallel_builds,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ~~~~~~~~~~~~~~~~~~~^\n",
      "        chunk_size,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<22 lines>...\n",
      "        decode_content=False,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/Users/alicja/Desktop/BA-code/.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Pre-Block: Downloads & Setup \n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"framenet_v17\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('names')\n",
    "\n",
    "%pip install geopandas\n",
    "%pip install geopy\n",
    "%pip install stanza\n",
    "%pip install geodatasets\n",
    "%pip install folium\n",
    "%pip install xgboost\n",
    "%pip install scikit-learn\n",
    "%pip install spacy\n",
    "%pip install fuzzywuzzy\n",
    "%pip install contractions\n",
    "%pip install rapidfuzz\n",
    "%pip install pycountry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ccd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline: ['sentencizer']\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Imports \n",
    "\n",
    "\n",
    "import os, re, time, json, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import yaml\n",
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "\n",
    "# Data / ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Geo / viz\n",
    "import folium\n",
    "from folium.plugins import AntPath\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NLP / NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, names, wordnet as wn, framenet as fn\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "# Project helpers\n",
    "from geoparser import nlp_helpers as nh\n",
    "import importlib; importlib.reload(nh)\n",
    "\n",
    "\n",
    "nlp, stanza_pipeline = nh.init_nlp(lang=\"es\", with_stanza=False)\n",
    "\n",
    "print(\"spaCy pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# stopwords for mixed ES/PT/EN corpora:\n",
    "stopset = nh.get_stopwords(nlp, langs=[\"es\", \"pt\", \"en\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 2: Extract text from PDF using PyMuPDF; pages 28-148\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str, start_page: int, end_page: int) -> str:\n",
    "    \"\"\"Extracts and returns text from a PDF given a path and page range.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = doc[start_page:end_page]\n",
    "    return \"\\n\".join(page.get_text() for page in pages)\n",
    "\n",
    "\n",
    "\n",
    "#Load configuration from YAML file\n",
    "def load_config(config_path: str = \"config.yaml\") -> dict:\n",
    "    \"\"\"Loads YAML configuration file and returns it as a dictionary.\"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        \n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "raw_text = extract_text_from_pdf(pdf_conf[\"path\"], pdf_conf[\"start_page\"], pdf_conf[\"end_page\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e33e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 3: Text Preprocessing Functions\n",
    "\n",
    "from geoparser.nlp_helpers import (\n",
    "    init_nlp,\n",
    "    get_stopwords,\n",
    "    tag_named_entities,\n",
    "    extract_text_from_pdf,\n",
    "    load_config,\n",
    "    normalize_punctuation,\n",
    "    clean_light,\n",
    "    preprocess_text,\n",
    "    segment_sentences,\n",
    "    clean_heavy,\n",
    "    load_filters,\n",
    "    normalize_diacritics,\n",
    "    apply_ocr_replacements,\n",
    "    is_caption_line,\n",
    "    filter_raw_sentences\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned outputs saved:\n",
      "- Geoparsing (light): outputs/cleaned_MotorcycleDiaries_geoparsing.txt\n",
      "- NLP prep (heavy): outputs/cleaned_MotorcycleDiaries_nlp.txt\n",
      "‚úÖ sentence_data prepared with 1975 narrative sentences.\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Clean and Save Tagged + NLP Versions \n",
    "\n",
    "# Load config\n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "gaz_conf = config[\"gazetteer\"]\n",
    "\n",
    "# Output filenames based on input PDF\n",
    "base_name = Path(pdf_conf[\"path\"]).stem\n",
    "tagged_path = Path(\"outputs\") / f\"cleaned_{base_name}_geoparsing.txt\"\n",
    "heavy_path = Path(\"outputs\") / f\"cleaned_{base_name}_nlp.txt\"\n",
    "\n",
    "# Create outputs dir if needed\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# üßπ Clean raw text (light and heavy)\n",
    "light_cleaned = clean_light(raw_text)\n",
    "sentences = segment_sentences(light_cleaned, nlp)\n",
    "sentences_with_tags = [f\"[SENT {i+1}] {s}\" for i, s in enumerate(sentences)]\n",
    "\n",
    "all_stops = get_stopwords(nlp)\n",
    "heavy_cleaned = clean_heavy(light_cleaned, nlp, all_stops)\n",
    "\n",
    "# Save both cleaned versions (one seems by this stage to be unnecessary, can as well drop the second pdf creation)\n",
    "with open(tagged_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sentences_with_tags))\n",
    "\n",
    "with open(heavy_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(heavy_cleaned)\n",
    "\n",
    "print(\"Cleaned outputs saved:\")\n",
    "print(f\"- Geoparsing (light): {tagged_path}\")\n",
    "print(f\"- NLP prep (heavy): {heavy_path}\")\n",
    "\n",
    "# Prepare sentence_data for Block 7\n",
    "sentence_data = [(i, sent.strip()) for i, sent in enumerate(sentences)]\n",
    "print(f\"sentence_data prepared with {len(sentence_data)} narrative sentences.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8272556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loaded 227 symbolic verb forms from WordNet.\n",
      "üõ£Ô∏è Loaded 238 movement verb forms from WordNet.\n"
     ]
    }
   ],
   "source": [
    "# Build symbolic verb lexicon from WordNet (only once)\n",
    "\n",
    "\n",
    "def get_symbolic_verb_synonyms():\n",
    "    base_words = [\n",
    "        \"dream\", \"hope\", \"struggle\", \"escape\", \"resist\", \"believe\", \"follow\", \n",
    "        \"ride\", \"rebel\", \"fight\", \"flee\", \"live\", \"return\", \"envision\", \"imagine\", \"create\", \"inspire\", \"transform\", \"change\", \"grow\"\n",
    "    ]\n",
    "    synonyms = set()\n",
    "    for word in base_words:\n",
    "        for syn in wordnet.synsets(word, pos=wordnet.VERB):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "    return synonyms\n",
    "\n",
    "from nltk.corpus import wordnet # Ensure WordNet is available, \n",
    "# Save the expanded verb set to reuse\n",
    "SYMBOLIC_VERBS = get_symbolic_verb_synonyms()\n",
    "print(f\"Loaded {len(SYMBOLIC_VERBS)} symbolic verb forms from WordNet.\")\n",
    "\n",
    "# Build movement verb list using WordNet\n",
    "\n",
    "def get_movement_verbs():\n",
    "    base = [\"go\", \"move\", \"travel\", \"walk\", \"drive\", \"ride\", \"arrive\", \"depart\", \"leave\", \"return\", \"cross\", \"fly\", \"sail\", \"swim\", \"follow\",\"hike\"]\n",
    "    move_verbs = set()\n",
    "    for word in base:\n",
    "        for syn in wordnet.synsets(word, pos=wordnet.VERB):\n",
    "            for lemma in syn.lemmas():\n",
    "                move_verbs.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "                \n",
    "    return move_verbs\n",
    "\n",
    "MOVEMENT_VERBS = get_movement_verbs()\n",
    "print(f\"Loaded {len(MOVEMENT_VERBS)} movement verb forms from WordNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41623c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found existing gazetteer file, loading...\n"
     ]
    }
   ],
   "source": [
    "# Block 6: Build Gazetteer\n",
    "\n",
    "\n",
    "# JSON \n",
    "def make_json_safe(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    raise TypeError(f\"Not JSON serializable: {type(obj)}\")\n",
    "\n",
    "#  Load config.yaml\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "gaz_conf = config[\"gazetteer\"]\n",
    "\n",
    "# Gazetteer path\n",
    "gazetteer_path = Path(\"outputs/gazetteer_cities.json\")\n",
    "\n",
    "# Build or load gazetteer\n",
    "if gazetteer_path.exists():\n",
    "    print(\"Found existing gazetteer file, loading...\")\n",
    "    with open(gazetteer_path, \"r\") as f:\n",
    "        gazetteer = json.load(f)\n",
    "else:\n",
    "    print(\"No gazetteer file found, building...\")\n",
    "    gazetteer = build_gazetteer(\n",
    "        username=gaz_conf[\"username\"],\n",
    "        countries=gaz_conf[\"countries\"],\n",
    "        max_rows=gaz_conf[\"max_rows\"]\n",
    "    )\n",
    "    with open(gazetteer_path, \"w\") as f:\n",
    "        json.dump(gazetteer, f, indent=2, default=make_json_safe) \n",
    "    print(\"Gazetteer built and saved with coordinates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c500c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßº Pre-filtered sentences: kept 1933 / 1975\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"config.yaml\")\n",
    "filters = load_filters(cfg)\n",
    "\n",
    "# Apply OCR fixes to original text (not lowercasing)\n",
    "if isinstance(sentence_data, list) and sentence_data and isinstance(sentence_data[0], tuple):\n",
    "    sentence_data_fixed = []\n",
    "    for sid, txt in sentence_data:\n",
    "        txt2 = apply_ocr_replacements(txt, filters[\"ocr_replacements\"])\n",
    "        sentence_data_fixed.append((sid, txt2))\n",
    "else:\n",
    "    sentence_data_fixed = [\n",
    "        apply_ocr_replacements(txt, filters[\"ocr_replacements\"]) for txt in sentence_data\n",
    "    ]\n",
    "\n",
    "# Drop footnotes/captions/sections based on config / one of the initial problems that caused a lot of noise \n",
    "sentence_data_prefiltered = filter_raw_sentences(sentence_data_fixed, filters)\n",
    "\n",
    "print(f\"Pre-filtered sentences: kept {len(sentence_data_prefiltered)} / {len(sentence_data)}\")\n",
    "SENTENCES_FOR_NER = sentence_data_prefiltered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70be0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline: ['sentencizer']\n",
      "üîé Running ensemble NER + gazetteer matching (with metonymy & filters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER + Gazetteer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1933/1933 [02:18<00:00, 14.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåé Restricted to South America: 391 -> 332 rows\n",
      "‚úÖ Saved: outputs/geoparsing_ner_ensemble.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 7: NER + Gazetteer with Metonymy Filtering (Country-Aware) \n",
    "\n",
    "\n",
    "from geoparser.gazetteer_helpers import (\n",
    "    build_gazetteer, gazetteer_names,\n",
    "    build_gazetteer_patterns, match_gazetteer_precompiled,\n",
    "    remove_overlapping_shorter\n",
    ")\n",
    "\n",
    "# --- Ensure NLP is loaded (nlp, stanza_pipeline come from your helpers) ---\n",
    "try:\n",
    "    nlp  # noqa: F821\n",
    "except NameError:\n",
    "    nlp, stanza_pipeline = init_nlp()  # noqa: F821\n",
    "\n",
    "# Make sure Stanza is only used if initialized\n",
    "USE_STANZA = bool(globals().get(\"stanza_pipeline\"))\n",
    "\n",
    "#  make sure spaCy has NER\n",
    "try:\n",
    "    print(\"spaCy pipeline:\", getattr(nlp, \"pipe_names\", []))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Stopwords and config \n",
    "stop_words = get_stopwords(nlp)   # noqa: F821\n",
    "try:\n",
    "    cfg  # noqa: F821\n",
    "except NameError:\n",
    "    cfg = load_config(\"config.yaml\")  # noqa: F821\n",
    "\n",
    "# Gazetteer: try cache -> then build if needed \n",
    "CACHE_PATH = Path(\"outputs/geonames_cache.json\")\n",
    "gazetteer_loaded_from_cache = False\n",
    "\n",
    "def _load_gazetteer_cache(path: Path) -> Dict[str, Dict]:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        df = pd.read_json(path)\n",
    "        # stored as index = name; orient=index\n",
    "        if \"name\" in df.columns:\n",
    "            df = df.set_index(\"name\")\n",
    "        return df.to_dict(orient=\"index\")\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _save_gazetteer_cache(g: Dict[str, Dict], path: Path) -> None:\n",
    "    try:\n",
    "        Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        df = pd.DataFrame.from_dict(g, orient=\"index\").reset_index().rename(columns={\"index\":\"name\"})\n",
    "        df.to_json(path, orient=\"records\")\n",
    "        print(f\"üíæ Saved gazetteer cache: {path}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Build gazetteer (must include coords + country info); \n",
    "try:\n",
    "    gazetteer  # noqa: F821\n",
    "except NameError:\n",
    "    gazetteer = _load_gazetteer_cache(CACHE_PATH)\n",
    "    if gazetteer:\n",
    "        gazetteer_loaded_from_cache = True\n",
    "        print(f\"üíæ Loaded gazetteer cache: {len(gazetteer)} names\")\n",
    "    else:\n",
    "        gcfg = cfg.get(\"gazetteer\", {})\n",
    "        gazetteer = build_gazetteer(\n",
    "            username=gcfg.get(\"username\", \"\"),\n",
    "            countries=gcfg.get(\"countries\", []),\n",
    "            max_rows=int(gcfg.get(\"max_rows\", 1000))\n",
    "        )\n",
    "        _save_gazetteer_cache(gazetteer, CACHE_PATH)\n",
    "\n",
    "# Ensure gazetteer has country fields; rebuild or retrofit if needed\n",
    "def _gaz_has_country(g: Dict[str, Dict]) -> bool:\n",
    "    return bool(g) and all((\"country\" in v and \"country_code\" in v) for v in g.values())\n",
    "\n",
    "def _retrofill_country(g: Dict[str, Dict]) -> int:\n",
    "    COUNTRY_NAME = {\n",
    "        \"AR\":\"Argentina\",\"CL\":\"Chile\",\"PE\":\"Peru\",\"CO\":\"Colombia\",\"VE\":\"Venezuela\",\n",
    "        \"BO\":\"Bolivia\",\"EC\":\"Ecuador\",\"PA\":\"Panama\",\"CR\":\"Costa Rica\",\"GT\":\"Guatemala\",\n",
    "        \"MX\":\"Mexico\",\"CU\":\"Cuba\",\"BR\":\"Brazil\",\"GY\":\"Guyana\",\"PY\":\"Paraguay\",\n",
    "        \"SR\":\"Suriname\",\"UY\":\"Uruguay\",\"HN\":\"Honduras\",\"SV\":\"El Salvador\",\"NI\":\"Nicaragua\"\n",
    "    }\n",
    "    changed = 0\n",
    "    for k, v in g.items():\n",
    "        cc = v.get(\"country_code\") or v.get(\"countryCode\") or v.get(\"cc\")\n",
    "        if cc and \"country\" not in v:\n",
    "            v[\"country\"] = COUNTRY_NAME.get(cc, cc)\n",
    "            changed += 1\n",
    "    return changed\n",
    "\n",
    "if not _gaz_has_country(gazetteer):\n",
    "    fixed = _retrofill_country(gazetteer)\n",
    "    if not _gaz_has_country(gazetteer):\n",
    "        # only rebuild if we didn't load from cache (avoid rate limit loops)\n",
    "        if gazetteer_loaded_from_cache:\n",
    "            print(\"Gazetteer cache lacks country fields; rebuild later when API quota allows.\")\n",
    "        else:\n",
    "            print(\"‚Ü∫ Rebuilding gazetteer with country fields (old object lacked them)...\")\n",
    "            gcfg = cfg.get(\"gazetteer\", {})\n",
    "            gazetteer = build_gazetteer(\n",
    "                username=gcfg.get(\"username\", \"\"),\n",
    "                countries=gcfg.get(\"countries\", []),\n",
    "                max_rows=int(gcfg.get(\"max_rows\", 1000))\n",
    "            )\n",
    "            _save_gazetteer_cache(gazetteer, CACHE_PATH)\n",
    "            if not _gaz_has_country(gazetteer):\n",
    "                raise RuntimeError(\"Rebuilt gazetteer still lacks 'country'/'country_code'. Check GeoNames username and network.\")\n",
    "    else:\n",
    "        print(f\"Retrofilled 'country' names for {fixed} entries from existing 'country_code'.\")\n",
    "\n",
    "# Figurative single-word country names (avoid as literal place mentions)\n",
    "_COUNTRY_NAMES_EN = {\n",
    "    \"argentina\",\"chile\",\"peru\",\"colombia\",\"venezuela\",\"bolivia\",\"ecuador\",\n",
    "    \"panama\",\"costa rica\",\"guatemala\",\"mexico\",\"cuba\",\"brazil\",\"guyana\",\n",
    "    \"paraguay\",\"suriname\",\"uruguay\",\"honduras\",\"el salvador\",\"nicaragua\"\n",
    "}\n",
    "\n",
    "# Safer gazetteer name gate BEFORE building regex \n",
    "_EN_STOP = set(stopwords.words(\"english\"))\n",
    "\n",
    "# head-words that are too generic alone; ban as singletons (used to be outliers that i could not figure out how to classify)\n",
    "BANNED_SINGLE_HEADS = {\n",
    "    \"hospital\",\"station\",\"school\",\"airport\",\"bridge\",\"park\",\"market\",\n",
    "    \"university\",\"college\",\"city\",\"province\",\"region\",\"mama\",\"friends\",\"best\"\n",
    "}\n",
    "\n",
    "# banned full names / bigrams that slipped through earlier (last resort; had to hard-code it since regex was too permissive)\n",
    "BANNED_FULL_NAMES = {\"the best\"}\n",
    "\n",
    "def _looks_like_place_name(name: str) -> bool:\n",
    "    t = (name or \"\").strip().lower()\n",
    "    if not t:\n",
    "        return False\n",
    "    if t in BANNED_FULL_NAMES:\n",
    "        return False\n",
    "    toks = re.findall(r\"[a-z√†-√ø]+\", t)\n",
    "    if not toks:\n",
    "        return False\n",
    "    if len(toks) == 1 and toks[0] in BANNED_SINGLE_HEADS:\n",
    "        return False\n",
    "    # kill all-stopword ngrams incl. \"the best\"\n",
    "    if all(tok in _EN_STOP for tok in toks):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "_PLACES: Set[str] = {n for n in gazetteer_names(gazetteer) if _looks_like_place_name(n)}\n",
    "GAZ_PATTERNS = build_gazetteer_patterns(_PLACES, _EN_STOP)\n",
    "\n",
    "def match_gazetteer_safe(text: str) -> List[Tuple[str, str, int, int]]:\n",
    "    hits = match_gazetteer_precompiled(text, GAZ_PATTERNS)\n",
    "    safe = []\n",
    "    for sl, _, s, e in hits:\n",
    "        if \" \" not in sl and sl.lower() in _COUNTRY_NAMES_EN:\n",
    "            continue\n",
    "        if sl.strip().lower() in BANNED_FULL_NAMES:\n",
    "            continue\n",
    "        safe.append((sl, \"GAZETTEER\", s, e))\n",
    "    return safe\n",
    "\n",
    "# English verb lexicons (for metonymy / movement cues)\n",
    "# the wordbook had been done in the previous step, so that the pipeline goes smoother\n",
    "SYMBOLIC_VERBS = get_symbolic_verb_synonyms()\n",
    "MOVEMENT_VERBS = get_movement_verbs()\n",
    "\n",
    "# --- NER extractors (spaCy + optional Stanza) ---\n",
    "def extract_entities_spacy(text: str) -> List[Tuple[str, str, int, int]]:\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "def extract_entities_stanza(text: str) -> List[Tuple[str, str, int, int]]:\n",
    "    if not USE_STANZA or stanza_pipeline is None:\n",
    "        return []\n",
    "    try:\n",
    "        doc = stanza_pipeline(text)\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for sent in doc.sentences:\n",
    "        for ent in getattr(sent, \"ents\", []):\n",
    "            out.append((ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "    return out\n",
    "\n",
    "# Lexical filters & helpers\n",
    "_EN_FIRSTNAMES: Set[str] = {n.lower() for n in names.words()}\n",
    "_MONTHS = {\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"}\n",
    "_DAYS   = {\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"}\n",
    "\n",
    "def _is_stoplike(tok: str) -> bool:\n",
    "    t = tok.lower()\n",
    "    return t in _EN_STOP or t in _MONTHS or t in _DAYS\n",
    "\n",
    "def _looks_like_firstname(tok: str) -> bool:\n",
    "    return tok.lower() in _EN_FIRSTNAMES\n",
    "\n",
    "def _valid_toponym(span_text: str) -> bool:\n",
    "    t = span_text.strip()\n",
    "    toks = re.findall(r\"[A-Za-z√Ä-√ø]+\", t)\n",
    "    if not toks:\n",
    "        return False\n",
    "    if len(toks) == 1:\n",
    "        tok = toks[0]\n",
    "        if t.islower(): return False\n",
    "        if _is_stoplike(tok): return False\n",
    "        if _looks_like_firstname(tok): return False\n",
    "    return True\n",
    "\n",
    "COMPOSITE_HEADS = {\"villa\",\"puerto\",\"bah√≠a\",\"bahia\",\"r√≠o\",\"rio\",\"cerro\",\"san\",\"santa\",\"santo\"}\n",
    "AMBIGUOUS_SINGLETONS = {\"sierra\",\"villa\",\"serra\",\"rio\"}\n",
    "\n",
    "def _person_prefix_rule(sent_text: str, span_text: str) -> bool:\n",
    "    head = re.findall(r\"[A-Za-z√Ä-√ø]+\", span_text.lower())[:1]\n",
    "    if head and head[0] in COMPOSITE_HEADS:\n",
    "        return False\n",
    "    m = re.search(r\"\\b([A-Z][a-z]+)\\s+\" + re.escape(span_text) + r\"\\b\", sent_text)\n",
    "    return bool(m and m.group(1).lower() in _EN_FIRSTNAMES)\n",
    "\n",
    "def _ambiguous_singleton_ok(ent_text: str, sentence: str, gazetteer_names: Set[str]) -> bool:\n",
    "    toks = re.findall(r\"[A-Za-z√Ä-√ø]+\", ent_text.strip())\n",
    "    if len(toks) != 1:\n",
    "        return True\n",
    "    head = toks[0].lower()\n",
    "    if head not in AMBIGUOUS_SINGLETONS:\n",
    "        return True\n",
    "    if head in gazetteer_names:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _norm_entity(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[‚Äô']s\\b\", \"\", s)\n",
    "    s = s.strip('\"\\'' \"‚Äú‚Äù‚Äò‚Äô\")\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"utf-8\")\n",
    "    return re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "NOISE_NAMES   = {\"la poderosa\",\"la poderosa i\",\"la poderosa ii\",\"la pedrosa\"}\n",
    "VEHICLE_TERMS = {\"motorcycle\",\"motorbike\",\"bike\",\"bicycle\",\"boat\",\"ship\",\"raft\",\"car\",\"truck\",\"jeep\",\"bus\",\"van\"}\n",
    "\n",
    "def is_named_object(entity_text: str, sentence: str) -> bool:\n",
    "    ent_n = _norm_entity(entity_text)\n",
    "    if ent_n in NOISE_NAMES:\n",
    "        return True\n",
    "    doc = nlp(sentence)\n",
    "    idxs = [i for i,t in enumerate(doc) if (t.lemma_.lower() in VEHICLE_TERMS)]\n",
    "    for i in idxs:\n",
    "        L, R = max(0, i-6), min(len(doc), i+7)\n",
    "        if any(_norm_entity(t.text) == ent_n for t in doc[L:R]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_probable_metonymy(entity_text: str, sentence: str, label: str) -> bool:\n",
    "    if label not in {\"GPE\",\"COUNTRY\"}:\n",
    "        return False\n",
    "    doc = nlp(sentence)\n",
    "    ent_l = entity_text.lower()\n",
    "    cue_nouns = {\"government\",\"policy\",\"military\",\"regime\",\"parliament\",\"industry\",\"media\",\"press\",\"power\"}\n",
    "    has_cue = any(tok.lemma_.lower() in cue_nouns for tok in doc)\n",
    "    if not has_cue:\n",
    "        return False\n",
    "    idxs = [i for i,t in enumerate(doc) if ent_l in t.text.lower()]\n",
    "    return any(\n",
    "        any(abs(j - i) <= 5 for j,_ in enumerate(doc) if doc[j].lemma_.lower() in cue_nouns)\n",
    "        for i in idxs\n",
    "    )\n",
    "\n",
    "# Cache one spaCy Doc per sentence \n",
    "DOCS = {sid: nlp(text) for sid, text in SENTENCES_FOR_NER}  \n",
    "persons_by_sentence = {\n",
    "    sid: [ent.text for ent in DOCS[sid].ents if ent.label_ == \"PERSON\"]\n",
    "    for sid, _ in SENTENCES_FOR_NER  \n",
    "}\n",
    "\n",
    "# Global + manual person blacklist (kills Granado even if local PERSON not detected) \n",
    "GLOBAL_PERSONS: Set[str] = {p.lower().strip() for plist in persons_by_sentence.values() for p in plist}\n",
    "GLOBAL_PERSON_HEADS: Set[str] = {p.split()[-1] for p in GLOBAL_PERSONS if p}\n",
    "\n",
    "# Corpus-specific safety net\n",
    "PERSON_BLACKLIST: Set[str] = {\n",
    "    \"alberto granado\",\"granado\",\"ernesto\",\"ernesto guevara\",\"che\",\"guevara\"\n",
    "}\n",
    "\n",
    "def _looks_like_person_here(ent_text: str, sid: int) -> bool:\n",
    "    cand = ent_text.lower().strip()\n",
    "    persons = [p.lower() for p in persons_by_sentence.get(sid, [])]\n",
    "    if cand in persons:\n",
    "        return True\n",
    "    return any(fuzz.token_set_ratio(cand, p) >= 90 for p in persons)\n",
    "\n",
    "#  Main pipeline \n",
    "def combine_ner_gazetteer(sentences: List[Tuple[int, str]], gazetteer_set: Set[str]) -> List[Dict]:\n",
    "    allowed = {\"GPE\",\"LOC\",\"GAZETTEER\",\"FAC\",\"CITY\",\"STATE_OR_PROVINCE\",\"COUNTRY\"}\n",
    "    results: List[Dict] = []\n",
    "\n",
    "    for sid, text in tqdm(sentences, desc=\"NER + Gazetteer\"):\n",
    "        ents: List[Tuple[str,str,int,int]] = []\n",
    "        # NER first (keep these unless they fail the *light* checks)\n",
    "        ents += [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in DOCS[sid].ents]\n",
    "        if USE_STANZA and stanza_pipeline is not None:\n",
    "            try:\n",
    "                for sent in stanza_pipeline(text).sentences:\n",
    "                    for ent in getattr(sent, \"ents\", []):\n",
    "                        ents.append((ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Gazetteer hits\n",
    "        ents += match_gazetteer_safe(text)\n",
    "\n",
    "        keep = []\n",
    "        for ent_text, label, start, end in ents:\n",
    "            if label not in allowed:\n",
    "                continue\n",
    "            if not _valid_toponym(ent_text):\n",
    "                continue\n",
    "            if _person_prefix_rule(text, ent_text):\n",
    "                continue\n",
    "\n",
    "            low = ent_text.lower().strip()\n",
    "\n",
    "            # suppress gazetteer hits that are actually persons (local + global + manual)\n",
    "            if label == \"GAZETTEER\":\n",
    "                if _looks_like_person_here(ent_text, sid):\n",
    "                    continue\n",
    "                if low in GLOBAL_PERSONS or low in GLOBAL_PERSON_HEADS or low in PERSON_BLACKLIST:\n",
    "                    continue\n",
    "\n",
    "            # if similar to a PERSON mention, only suppress for gazetteer; keep NER\n",
    "            if any(fuzz.token_set_ratio(ent_text, p) >= 90 for p in persons_by_sentence.get(sid, [])):\n",
    "                if label == \"GAZETTEER\":\n",
    "                    continue\n",
    "\n",
    "            if is_named_object(ent_text, text):\n",
    "                continue\n",
    "            if label == \"GAZETTEER\" and not _ambiguous_singleton_ok(ent_text, text, gazetteer_set):\n",
    "                continue\n",
    "            if is_probable_metonymy(ent_text, text, \"GPE\" if label in {\"CITY\",\"STATE_OR_PROVINCE\",\"COUNTRY\"} else label):\n",
    "                continue\n",
    "\n",
    "            keep.append((ent_text, label, start, end))\n",
    "\n",
    "        # Per-sentence longest-match + strict dedup (exact + near-dup)\n",
    "        spans = []\n",
    "        if keep:\n",
    "            dfk = pd.DataFrame(\n",
    "                [{\"t\":t, \"l\":l, \"s\":s, \"e\":e, \"len\": e-s} for t,l,s,e in keep]\n",
    "            ).sort_values(\"len\", ascending=False)\n",
    "            for r in dfk.itertuples():\n",
    "                if not any(not (r.e <= s or r.s >= e) for _,_,s,e in spans):\n",
    "                    # avoid adding duplicates that are nearly the same string in same sentence\n",
    "                    if any(fuzz.token_set_ratio(r.t, t0) >= 95 for t0,_,_,_ in spans):\n",
    "                        continue\n",
    "                    spans.append((r.t, r.l, r.s, r.e))\n",
    "\n",
    "        for ent_text, label, start, end in spans:\n",
    "            results.append({\n",
    "                \"sentence_id\": sid,\n",
    "                \"entity\": ent_text,\n",
    "                \"entity_norm\": ent_text.lower().strip(),\n",
    "                \"label\": (\"GPE\" if label in {\"CITY\",\"STATE_OR_PROVINCE\",\"COUNTRY\"} else label),\n",
    "                \"start_char\": start,\n",
    "                \"end_char\": end,\n",
    "                \"sentence\": text,\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# Run pipeline + prune overlaps \n",
    "print(\"üîé Running ensemble NER + gazetteer matching (with metonymy & filters)...\")\n",
    "entities_combined = combine_ner_gazetteer(SENTENCES_FOR_NER, _PLACES)  # noqa: F821\n",
    "df_combined = pd.DataFrame(entities_combined)\n",
    "df_combined = remove_overlapping_shorter(df_combined)\n",
    "\n",
    "# Enrich with coordinates + country (diacritics-safe) \n",
    "def strip_diacritics(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKD\", s or \"\").encode(\"ascii\",\"ignore\").decode(\"utf-8\")\n",
    "\n",
    "gaz_rows = [{\n",
    "    \"name_lower\":   n,\n",
    "    \"name_stripped\":strip_diacritics(n),\n",
    "    \"lat\":          geo.get(\"lat\"),\n",
    "    \"lon\":          geo.get(\"lon\"),\n",
    "    \"country\":      geo.get(\"country\"),\n",
    "    \"country_code\": geo.get(\"country_code\"),\n",
    "} for n, geo in gazetteer.items()]\n",
    "gaz_df = pd.DataFrame(gaz_rows).drop_duplicates(subset=[\"name_lower\"])\n",
    "\n",
    "df_combined[\"entity_lower\"]    = df_combined[\"entity\"].str.lower()\n",
    "df_combined[\"entity_stripped\"] = df_combined[\"entity_lower\"].map(strip_diacritics)\n",
    "\n",
    "df_enriched = df_combined.merge(gaz_df, left_on=\"entity_lower\", right_on=\"name_lower\", how=\"left\")\n",
    "\n",
    "missing = df_enriched[\"lat\"].isna()\n",
    "if missing.any():\n",
    "    fallback = df_combined[missing].merge(\n",
    "        gaz_df, left_on=\"entity_stripped\", right_on=\"name_stripped\", how=\"left\"\n",
    "    )\n",
    "    for col in [\"lat\",\"lon\",\"country\",\"country_code\"]:\n",
    "        df_enriched.loc[missing, col] = fallback[col].values\n",
    "\n",
    "#  restrict to South America countries (prevents MX/Central America leakage) ---\n",
    "SOUTH_AM = {\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"BO\",\"EC\",\"BR\",\"PY\",\"UY\",\"GY\",\"SR\"}\n",
    "if \"country_code\" in df_enriched.columns:\n",
    "    before_sa = len(df_enriched)\n",
    "    df_enriched = df_enriched[df_enriched[\"country_code\"].isin(SOUTH_AM)].copy()\n",
    "    print(f\"üåé Restricted to South America: {before_sa} -> {len(df_enriched)} rows\")\n",
    "\n",
    "\n",
    "if df_enriched[\"country\"].isna().all():\n",
    "    print(\" Country enrichment missing for all rows ‚Äî check gazetteer build & GeoNames credentials.\")\n",
    "\n",
    "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "out_csv = \"outputs/geoparsing_ner_ensemble.csv\"\n",
    "df_enriched.drop(columns=[\"name_lower\",\"name_stripped\"], errors=\"ignore\").to_csv(out_csv, index=False)\n",
    "print(f\"Saved: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Using Block 7 output: outputs/geoparsing_ner_ensemble.csv  (SAMPLE=False)\n",
      "‚úÖ Enriched data saved ‚Üí outputs/geoparsing_final_enriched.csv  (406 rows)\n"
     ]
    }
   ],
   "source": [
    "# Block 8: Symbolic Enrichment (with Enhanced Metadata Extraction) ===\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0) Resolve inputs (config + presence)\n",
    "\n",
    "CFG_PATH = Path(\"config.yaml\")\n",
    "cfg = {}\n",
    "if CFG_PATH.exists():\n",
    "    with open(CFG_PATH) as f:\n",
    "        cfg = yaml.safe_load(f) or {}\n",
    "\n",
    "# Prefer full run if it exists; allow config override\n",
    "outputs = Path(\"outputs\")\n",
    "full_csv   = outputs / \"geoparsing_ner_ensemble.csv\"\n",
    "#sample_csv = outputs / \"geoparsing_ner_sample_test.csv\"\n",
    "\n",
    "#sample_only_cfg = bool(cfg.get(\"testing\", {}).get(\"sample_only\", False))\n",
    "#if not sample_only_cfg and full_csv.exists():\n",
    "    #in_path = full_csv\n",
    "    #SAMPLE = False\n",
    "#else:\n",
    "   # in_path = sample_csv\n",
    "    #SAMPLE = True\n",
    "\n",
    "#print(f\"Using Block 7 output: {in_path}  (SAMPLE={SAMPLE})\")\n",
    "\n",
    "\n",
    "# 1) Load gazetteer\n",
    "\n",
    "gaz_path = outputs / \"gazetteer_cities.json\"\n",
    "with open(gaz_path, \"r\") as f:\n",
    "    gazetteer = json.load(f)  # keys are lowercase\n",
    "gazetteer_set = set(gazetteer.keys())\n",
    "\n",
    "\n",
    "# 2) Load Block 7 result\n",
    "\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "# persons_in_sentence is serialized list -> parse safely\n",
    "if \"persons_in_sentence\" in df.columns:\n",
    "    def _safe_list(x):\n",
    "        if isinstance(x, list): return x\n",
    "        if isinstance(x, str) and x.strip().startswith(\"[\"):\n",
    "            try: return ast.literal_eval(x)\n",
    "            except Exception: return []\n",
    "        return [] if pd.isna(x) else [str(x)]\n",
    "    df[\"persons_in_sentence\"] = df[\"persons_in_sentence\"].apply(_safe_list)\n",
    "else:\n",
    "    df[\"persons_in_sentence\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "# 3) Helpers\n",
    "\n",
    "def _norm(text: str) -> str:\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text).strip()\n",
    "\n",
    "def get_lat(entity): return gazetteer.get(_norm(entity), {}).get(\"lat\")\n",
    "def get_lon(entity): return gazetteer.get(_norm(entity), {}).get(\"lon\")\n",
    "def country_valid(entity): return _norm(entity) in gazetteer_set\n",
    "\n",
    "# Named-object (vehicle/boat/etc.) signals from Block 7 logs (optional but helpful)\n",
    "obj_log = outputs / \"object_filtered.csv\"\n",
    "named_obj = set()\n",
    "if obj_log.exists():\n",
    "    try:\n",
    "        _obj = pd.read_csv(obj_log)\n",
    "        if \"entity\" in _obj.columns:\n",
    "            named_obj = set(_obj[\"entity\"].dropna().str.lower().unique())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "VEHICLE_TERMS = {\n",
    "    \"motorcycle\",\"motorbike\",\"bike\",\"bicycle\",\"moto\",\"motocicleta\",\n",
    "    \"boat\",\"ship\",\"barco\",\"lancha\",\"car\",\"truck\",\"jeep\",\"bus\",\"camion\",\"train\",\"plane\",\"avion\"\n",
    "}\n",
    "\n",
    "def is_named_object_context(entity: str, sentence: str) -> bool:\n",
    "    e = entity.lower().strip()\n",
    "    if e in named_obj:\n",
    "        return True\n",
    "    # quick context sweep for vehicle terms near the entity\n",
    "    doc = nlp(sentence)\n",
    "    # find any vehicle token and check a small window for the entity\n",
    "    veh_ix = [i for i,t in enumerate(doc) if t.lemma_.lower() in VEHICLE_TERMS or t.text.lower() in VEHICLE_TERMS]\n",
    "    for i in veh_ix:\n",
    "        L = max(0, i-6); R = min(len(doc), i+7)\n",
    "        if any(e in t.text.lower() for t in doc[L:R]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Movement / symbolic cues (lightweight, language-mixed)\n",
    "MOVEMENT_VERBS = {\n",
    "    \"travel\",\"go\",\"arrive\",\"leave\",\"depart\",\"walk\",\"ride\",\"sail\",\"drive\",\"cross\",\"reach\",\"head\",\"return\",\n",
    "    \"ir\",\"llegar\",\"salir\",\"partir\",\"caminar\",\"andar\",\"montar\",\"navegar\",\"conducir\",\"cruzar\",\"alcanzar\",\"volver\"\n",
    "}\n",
    "SYMBOLIC_VERBS = {\"govern\",\"rule\",\"dominate\",\"represent\",\"symbolize\",\"embody\",\"gobernar\",\"dominar\",\"representar\",\"simbolizar\",\"encarnar\"}\n",
    "\n",
    "def movement_verb_present(sentence: str, entity: str, persons: list[str]) -> bool:\n",
    "    if not isinstance(sentence, str): return False\n",
    "    doc = nlp(sentence)\n",
    "    ent_l = entity.lower()\n",
    "    ppl = [p.lower() for p in (persons or [])]\n",
    "    for tok in doc:\n",
    "        if tok.lemma_.lower() in MOVEMENT_VERBS:\n",
    "            win = [tok] + list(tok.children) + [tok.head]\n",
    "            if any(ent_l in t.text.lower() for t in win) or any(any(p in t.text.lower() for p in ppl) for t in win):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def symbolic_context(sentence: str, entity: str, persons: list[str]) -> bool:\n",
    "    if not isinstance(sentence, str): return False\n",
    "    doc = nlp(sentence)\n",
    "    ent_l = entity.lower()\n",
    "    ppl = [p.lower() for p in (persons or [])]\n",
    "    for tok in doc:\n",
    "        if tok.lemma_.lower() in SYMBOLIC_VERBS:\n",
    "            win = [tok] + list(tok.children) + [tok.head]\n",
    "            if any(ent_l in t.text.lower() for t in win) or any(any(p in t.text.lower() for p in ppl) for t in win):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Reuse Block‚Äë7 metonymy if available; otherwise define here\n",
    "if \"metonymy_flagged\" in df.columns:\n",
    "    _has_block7_meta = True\n",
    "else:\n",
    "    _has_block7_meta = False\n",
    "    CUE_WORDS = {\n",
    "        \"government\",\"policy\",\"military\",\"regime\",\"parliament\",\"industry\",\n",
    "        \"media\",\"revolution\",\"capital\",\"press\",\"organization\",\"power\"\n",
    "    }\n",
    "    def is_probable_metonymy(entity_text: str, sentence: str) -> bool:\n",
    "        doc = nlp(sentence)\n",
    "        ent_l = entity_text.lower()\n",
    "        entity_tokens = [t for t in doc if ent_l in t.text.lower()]\n",
    "        for token in doc:\n",
    "            if token.text.lower() in CUE_WORDS and token.pos_ == \"NOUN\":\n",
    "                for ent_token in entity_tokens:\n",
    "                    if abs(token.i - ent_token.i) <= 10:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# 4) Compute enrichment columns\n",
    "\n",
    "df[\"lat\"] = df[\"entity\"].apply(get_lat)\n",
    "df[\"lon\"] = df[\"entity\"].apply(get_lon)\n",
    "df[\"country_valid\"] = df[\"entity\"].apply(country_valid)\n",
    "\n",
    "# named object context flag (helps keep ‚ÄúLa Poderosa/Pedrosa‚Äù out downstream)\n",
    "df[\"named_object_flag\"] = df.apply(lambda r: is_named_object_context(r[\"entity\"], r[\"sentence\"]), axis=1)\n",
    "\n",
    "df[\"movement_verb_present\"] = df.apply(lambda r: movement_verb_present(r[\"sentence\"], r[\"entity\"], r[\"persons_in_sentence\"]), axis=1)\n",
    "df[\"symbolic_context\"] = df.apply(lambda r: symbolic_context(r[\"sentence\"], r[\"entity\"], r[\"persons_in_sentence\"]), axis=1)\n",
    "\n",
    "if _has_block7_meta:\n",
    "    # Already set by Block 7\n",
    "    df[\"metonymy_flagged\"] = df[\"metonymy_flagged\"].astype(bool)\n",
    "else:\n",
    "    df[\"metonymy_flagged\"] = df.apply(lambda r: is_probable_metonymy(r[\"entity\"], r[\"sentence\"]), axis=1)\n",
    "\n",
    "\n",
    "# 5) Lightweight final label (no leakage into ML features)\n",
    "\n",
    "# Note: this is for analysis & downstream use; ML in Block 10 will not use it as a feature.\n",
    "def final_label(row):\n",
    "    # Named objects trump everything: treat as NOISE\n",
    "    if row[\"named_object_flag\"]:\n",
    "        return \"NOISE\"\n",
    "    # Symbolic if symbolic cue or movement alongside metonymy\n",
    "    if row[\"symbolic_context\"] or (row[\"movement_verb_present\"] and row[\"metonymy_flagged\"]):\n",
    "        return \"SYMBOLIC\"\n",
    "    # Literal if gazetteer-backed and not metonymic\n",
    "    if row[\"country_valid\"] and not row[\"metonymy_flagged\"]:\n",
    "        return \"LITERAL\"\n",
    "    return \"NOISE\"\n",
    "\n",
    "df[\"final_label\"] = df.apply(final_label, axis=1)\n",
    "\n",
    "\n",
    "# 6) Metadata: year & transport\n",
    "\n",
    "def extract_year(sentence: str):\n",
    "    if not isinstance(sentence, str): return None\n",
    "    m = re.search(r\"\\b(19\\d{2}|20\\d{2})\\b\", sentence)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "TRANSPORT = {\n",
    "    \"motorcycle\",\"motorbike\",\"bike\",\"bicycle\",\"bus\",\"truck\",\"car\",\"jeep\",\"boat\",\"ship\",\"raft\",\"train\",\"plane\",\"foot\",\n",
    "    \"moto\",\"bicicleta\",\"bus\",\"camion\",\"coche\",\"auto\",\"jeep\",\"barco\",\"lancha\",\"tren\",\"avion\",\"a pie\"\n",
    "}\n",
    "def extract_transport(sentence: str):\n",
    "    if not isinstance(sentence, str): return None\n",
    "    doc = nlp(sentence)\n",
    "    found = {t.text.lower() for t in doc if t.lemma_.lower() in TRANSPORT and t.pos_ == \"NOUN\"}\n",
    "    return \", \".join(sorted(found)) if found else None\n",
    "\n",
    "df[\"year\"] = df[\"sentence\"].apply(extract_year)\n",
    "df[\"transport\"] = df[\"sentence\"].apply(extract_transport)\n",
    "df[\"people_involved\"] = df[\"persons_in_sentence\"].apply(lambda xs: \", \".join(xs) if xs else None)\n",
    "\n",
    "# Keep columns tidy if present\n",
    "preferred_cols = [\n",
    "    \"sentence_id\",\"entity\",\"entity_norm\",\"label\",\"lat\",\"lon\",\"country_valid\",\n",
    "    \"symbolic_context\",\"movement_verb_present\",\"metonymy_flagged\",\"named_object_flag\",\n",
    "    \"final_label\",\"year\",\"transport\",\"start_char\",\"end_char\",\"sentence\",\"persons_in_sentence\"\n",
    "]\n",
    "df = df[[c for c in preferred_cols if c in df.columns]]\n",
    "\n",
    "\n",
    "# 7) Save\n",
    "\n",
    "out_path = outputs / (\"geoparsing_final_enriched.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"Enriched data saved ‚Üí {out_path}  ({len(df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3fbf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'entity_is_valid'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'entity_is_valid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33moutputs/geoparsing_final_enriched.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m df = df[df[\u001b[33m\"\u001b[39m\u001b[33mfinal_label\u001b[39m\u001b[33m\"\u001b[39m].isin([\u001b[33m\"\u001b[39m\u001b[33mLITERAL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSYMBOLIC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNOISE\u001b[39m\u001b[33m\"\u001b[39m])].copy()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mentity_is_valid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m == \u001b[38;5;28;01mTrue\u001b[39;00m].copy()\n\u001b[32m      7\u001b[39m df = df[~((df[\u001b[33m\"\u001b[39m\u001b[33mfinal_label\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mLITERAL\u001b[39m\u001b[33m\"\u001b[39m) & (df[\u001b[33m\"\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m\"\u001b[39m].isna() | df[\u001b[33m\"\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m\"\u001b[39m].isna()))].copy()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# === Gazetteer ===\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'entity_is_valid'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Block 10: ML Classification (Clean, Symbolic-aware, Person-aware) ===\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "df = df[df[\"final_label\"].isin([\"LITERAL\", \"SYMBOLIC\", \"NOISE\"])].copy()\n",
    "df = df[df[\"entity_is_valid\"] == True].copy()\n",
    "df = df[~((df[\"final_label\"] == \"LITERAL\") & (df[\"lat\"].isna() | df[\"lon\"].isna()))].copy()\n",
    "\n",
    "# Gazetteer\n",
    "with open(\"outputs/gazetteer_cities.json\", \"r\") as f:\n",
    "    gazetteer = set(json.load(f).keys())\n",
    "\n",
    "# Load SpaCy model with vectors\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "except:\n",
    "    raise ValueError(\"Model 'en_core_web_md' not found. Run: python -m spacy download en_core_web_md\")\n",
    "\n",
    "# Regex helpers\n",
    "month_regex = r\"\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b\"\n",
    "year_regex = r\"\\b(19|20)\\d{2}\\b\"\n",
    "def contains_date_mention(text: str) -> bool:\n",
    "    return bool(re.search(month_regex, text.lower())) or bool(re.search(year_regex, text.lower()))\n",
    "\n",
    "REGION_LIKE = {\n",
    "    \"andes\", \"amazon\", \"patagonia\", \"altiplano\", \"la plata\",\n",
    "    \"pampas\", \"amazonas\", \"chaco\", \"conosur\"\n",
    "}\n",
    "def is_region_like(entity): return entity.lower().strip() in REGION_LIKE\n",
    "\n",
    "historical_keywords = {\n",
    "    \"revolution\", \"regime\", \"independence\", \"liberation\", \"martyr\",\n",
    "    \"battle\", \"hero\", \"military\", \"colonial\", \"freedom\", \"leader\", \"movement\"\n",
    "}\n",
    "def verb_density(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return sum(1 for t in doc if t.pos_ == \"VERB\") / (len(doc) or 1)\n",
    "\n",
    "def name_is_person(entity):\n",
    "    doc = nlp(entity)\n",
    "    return any(ent.label_ == \"PERSON\" for ent in doc.ents)\n",
    "\n",
    "# Feature Extractor \n",
    "def extract_final_features(row):\n",
    "    entity = row[\"entity\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "    person_names = row[\"persons_in_sentence\"] if isinstance(row[\"persons_in_sentence\"], list) else []\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    ent_doc = nlp(entity)\n",
    "    sim = doc.similarity(ent_doc) if doc.vector_norm and ent_doc.vector_norm else 0.0\n",
    "\n",
    "    return {\n",
    "        \"entity_len\": len(entity),\n",
    "        \"sentence_len\": len(sentence),\n",
    "        \"entity_capital_ratio\": sum(c.isupper() for c in entity) / (len(entity) or 1),\n",
    "        \"starts_with_cap\": entity[0].isupper(),\n",
    "        \"has_digits\": any(c.isdigit() for c in entity),\n",
    "        \"in_quotes\": '\"' in sentence or \"'\" in sentence,\n",
    "        \"person_like_fuzzy\": any(fuzz.token_set_ratio(entity.lower(), p.lower()) > 85 for p in person_names),\n",
    "        \"mentions_date\": contains_date_mention(sentence),\n",
    "        \"gazetteer_match\": int(entity.lower() in gazetteer),\n",
    "        \"entity_position_ratio\": row[\"start_char\"] / (len(sentence) or 1),\n",
    "        \"entity_sentence_sim\": sim,\n",
    "        \"historical_context\": any(word in sentence.lower() for word in historical_keywords),\n",
    "        \"is_region_like\": is_region_like(entity),\n",
    "        \"verb_density\": verb_density(sentence),\n",
    "        \"name_is_person\": name_is_person(entity)\n",
    "    }\n",
    "\n",
    "# Feature Extraction\n",
    "X_dict_list = []\n",
    "valid_rows = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        X_dict_list.append(extract_final_features(row))\n",
    "        valid_rows.append(i)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped row {i}: {e}\")\n",
    "\n",
    "df = df.loc[valid_rows].reset_index(drop=True)\n",
    "y = df[\"final_label\"].reset_index(drop=True)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Vectorize features\n",
    "vec = DictVectorizer(sparse=False)\n",
    "X = vec.fit_transform(X_dict_list)\n",
    "\n",
    "# Outlier filtering\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "mask = iso.fit_predict(X) == 1\n",
    "X = X[mask]\n",
    "y_encoded = y_encoded[mask]\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\" Outlier removal: Kept {len(X)} rows\")\n",
    "\n",
    "# Save indices to track test rows\n",
    "df[\"original_index\"] = df.index\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y_encoded, df[\"original_index\"].values, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "clf = XGBClassifier(n_estimators=120, use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Decode predictions\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred_labels))\n",
    "\n",
    "# Feature Importances\n",
    "importances = sorted(\n",
    "    zip(vec.get_feature_names_out(), clf.feature_importances_),\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh([f for f, _ in importances], [imp for _, imp in importances])\n",
    "plt.title(\"Feature Importances (Final Model)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save test predictions\n",
    "df_test = df.loc[idx_test].copy()\n",
    "df_test[\"ml_prediction\"] = y_pred_labels\n",
    "df_test.to_csv(\"outputs/geoparser_ml_predictions_leakfree.csv\", index=False)\n",
    "print(\" Saved final predictions to outputs/geoparser_ml_predictions_leakfree.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63008e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/geoparser_ml_predictions_leakfree.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m year: parts.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÖ Year: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m<br>\u001b[39m\u001b[33m\"\u001b[39m.join(parts) \u001b[38;5;28;01mif\u001b[39;00m parts \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m‚ÑπÔ∏è No metadata\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs/geoparser_ml_predictions_leakfree.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m df = df[(df[\u001b[33m\"\u001b[39m\u001b[33mml_prediction\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mLITERAL\u001b[39m\u001b[33m\"\u001b[39m) & df[\u001b[33m\"\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m\"\u001b[39m].notna() & df[\u001b[33m\"\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m\"\u001b[39m].notna()].copy()\n\u001b[32m     14\u001b[39m df.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BA-code/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'outputs/geoparser_ml_predictions_leakfree.csv'"
     ]
    }
   ],
   "source": [
    "# Block 12: Interactive Map with Hover Metadata \n",
    "def generate_hover_info(row):\n",
    "    people = row.get(\"people_involved\", \"\")\n",
    "    transport = row.get(\"transport\", \"\")\n",
    "    year = row.get(\"year\", \"\")\n",
    "    parts = []\n",
    "    if people: parts.append(f\"üë• People: {people}\")\n",
    "    if transport: parts.append(f\"üöó Transport: {transport}\")\n",
    "    if year: parts.append(f\"üìÖ Year: {year}\")\n",
    "    return \"<br>\".join(parts) if parts else \"‚ÑπÔ∏è No metadata\"\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparser_ml_predictions_leakfree.csv\")\n",
    "df = df[(df[\"ml_prediction\"] == \"LITERAL\") & df[\"lat\"].notna() & df[\"lon\"].notna()].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"hover_text\"] = df.apply(generate_hover_info, axis=1)\n",
    "\n",
    "start_lat, start_lon = df.iloc[0][\"lat\"], df.iloc[0][\"lon\"]\n",
    "m = folium.Map(location=[start_lat, start_lon], zoom_start=4, tiles=\"CartoDB positron\")\n",
    "\n",
    "route_coords = df[[\"lat\", \"lon\"]].values.tolist()\n",
    "AntPath(route_coords, color=\"red\", weight=3, delay=1000).add_to(m)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"lat\"], row[\"lon\"]],\n",
    "        radius=6,\n",
    "        color=\"blue\",\n",
    "        fill=True,\n",
    "        fill_opacity=0.8,\n",
    "        popup=folium.Popup(row[\"hover_text\"], max_width=300),\n",
    "        tooltip=f\"#{i+1}: {row['entity']}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"outputs/interactive_geoparsing_map.html\")\n",
    "print(\"Interactive map saved to outputs/interactive_geoparsing_map.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
