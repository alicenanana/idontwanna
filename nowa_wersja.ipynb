{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfdb186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package names to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in ./.venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.24 in ./.venv/lib/python3.13/site-packages (from geopandas) (2.1.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in ./.venv/lib/python3.13/site-packages (from geopandas) (0.11.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from geopandas) (25.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in ./.venv/lib/python3.13/site-packages (from geopandas) (2.2.3)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in ./.venv/lib/python3.13/site-packages (from geopandas) (3.7.2)\n",
      "Requirement already satisfied: shapely>=2.0.0 in ./.venv/lib/python3.13/site-packages (from geopandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=2.0.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=2.0.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from pyogrio>=0.7.2->geopandas) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: geopy in ./.venv/lib/python3.13/site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in ./.venv/lib/python3.13/site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: stanza in ./.venv/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: emoji in ./.venv/lib/python3.13/site-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from stanza) (2.1.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in ./.venv/lib/python3.13/site-packages (from stanza) (6.31.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from stanza) (2.32.4)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from stanza) (3.5)\n",
      "Requirement already satisfied: torch>=1.3.0 in ./.venv/lib/python3.13/site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (75.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->stanza) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: geodatasets in ./.venv/lib/python3.13/site-packages (2024.8.0)\n",
      "Requirement already satisfied: pooch in ./.venv/lib/python3.13/site-packages (from geodatasets) (1.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from pooch->geodatasets) (4.3.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from pooch->geodatasets) (25.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.13/site-packages (from pooch->geodatasets) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch->geodatasets) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: folium in ./.venv/lib/python3.13/site-packages (0.20.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in ./.venv/lib/python3.13/site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in ./.venv/lib/python3.13/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from folium) (2.1.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from folium) (2.32.4)\n",
      "Requirement already satisfied: xyzservices in ./.venv/lib/python3.13/site-packages (from folium) (2025.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->folium) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->folium) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->folium) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in ./.venv/lib/python3.13/site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from xgboost) (2.1.2)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.13/site-packages (from xgboost) (1.16.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.13/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.13/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.13/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.13/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.13/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.13/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.13/site-packages (from spacy) (2.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.13/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from spacy) (75.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.13/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fuzzywuzzy in ./.venv/lib/python3.13/site-packages (0.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: contractions in ./.venv/lib/python3.13/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./.venv/lib/python3.13/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in ./.venv/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in ./.venv/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rapidfuzz in ./.venv/lib/python3.13/site-packages (3.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pycountry in ./.venv/lib/python3.13/site-packages (24.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Pre-Block: Downloads & Setup \n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"framenet_v17\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('names')\n",
    "\n",
    "%pip install geopandas\n",
    "%pip install geopy\n",
    "%pip install stanza\n",
    "%pip install geodatasets\n",
    "%pip install folium\n",
    "%pip install xgboost\n",
    "%pip install scikit-learn\n",
    "%pip install spacy\n",
    "%pip install fuzzywuzzy\n",
    "%pip install contractions\n",
    "%pip install rapidfuzz\n",
    "%pip install pycountry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ccd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline: ['sentencizer']\n",
      "spaCy pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Imports \n",
    "\n",
    "\n",
    "import os, re, time, json, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import yaml\n",
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "\n",
    "# Data / ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Geo / viz\n",
    "import folium\n",
    "from folium.plugins import AntPath\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NLP / NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, names, wordnet as wn, framenet as fn\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "# Project helpers\n",
    "from geoparser import nlp_helpers as nh\n",
    "import importlib; importlib.reload(nh)\n",
    "from geoparser import enrichment_helpers\n",
    "\n",
    "nlp, stanza_pipeline = nh.init_nlp(lang=\"es\", with_stanza=False)\n",
    "\n",
    "print(\"spaCy pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# stopwords for mixed ES/PT/EN corpora:\n",
    "stopset = nh.get_stopwords(nlp, langs=[\"es\", \"pt\", \"en\"])\n",
    "\n",
    "\n",
    "\n",
    "# Ensure a real English model is installed and loaded\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "    except Exception:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError as e:\n",
    "    raise RuntimeError(\n",
    "        \"spaCy model not installed. Run: \"\n",
    "        \"python -m spacy download en_core_web_md  (or en_core_web_sm)\"\n",
    "    ) from e\n",
    "\n",
    "print(\"spaCy pipeline:\", nlp.pipe_names)\n",
    "if set(nlp.pipe_names) <= {\"sentencizer\",\"senter\"}:\n",
    "    raise RuntimeError(\n",
    "        \"spaCy model lacks tagger/ner; install en_core_web_md or en_core_web_sm.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d76d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 2: Extract text from PDF using PyMuPDF; pages 28-148\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str, start_page: int, end_page: int) -> str:\n",
    "    \"\"\"Extracts and returns text from a PDF given a path and page range.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = doc[start_page:end_page]\n",
    "    return \"\\n\".join(page.get_text() for page in pages)\n",
    "\n",
    "\n",
    "\n",
    "#Load configuration from YAML file\n",
    "def load_config(config_path: str = \"config.yaml\") -> dict:\n",
    "    \"\"\"Loads YAML configuration file and returns it as a dictionary.\"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        \n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "raw_text = extract_text_from_pdf(pdf_conf[\"path\"], pdf_conf[\"start_page\"], pdf_conf[\"end_page\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e33e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 3: Text Preprocessing Functions\n",
    "\n",
    "from geoparser.nlp_helpers import (\n",
    "    init_nlp,\n",
    "    get_stopwords,\n",
    "    tag_named_entities,\n",
    "    extract_text_from_pdf,\n",
    "    load_config,\n",
    "    normalize_punctuation,\n",
    "    clean_light,\n",
    "    preprocess_text,\n",
    "    segment_sentences,\n",
    "    clean_heavy,\n",
    "    load_filters,\n",
    "    normalize_diacritics,\n",
    "    apply_ocr_replacements,\n",
    "    is_caption_line,\n",
    "    filter_raw_sentences\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2737feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned outputs saved:\n",
      "- Geoparsing (light): outputs/cleaned_MotorcycleDiaries_geoparsing.txt\n",
      "- NLP prep (heavy): outputs/cleaned_MotorcycleDiaries_nlp.txt\n",
      "sentence_data prepared with 2007 narrative sentences.\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Clean and Save Tagged + NLP Versions \n",
    "\n",
    "# Load config\n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "gaz_conf = config[\"gazetteer\"]\n",
    "\n",
    "# Output filenames based on input PDF\n",
    "base_name = Path(pdf_conf[\"path\"]).stem\n",
    "tagged_path = Path(\"outputs\") / f\"cleaned_{base_name}_geoparsing.txt\"\n",
    "heavy_path = Path(\"outputs\") / f\"cleaned_{base_name}_nlp.txt\"\n",
    "\n",
    "# Create outputs dir if needed\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# 🧹 Clean raw text (light and heavy)\n",
    "light_cleaned = clean_light(raw_text)\n",
    "sentences = segment_sentences(light_cleaned, nlp)\n",
    "sentences_with_tags = [f\"[SENT {i+1}] {s}\" for i, s in enumerate(sentences)]\n",
    "\n",
    "all_stops = get_stopwords(nlp)\n",
    "heavy_cleaned = clean_heavy(light_cleaned, nlp, all_stops)\n",
    "\n",
    "# Save both cleaned versions (one seems by this stage to be unnecessary, can as well drop the second pdf creation)\n",
    "with open(tagged_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sentences_with_tags))\n",
    "\n",
    "with open(heavy_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(heavy_cleaned)\n",
    "\n",
    "print(\"Cleaned outputs saved:\")\n",
    "print(f\"- Geoparsing (light): {tagged_path}\")\n",
    "print(f\"- NLP prep (heavy): {heavy_path}\")\n",
    "\n",
    "# Prepare sentence_data for Block 7\n",
    "sentence_data = [(i, sent.strip()) for i, sent in enumerate(sentences)]\n",
    "print(f\"sentence_data prepared with {len(sentence_data)} narrative sentences.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8272556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 227 symbolic verb forms from WordNet.\n",
      "Loaded 238 movement verb forms from WordNet.\n"
     ]
    }
   ],
   "source": [
    "# Build symbolic verb lexicon from WordNet (only once)\n",
    "\n",
    "\n",
    "def get_symbolic_verb_synonyms():\n",
    "    base_words = [\n",
    "        \"dream\", \"hope\", \"struggle\", \"escape\", \"resist\", \"believe\", \"follow\", \n",
    "        \"ride\", \"rebel\", \"fight\", \"flee\", \"live\", \"return\", \"envision\", \"imagine\", \"create\", \"inspire\", \"transform\", \"change\", \"grow\"\n",
    "    ]\n",
    "    synonyms = set()\n",
    "    for word in base_words:\n",
    "        for syn in wordnet.synsets(word, pos=wordnet.VERB):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "    return synonyms\n",
    "\n",
    "from nltk.corpus import wordnet # Ensure WordNet is available, \n",
    "# Save the expanded verb set to reuse\n",
    "SYMBOLIC_VERBS = get_symbolic_verb_synonyms()\n",
    "print(f\"Loaded {len(SYMBOLIC_VERBS)} symbolic verb forms from WordNet.\")\n",
    "\n",
    "# Build movement verb list using WordNet\n",
    "\n",
    "def get_movement_verbs():\n",
    "    base = [\"go\", \"move\", \"travel\", \"walk\", \"drive\", \"ride\", \"arrive\", \"depart\", \"leave\", \"return\", \"cross\", \"fly\", \"sail\", \"swim\", \"follow\",\"hike\"]\n",
    "    move_verbs = set()\n",
    "    for word in base:\n",
    "        for syn in wordnet.synsets(word, pos=wordnet.VERB):\n",
    "            for lemma in syn.lemmas():\n",
    "                move_verbs.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "                \n",
    "    return move_verbs\n",
    "\n",
    "MOVEMENT_VERBS = get_movement_verbs()\n",
    "print(f\"Loaded {len(MOVEMENT_VERBS)} movement verb forms from WordNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41623c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gazetteer from GeoNames (config-driven)…\n",
      "Downloading cities from GeoNames...\n",
      "AR: SSL error on HTTPS; falling back to HTTP…\n",
      "AR: Loaded 1000 cities (kept most-populous per name).\n",
      "CL: Loaded 1000 cities (kept most-populous per name).\n",
      "PE: Loaded 1000 cities (kept most-populous per name).\n",
      "CO: Loaded 1000 cities (kept most-populous per name).\n",
      "VE: Loaded 1000 cities (kept most-populous per name).\n",
      "BO: Loaded 1000 cities (kept most-populous per name).\n",
      "EC: Loaded 1000 cities (kept most-populous per name).\n",
      "PA: Loaded 1000 cities (kept most-populous per name).\n",
      "CR: Loaded 1000 cities (kept most-populous per name).\n",
      "GT: Loaded 1000 cities (kept most-populous per name).\n",
      "MX: Loaded 1000 cities (kept most-populous per name).\n",
      "CU: Loaded 1000 cities (kept most-populous per name).\n",
      "BR: Loaded 1000 cities (kept most-populous per name).\n",
      "GY: Loaded 906 cities (kept most-populous per name).\n",
      "PY: Loaded 1000 cities (kept most-populous per name).\n",
      "SR: Loaded 548 cities (kept most-populous per name).\n",
      "UY: Loaded 1000 cities (kept most-populous per name).\n",
      "HN: Loaded 1000 cities (kept most-populous per name).\n",
      "SV: Loaded 1000 cities (kept most-populous per name).\n",
      "NI: Loaded 1000 cities (kept most-populous per name).\n",
      "DO: Loaded 1000 cities (kept most-populous per name).\n",
      "HT: Loaded 1000 cities (kept most-populous per name).\n",
      "Total unique names in gazetteer: 15632\n",
      "Gazetteer built and saved with coordinates.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Block 6 preamble: make sure gazetteer helpers are importable ---\n",
    "import sys, json, yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on sys.path so 'geoparser/...' resolves\n",
    "def _ensure_project_root_on_path() -> None:\n",
    "    here = Path.cwd()\n",
    "    for p in [here, *list(here.parents)][:5]:\n",
    "        if (p / \"geoparser\" / \"gazetteer_helpers.py\").exists():\n",
    "            if str(p) not in sys.path:\n",
    "                sys.path.insert(0, str(p))\n",
    "            return\n",
    "_ensure_project_root_on_path()\n",
    "\n",
    "# Try normal import\n",
    "try:\n",
    "    from geoparser.gazetteer_helpers import build_gazetteer_from_conf, build_gazetteer  # type: ignore\n",
    "except ImportError:\n",
    "    # Fallback: same-dir module name\n",
    "    try:\n",
    "        from gazetteer_helpers import build_gazetteer_from_conf, build_gazetteer  # type: ignore\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Couldn't import gazetteer helpers. \"\n",
    "            \"Check that 'geoparser/gazetteer_helpers.py' exists AND 'geoparser/__init__.py' is present.\"\n",
    "        ) from e\n",
    "\n",
    "# If the module exports only build_gazetteer, create a tiny wrapper so Block 6 can call the same name\n",
    "if \"build_gazetteer_from_conf\" not in globals():\n",
    "    def build_gazetteer_from_conf(gconf: dict):\n",
    "        return build_gazetteer(\n",
    "            username=gconf[\"username\"],\n",
    "            countries=gconf[\"countries\"],\n",
    "            max_rows=gconf.get(\"max_rows\", 1000),\n",
    "            host=gconf.get(\"host\", \"api.geonames.org\"),\n",
    "            https=bool(gconf.get(\"https\", False)),\n",
    "            page_size=gconf.get(\"page_size\", 1000),\n",
    "            timeout=gconf.get(\"timeout\", 20),\n",
    "            retries=gconf.get(\"retries\", 4),\n",
    "            backoff_base=gconf.get(\"backoff_base\", 1.5),\n",
    "            sleep_between=gconf.get(\"sleep_between\", 0.6),\n",
    "        )\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "gaz_conf = config[\"gazetteer\"]\n",
    "\n",
    "gazetteer_path = Path(\"outputs/gazetteer_cities.json\")\n",
    "\n",
    "if gaz_conf.get(\"force_rebuild\", False) or not gazetteer_path.exists():\n",
    "    print(\"Building gazetteer from GeoNames (config-driven)…\")\n",
    "    gazetteer = build_gazetteer_from_conf(gaz_conf)\n",
    "    gazetteer_path.write_text(json.dumps(gazetteer, ensure_ascii=False, indent=2))\n",
    "    print(\"Gazetteer built and saved with coordinates.\")\n",
    "else:\n",
    "    print(\"Found existing gazetteer file, loading…\")\n",
    "    gazetteer = json.loads(gazetteer_path.read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c500c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-filtered sentences: kept 1971 / 2007\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"config.yaml\")\n",
    "filters = load_filters(cfg)\n",
    "\n",
    "# Apply OCR fixes to original text (not lowercasing)\n",
    "if isinstance(sentence_data, list) and sentence_data and isinstance(sentence_data[0], tuple):\n",
    "    sentence_data_fixed = []\n",
    "    for sid, txt in sentence_data:\n",
    "        txt2 = apply_ocr_replacements(txt, filters[\"ocr_replacements\"])\n",
    "        sentence_data_fixed.append((sid, txt2))\n",
    "else:\n",
    "    sentence_data_fixed = [\n",
    "        apply_ocr_replacements(txt, filters[\"ocr_replacements\"]) for txt in sentence_data\n",
    "    ]\n",
    "\n",
    "# Drop footnotes/captions/sections based on config / one of the initial problems that caused a lot of noise \n",
    "sentence_data_prefiltered = filter_raw_sentences(sentence_data_fixed, filters)\n",
    "\n",
    "print(f\"Pre-filtered sentences: kept {len(sentence_data_prefiltered)} / {len(sentence_data)}\")\n",
    "SENTENCES_FOR_NER = sentence_data_prefiltered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d70be0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "💾 Loaded gazetteer cache: 15632 names\n",
      "🔎 Running ensemble NER + gazetteer matching (with metonymy & filters)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER + Gazetteer: 100%|██████████| 1971/1971 [00:48<00:00, 40.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↪️ dropped country-only mentions (defensive): 98\n",
      "↪️ pruned oceans/regions: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enrich + disambiguate: 100%|██████████| 258/258 [00:04<00:00, 57.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↪️ dropped unresolved facilities: 1\n",
      "Saved: outputs/geoparsing_ner_ensemble.csv  (231 rows) | pruned phrases: 26 | low-evidence nulled: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Block 7: NER + Gazetteer with Metonymy Filtering (Country-Aware, Disambiguation-Scored, Final Tweaks) ===\n",
    "from __future__ import annotations\n",
    "\n",
    "# stdlib\n",
    "from pathlib import Path\n",
    "import re, json, unicodedata\n",
    "from typing import List, Dict, Set, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# third-party\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz, process\n",
    "import spacy\n",
    "\n",
    "# project helpers\n",
    "from geoparser.nlp_helpers import (\n",
    "    init_nlp, load_config, get_stopwords\n",
    ")\n",
    "from geoparser.gazetteer_helpers import (\n",
    "    build_gazetteer, gazetteer_names,\n",
    "    build_gazetteer_patterns, match_gazetteer_precompiled,\n",
    "    remove_overlapping_shorter\n",
    ")\n",
    "\n",
    "# ---------------------- NLP & Config ----------------------\n",
    "try:\n",
    "    nlp  # noqa: F821\n",
    "except NameError:\n",
    "    nlp, stanza_pipeline = init_nlp(lang=\"en\", prefer=[\"en_core_web_md\", \"en_core_web_sm\"], with_stanza=False)\n",
    "\n",
    "USE_STANZA = bool(globals().get(\"stanza_pipeline\"))\n",
    "print(\"spaCy pipeline:\", getattr(nlp, \"pipe_names\", []))\n",
    "if set(getattr(nlp, \"pipe_names\", [])) <= {\"sentencizer\", \"senter\"}:\n",
    "    raise RuntimeError(\"spaCy model lacks tagger/ner. Load 'en_core_web_md' or 'en_core_web_sm' before Block 7.\")\n",
    "\n",
    "try:\n",
    "    cfg  # noqa: F821\n",
    "except NameError:\n",
    "    cfg = load_config(\"config.yaml\")\n",
    "\n",
    "stop_words = get_stopwords(nlp, langs=[\"en\", \"es\", \"pt\"])\n",
    "\n",
    "# ---------------------- Gazetteer cache I/O ----------------------\n",
    "CACHE_PATH = Path(\"outputs/geonames_cache.json\")\n",
    "CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _load_gazetteer_cache(path: Path) -> Dict[str, Dict]:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            return {row[\"name\"].lower(): {k: v for k, v in row.items() if k != \"name\"} for row in data if \"name\" in row}\n",
    "        if isinstance(data, dict):\n",
    "            return {k.lower(): v for k, v in data.items()}\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "def _save_gazetteer_cache(g: Dict[str, Dict], path: Path) -> None:\n",
    "    try:\n",
    "        rows = [{\"name\": k, **v} for k, v in g.items()]\n",
    "        path.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"💾 Saved gazetteer cache: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cache save failed: {e}\")\n",
    "\n",
    "# SAFETY: ensure 'gazetteer' is dict\n",
    "try:\n",
    "    gazetteer  # noqa: F821\n",
    "    if not isinstance(gazetteer, dict):\n",
    "        print(f\"⚠️ Detected stale gazetteer of type {type(gazetteer).__name__}; resetting.\")\n",
    "        gazetteer = {}\n",
    "except NameError:\n",
    "    gazetteer = {}\n",
    "\n",
    "gcfg = cfg.get(\"gazetteer\", {})\n",
    "_cache = _load_gazetteer_cache(CACHE_PATH)\n",
    "gazetteer_loaded_from_cache = False\n",
    "\n",
    "if (_cache):\n",
    "    gazetteer = _cache\n",
    "    gazetteer_loaded_from_cache = True\n",
    "    print(f\"💾 Loaded gazetteer cache: {len(gazetteer)} names\")\n",
    "else:\n",
    "    print(\"↺ Building gazetteer from source…\")\n",
    "    try:\n",
    "        candidate = build_gazetteer(\n",
    "            username=gcfg.get(\"username\", \"\"),\n",
    "            countries=gcfg.get(\"countries\", []),\n",
    "            max_rows=int(gcfg.get(\"max_rows\", 1000))\n",
    "        )\n",
    "        if not candidate or not any((\"country\" in v or \"country_code\" in v) for v in candidate.values()):\n",
    "            raise RuntimeError(\"GeoNames quota/throttle? Missing 'country'/'country_code' in build.\")\n",
    "        gazetteer = candidate\n",
    "        _save_gazetteer_cache(gazetteer, CACHE_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"⏭️ Skipping gazetteer rebuild due to API issue: {e}\")\n",
    "        gazetteer = _load_gazetteer_cache(CACHE_PATH) or {}\n",
    "\n",
    "# ensure country fields exist (retrofill if only country_code present)\n",
    "def _retrofill_country(g: Dict[str, Dict]) -> int:\n",
    "    COUNTRY_NAME = {\n",
    "        \"AR\":\"Argentina\",\"CL\":\"Chile\",\"PE\":\"Peru\",\"CO\":\"Colombia\",\"VE\":\"Venezuela\",\n",
    "        \"BO\":\"Bolivia\",\"EC\":\"Ecuador\",\"PA\":\"Panama\",\"CR\":\"Costa Rica\",\"GT\":\"Guatemala\",\n",
    "        \"MX\":\"Mexico\",\"CU\":\"Cuba\",\"BR\":\"Brazil\",\"GY\":\"Guyana\",\"PY\":\"Paraguay\",\n",
    "        \"SR\":\"Suriname\",\"UY\":\"Uruguay\",\"HN\":\"Honduras\",\"SV\":\"El Salvador\",\"NI\":\"Nicaragua\",\n",
    "        \"GF\":\"French Guiana\"\n",
    "    }\n",
    "    changed = 0\n",
    "    for k, v in g.items():\n",
    "        cc = v.get(\"country_code\") or v.get(\"countryCode\") or v.get(\"cc\")\n",
    "        if cc and \"country\" not in v:\n",
    "            v[\"country\"] = COUNTRY_NAME.get(cc, cc)\n",
    "            changed += 1\n",
    "    return changed\n",
    "\n",
    "def _gaz_has_country(g) -> bool:\n",
    "    if not isinstance(g, dict) or not g:\n",
    "        return False\n",
    "    ok = sum((\"country\" in v and \"country_code\" in v) for v in g.values())\n",
    "    return (ok / max(len(g), 1)) >= 0.95\n",
    "\n",
    "if not _gaz_has_country(gazetteer):\n",
    "    _retrofill_country(gazetteer)\n",
    "\n",
    "# ---------------------- Gazetteer regex filters ----------------------\n",
    "try:\n",
    "    from nltk.corpus import stopwords as nltk_stopwords\n",
    "    _EN_STOP = set(nltk_stopwords.words(\"english\"))\n",
    "except Exception:\n",
    "    _EN_STOP = set(w.lower() for w in stop_words)\n",
    "\n",
    "BANNED_SINGLE_HEADS = {\n",
    "    \"hospital\",\"station\",\"school\",\"airport\",\"bridge\",\"park\",\"market\",\n",
    "    \"university\",\"college\",\"city\",\"province\",\"region\",\"mama\",\"friends\",\"best\"\n",
    "}\n",
    "BANNED_FULL_NAMES = {\"the best\"}\n",
    "_COUNTRY_NAMES_EN = {\n",
    "    \"argentina\",\"chile\",\"peru\",\"colombia\",\"venezuela\",\"bolivia\",\"ecuador\",\n",
    "    \"panama\",\"costa rica\",\"guatemala\",\"mexico\",\"cuba\",\"brazil\",\"guyana\",\n",
    "    \"paraguay\",\"suriname\",\"uruguay\",\"honduras\",\"el salvador\",\"nicaragua\"\n",
    "}\n",
    "\n",
    "def _looks_like_place_name(name: str) -> bool:\n",
    "    t = (name or \"\").strip().lower()\n",
    "    if not t: return False\n",
    "    if t in BANNED_FULL_NAMES: return False\n",
    "    toks = re.findall(r\"[a-zà-ÿ]+\", t)\n",
    "    if not toks: return False\n",
    "    if len(toks) == 1 and toks[0] in BANNED_SINGLE_HEADS: return False\n",
    "    if all(tok in _EN_STOP for tok in toks): return False\n",
    "    return True\n",
    "\n",
    "_PLACES: Set[str] = {n for n in gazetteer_names(gazetteer) if _looks_like_place_name(n)}\n",
    "GAZ_PATTERNS = build_gazetteer_patterns(_PLACES, _EN_STOP)\n",
    "\n",
    "def match_gazetteer_safe(text: str) -> List[Tuple[str, str, int, int]]:\n",
    "    hits = match_gazetteer_precompiled(text, GAZ_PATTERNS)\n",
    "    safe = []\n",
    "    for sl, _, s, e in hits:\n",
    "        # drop bare country words from regex layer (handled explicitly later)\n",
    "        if \" \" not in sl and sl.lower() in _COUNTRY_NAMES_EN:\n",
    "            continue\n",
    "        if sl.strip().lower() in BANNED_FULL_NAMES:\n",
    "            continue\n",
    "        safe.append((sl, \"GAZETTEER\", s, e))\n",
    "    return safe\n",
    "\n",
    "# ---------------------- Narrative cues / heuristics ----------------------\n",
    "try:\n",
    "    from geoparser.nlp_helpers import get_symbolic_verb_synonyms\n",
    "    SYMBOLIC_VERBS = get_symbolic_verb_synonyms()\n",
    "except Exception:\n",
    "    SYMBOLIC_VERBS = {\"govern\",\"rule\",\"dominate\",\"represent\",\"symbolize\",\"embody\"}\n",
    "\n",
    "MOVEMENT_VERBS = {\"arrive\",\"depart\",\"cross\",\"enter\",\"leave\",\"visit\",\n",
    "                  \"walk\",\"ride\",\"drive\",\"sail\",\"return\",\"board\",\"catch\",\"take\"}\n",
    "\n",
    "PREPS = {\"in\",\"to\",\"from\",\"into\",\"through\",\"via\",\"at\",\"near\",\"towards\",\"toward\",\"onto\",\"across\",\"over\",\"along\",\"past\",\"around\"}\n",
    "\n",
    "def heuristic_accept(ent_text: str, doc) -> bool:\n",
    "    ent_l = ent_text.lower()\n",
    "    span = next((ent for ent in doc.ents if ent.text.lower() == ent_l), None)\n",
    "    if span is None:\n",
    "        for i, t in enumerate(doc):\n",
    "            if ent_l in t.text.lower():\n",
    "                span = doc[i:i+1]; break\n",
    "    if span is None:\n",
    "        return True\n",
    "    head = span.root if hasattr(span, \"root\") else span[0]\n",
    "    cap_ok = (span.text[:1].isupper() or len(span) > 1) and (head.pos_ in {\"PROPN\",\"NOUN\"} or span.label_ in {\"GPE\",\"LOC\",\"FAC\"})\n",
    "    if cap_ok:\n",
    "        return True\n",
    "    L, R = max(0, head.i-3), min(len(doc), head.i+4)\n",
    "    if any(tok.pos_ == \"ADP\" and tok.lower_ in PREPS for tok in doc[L:R]):\n",
    "        return True\n",
    "    if any(tok.pos_ == \"VERB\" and tok.lemma_.lower() in MOVEMENT_VERBS for tok in doc[L:R]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ---------------------- Person / stoplike helpers ----------------------\n",
    "try:\n",
    "    from nltk.corpus import names\n",
    "    _EN_FIRSTNAMES: Set[str] = {n.lower() for n in names.words()}\n",
    "except Exception:\n",
    "    _EN_FIRSTNAMES = set()\n",
    "\n",
    "# small, high-frequency ES first names (diacritic-insensitive check)\n",
    "_ES_FIRSTNAMES = {\n",
    "    \"jose\",\"juan\",\"luis\",\"carlos\",\"miguel\",\"jorge\",\"pedro\",\"maria\",\"ana\",\"rosa\",\n",
    "    \"teresa\",\"marta\",\"carmen\",\"elena\",\"laura\",\"silvia\",\"sofia\",\"patricia\",\"andrea\",\"daniela\",\n",
    "    \"benjamin\",\"benjamín\"\n",
    "}\n",
    "\n",
    "_MONTHS_EN = {\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"}\n",
    "_MONTHS_EN_ABBR = {\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"sept\",\"oct\",\"nov\",\"dec\"}\n",
    "_MONTHS_ES = {\"enero\",\"febrero\",\"marzo\",\"abril\",\"mayo\",\"junio\",\"julio\",\"agosto\",\"septiembre\",\"setiembre\",\"octubre\",\"noviembre\",\"diciembre\"}\n",
    "_MONTHS_ES_ABBR = {\"ene\",\"feb\",\"mar\",\"abr\",\"may\",\"jun\",\"jul\",\"ago\",\"sept\",\"set\",\"oct\",\"nov\",\"dic\"}\n",
    "_DAYS_EN   = {\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"}\n",
    "\n",
    "def strip_diacritics(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKD\", s or \"\").encode(\"ascii\",\"ignore\").decode(\"utf-8\")\n",
    "\n",
    "def _is_stoplike(tok: str) -> bool:\n",
    "    t = tok.lower()\n",
    "    return t in _EN_STOP or t in _MONTHS_EN or t in _DAYS_EN\n",
    "\n",
    "def _looks_like_firstname(tok: str) -> bool:\n",
    "    t = tok.lower()\n",
    "    return (t in _EN_FIRSTNAMES) or (strip_diacritics(t) in _ES_FIRSTNAMES)\n",
    "\n",
    "def _valid_toponym(span_text: str) -> bool:\n",
    "    t = span_text.strip()\n",
    "    toks = re.findall(r\"[A-Za-zÀ-ÿ]+\", t)\n",
    "    if not toks:\n",
    "        return False\n",
    "    if len(toks) == 1:\n",
    "        tok = toks[0]\n",
    "        if t.islower(): return False\n",
    "        if _is_stoplike(tok): return False\n",
    "        if _looks_like_firstname(tok): return False\n",
    "    return True\n",
    "\n",
    "# PATCH: extend composite heads (helps grow one-word leads)\n",
    "COMPOSITE_HEADS = {\n",
    "    \"villa\",\"puerto\",\"bahía\",\"bahia\",\"río\",\"rio\",\"cerro\",\"san\",\"santa\",\"santo\",\n",
    "    \"laguna\",\"lago\",\"isla\",\"punta\",\"playa\",\"quebrada\",\"arroyo\",\"valle\",\"valley\"\n",
    "}\n",
    "AMBIGUOUS_SINGLETONS = {\"sierra\",\"villa\",\"serra\",\"rio\"}\n",
    "\n",
    "def _person_prefix_rule(sent_text: str, span_text: str) -> bool:\n",
    "    head = re.findall(r\"[A-Za-zÀ-ÿ]+\", span_text.lower())[:1]\n",
    "    if head and head[0] in COMPOSITE_HEADS:\n",
    "        return False\n",
    "    # allow diacritics & multi-token first names\n",
    "    m = re.search(r\"\\b([A-ZÀ-Ý][a-zà-ÿ]+(?:\\s+[A-ZÀ-Ý][a-zà-ÿ]+)?)\\s+\" + re.escape(span_text) + r\"\\b\", sent_text)\n",
    "    return bool(m and _looks_like_firstname(m.group(1)))\n",
    "\n",
    "NOISE_NAMES   = {\"la pedrosa\"}  # 'la poderosa' handled as transport\n",
    "VEHICLE_TERMS = {\n",
    "    \"motorcycle\",\"motorbike\",\"bike\",\"bicycle\",\"boat\",\"ship\",\"raft\",\"car\",\"truck\",\n",
    "    \"jeep\",\"bus\",\"van\",\"lorry\",\"pickup\",\"steamer\",\"steamship\",\"vessel\",\"launch\",\"lancha\",\"barco\",\"bote\",\"ferry\"\n",
    "}\n",
    "\n",
    "_HONORIFICS = r\"(Mr|Mrs|Ms|Dr|Sr|Sra|Srta)\\.\"\n",
    "_HONORIFIC_NEAR_ENTITY = re.compile(rf\"\\b{_HONORIFICS}\\s+([A-ZÀ-Ý][a-zà-ÿ]+)\\b\")\n",
    "\n",
    "_THE_TITLE = re.compile(r\"^the\\s+[A-Z][\\wÀ-ÿ-]+(?:\\s+[A-Z][\\wÀ-ÿ-]+)?$\", re.IGNORECASE)\n",
    "\n",
    "_SHIP_MOVE_VERBS = {\n",
    "    \"board\",\"embark\",\"sail\",\"moor\",\"dock\",\"berth\",\"launch\",\"load\",\"unload\",\"carry\",\"hoist\",\n",
    "    \"abordar\",\"embarcar\",\"zarpar\",\"atracar\",\"fondear\",\"cargar\",\"descargar\"\n",
    "}\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = (s or \"\")\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"utf-8\")\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "def _norm_entity(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[’']s\\b\", \"\", s)\n",
    "    s = s.strip('\"\\'' \"“”‘’\")\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"utf-8\")\n",
    "    return re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "def is_named_object(entity_text: str, sentence: str) -> bool:\n",
    "    ent_n = _norm_entity(entity_text)\n",
    "    if ent_n in NOISE_NAMES:\n",
    "        return True\n",
    "    # transport proximity → probably not a place\n",
    "    doc = nlp(sentence)\n",
    "    for i, tok in enumerate(doc):\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if lemma in VEHICLE_TERMS or lemma in _SHIP_MOVE_VERBS:\n",
    "            L, R = max(0, i-6), min(len(doc), i+7)\n",
    "            win_text = _norm_text(\" \".join(t.text for t in doc[L:R]))\n",
    "            if ent_n and ent_n in win_text:\n",
    "                return True\n",
    "    # honorific fallback\n",
    "    m = _HONORIFIC_NEAR_ENTITY.search(sentence)\n",
    "    if m and entity_text.strip() == m.group(1):\n",
    "        return True\n",
    "    # \"The Modesta Victoria\" with movement context\n",
    "    if _THE_TITLE.fullmatch(entity_text.strip()) and re.search(\n",
    "        r\"\\b(board|embark|sail|ship|boat|barco|lancha)\\b\", sentence.lower()\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_probable_metonymy(entity_text: str, sentence: str, label: str) -> bool:\n",
    "    if label not in {\"GPE\",\"COUNTRY\"}:\n",
    "        return False\n",
    "    doc = nlp(sentence)\n",
    "    ent_l = entity_text.lower()\n",
    "    cue_nouns = {\"government\",\"policy\",\"military\",\"regime\",\"parliament\",\"industry\",\"media\",\"press\",\"power\"}\n",
    "    has_cue = any(tok.lemma_.lower() in cue_nouns for tok in doc)\n",
    "    if not has_cue:\n",
    "        return False\n",
    "    idxs = [i for i,t in enumerate(doc) if ent_l in t.text.lower()]\n",
    "    return any(\n",
    "        any(abs(j - i) <= 5 for j,_ in enumerate(doc) if doc[j].lemma_.lower() in cue_nouns)\n",
    "        for i in idxs\n",
    "    )\n",
    "\n",
    "# ---------------------- Temporal + Transport (EN + ES) ----------------------\n",
    "DATE_PATTERNS = [\n",
    "    # English\n",
    "    (re.compile(r\"\\b(?:%s)\\s+\\d{1,2},\\s*(\\d{4})\\b\" % \"|\".join(_MONTHS_EN), re.I), \"year\"),\n",
    "    (re.compile(r\"\\b\\d{1,2}\\s+(?:%s)\\s+(\\d{4})\\b\" % \"|\".join(_MONTHS_EN), re.I), \"year\"),\n",
    "    (re.compile(r\"\\b(?:%s)\\s+(\\d{4})\\b\" % \"|\".join(_MONTHS_EN), re.I), \"year\"),\n",
    "    (re.compile(r\"\\b(?:%s)\\.?\\s+(\\d{4})\\b\" % \"|\".join(_MONTHS_EN_ABBR), re.I), \"year\"),\n",
    "    (re.compile(r\"\\b(19|20)\\d{2}\\b\"), \"year\"),\n",
    "    # Spanish\n",
    "    (re.compile(r\"\\b(\\d{1,2})\\s+de\\s+(%s)(?:\\s+de\\s+(\\d{4}))?\\b\" % \"|\".join(_MONTHS_ES), re.I), \"day-month-year?\"),\n",
    "    (re.compile(r\"\\b(en|del)\\s+(%s)\\s+de\\s+(\\d{4})\\b\" % \"|\".join(_MONTHS_ES), re.I), \"month-year\"),\n",
    "    (re.compile(r\"\\b(%s)\\s+de\\s+(\\d{4})\\b\" % \"|\".join(_MONTHS_ES), re.I), \"month-year\"),\n",
    "    (re.compile(r\"\\b(?:%s)\\.?\\s+(?:de\\s+)?(\\d{4})\\b\" % \"|\".join(_MONTHS_ES_ABBR), re.I), \"year\"),\n",
    "]\n",
    "\n",
    "def _extract_first_date(text: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Returns (date_norm, year, granularity).\n",
    "    Robust 4-digit year capture; Spanish keeps month names for downstream normalization.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None, None, None\n",
    "    for rx, kind in DATE_PATTERNS:\n",
    "        m = rx.search(text)\n",
    "        if not m:\n",
    "            continue\n",
    "        if kind == \"year\":\n",
    "            # prefer any captured 4-digit group; else find 4-digit in the full match\n",
    "            y = None\n",
    "            if m.lastindex:\n",
    "                for i in range(1, m.lastindex + 1):\n",
    "                    gi = m.group(i)\n",
    "                    if gi and re.fullmatch(r\"\\d{4}\", gi):\n",
    "                        y = gi; break\n",
    "            if not y:\n",
    "                m_year = re.search(r\"\\b(19|20)\\d{2}\\b\", m.group(0))\n",
    "                y = m_year.group(0) if m_year else None\n",
    "            return (y, y, \"year\") if y else (None, None, None)\n",
    "        if kind == \"day-month-year?\":\n",
    "            d, mon, y = m.group(1), m.group(2).lower(), m.group(3)\n",
    "            if y:\n",
    "                return (f\"{y}-{mon}-{d}\", y, \"day-month-year\")\n",
    "            else:\n",
    "                return (f\"{mon}-{d}\", None, \"day-month\")\n",
    "        if kind in {\"month-year\"}:\n",
    "            mon = (m.group(2) if m.lastindex and m.lastindex >= 2 else m.group(1)).lower()\n",
    "            y   = (m.group(3) if m.lastindex and m.lastindex >= 3 else (m.group(2) if m.lastindex and m.lastindex >= 2 else None))\n",
    "            if y:\n",
    "                return (f\"{y}-{mon}\", y, \"month-year\")\n",
    "    return None, None, None\n",
    "\n",
    "# ---------------------- Transport (recall-boosted) ----------------------\n",
    "TRANSPORT_MAP = {\n",
    "    # EN\n",
    "    \"motorcycle\":\"motorcycle\",\"motorbike\":\"motorcycle\",\"bike\":\"motorcycle\",\"moped\":\"motorcycle\",\"scooter\":\"motorcycle\",\n",
    "    \"bicycle\":\"bicycle\",\"cycle\":\"bicycle\",\n",
    "    \"car\":\"car\",\"truck\":\"truck\",\"jeep\":\"car\",\"van\":\"car\",\"pickup\":\"truck\",\"lorry\":\"truck\",\n",
    "    \"bus\":\"bus\",\"coach\":\"bus\",\"minibus\":\"bus\",\n",
    "    \"train\":\"train\",\"railway\":\"train\",\n",
    "    \"boat\":\"boat\",\"raft\":\"raft\",\"canoe\":\"boat\",\"kayak\":\"boat\",\"kayac\":\"boat\",\"yacht\":\"boat\",\"sailboat\":\"boat\",\"barge\":\"boat\",\n",
    "    \"ship\":\"boat\",\"ferry\":\"boat\",\n",
    "    \"plane\":\"plane\",\"airplane\":\"plane\",\"aeroplane\":\"plane\",\"aircraft\":\"plane\",\n",
    "    \"horse\":\"horse\",\"mule\":\"horse\",\"donkey\":\"horse\",\"foot\":\"foot\",\"walk\":\"foot\",\"walking\":\"foot\",\"hike\":\"foot\",\"trek\":\"foot\",\n",
    "    # ES\n",
    "    \"moto\":\"motorcycle\",\"motocicleta\":\"motorcycle\",\"mototaxi\":\"motorcycle\",\"moto-taxi\":\"motorcycle\",\n",
    "    \"bicicleta\":\"bicycle\",\"bici\":\"bicycle\",\n",
    "    \"auto\":\"car\",\"coche\":\"car\",\"camión\":\"truck\",\"camion\":\"truck\",\"camioneta\":\"truck\",\"pick-up\":\"truck\",\n",
    "    \"ómnibus\":\"bus\",\"omnibus\":\"bus\",\"autobús\":\"bus\",\"autobus\":\"bus\",\"colectivo\":\"bus\",\"micro\":\"bus\",\n",
    "    \"tren\":\"train\",\"ferrocarril\":\"train\",\n",
    "    \"balsa\":\"raft\",\"bote\":\"boat\",\"barco\":\"boat\",\"lancha\":\"boat\",\"barcaza\":\"boat\",\"canoa\":\"boat\",\"kayak\":\"boat\",\"yate\":\"boat\",\"velero\":\"boat\",\n",
    "    \"avión\":\"plane\",\"avion\":\"plane\",\"aeronave\":\"plane\",\n",
    "    \"caballo\":\"horse\",\"mula\":\"horse\",\"burro\":\"horse\",\"a pie\":\"foot\",\"a caballo\":\"horse\",\n",
    "    # diary-specific name\n",
    "    \"la poderosa\":\"motorcycle\",\"la poderosa i\":\"motorcycle\",\"la poderosa ii\":\"motorcycle\",\n",
    "}\n",
    "\n",
    "VERB_TRIGGERS = {\n",
    "    # EN lemmas\n",
    "    \"ride\",\"take\",\"board\",\"catch\",\"drive\",\"sail\",\"row\",\"paddle\",\"cross\",\"walk\",\"return\",\n",
    "    \"carry\",\"load\",\"unload\",\"hoist\",\"travel\",\"hike\",\"trek\",\"cycle\",\"canoe\",\"kayak\",\n",
    "    # ES lemmas\n",
    "    \"tomar\",\"subir\",\"montar\",\"viajar\",\"cruzar\",\"llegar\",\"salir\",\"partir\",\n",
    "    \"embarcar\",\"descender\",\"avanzar\",\"cabalgar\",\"navegar\",\"abordar\",\"cargar\",\"descargar\",\"remar\",\"caminar\",\"ir\"\n",
    "}\n",
    "\n",
    "# broader set of prepositions/intros that can precede vehicles\n",
    "PREP_TRIGGERS = {\"by\",\"on\",\"in\",\"onto\",\"into\",\"aboard\",\"over\",\"via\",\"en\",\"a\",\"al\",\"sobre\",\"por\",\"con\"}\n",
    "\n",
    "def _normalize_transport(tok: str) -> Optional[str]:\n",
    "    t = tok.lower().strip()\n",
    "    return TRANSPORT_MAP.get(t)\n",
    "\n",
    "# Regex fallback (POS-free) with \"a bordo de\" / \"on board (of)\" and broader preps.\n",
    "_TRANSPORT_FALLBACK = re.compile(\n",
    "    r\"\\b(?:a\\s+bordo\\s+de|on\\s+board(?:\\s+of)?|by|on|aboard|in|en|a|al|sobre|por|con)\\s+\"\n",
    "    r\"(?:el|la|los|las)?\\s*\"\n",
    "    r\"(motorcycle|motorbike|bike|bicycle|cycle|bus|coach|minibus|train|truck|car|boat|raft|ship|ferry|yacht|sailboat|barge|canoe|kayak|plane|airplane|aeroplane|aircraft|moped|scooter|\"\n",
    "    r\"moto|motocicleta|mototaxi|moto-taxi|bicicleta|bici|camión|camion|camioneta|pick-up|ómnibus|omnibus|autobús|autobus|colectivo|micro|tren|ferrocarril|balsa|bote|barco|lancha|barcaza|canoa|kayak|yate|velero|avión|avion|aeronave|caballo|mula|burro|a pie|a caballo)\\b\"\n",
    "    r\"|la\\s+poderosa\\s*(?:ii|i)?\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_transport_spans(sentence: str) -> Tuple[Optional[str], bool]:\n",
    "    if not isinstance(sentence, str) or not sentence.strip():\n",
    "        return None, False\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # lemma-based movement cue\n",
    "    verbs = {t.lemma_.lower() for t in doc if t.pos_ == \"VERB\"}\n",
    "    has_move = bool(verbs & VERB_TRIGGERS)\n",
    "\n",
    "    toks = [t.text for t in doc]\n",
    "    lemmas = [t.lemma_.lower() for t in doc]\n",
    "    pos = [t.pos_ for t in doc]\n",
    "    lowers = [t.text.lower() for t in doc]\n",
    "\n",
    "    # diary-specific name first\n",
    "    joined = \" \".join(lowers)\n",
    "    for pname in (\"la poderosa ii\",\"la poderosa i\",\"la poderosa\"):\n",
    "        if pname in joined:\n",
    "            return TRANSPORT_MAP[pname], True\n",
    "\n",
    "    # PREP(+optional stuff) + VEHICLE  (add special-case for \"a bordo de …\")\n",
    "    i = 0\n",
    "    while i < len(lemmas):\n",
    "        w, p = lemmas[i], pos[i]\n",
    "        if w in PREP_TRIGGERS and p == \"ADP\":\n",
    "            j = i + 1\n",
    "\n",
    "            # special-case: \"a bordo de\" / \"on board (of)\"\n",
    "            if j + 2 < len(lemmas):\n",
    "                if (lowers[i] == \"a\" and lowers[j] == \"bordo\" and pos[j+1] == \"ADP\") or \\\n",
    "                   (lowers[i] == \"on\" and lowers[j] == \"board\"):\n",
    "                    # skip \"a|on board (de|of)?\"\n",
    "                    j = j + 2 if lowers[i] == \"on\" else j + 2  # move to token after 'de'/'board'\n",
    "                    # if there's an extra 'of' after 'on board', skip it\n",
    "                    if lowers[i] == \"on\" and j < len(lemmas) and lowers[j] == \"of\":\n",
    "                        j += 1\n",
    "                    # skip optional determiners after that\n",
    "                    while j < len(lemmas) and pos[j] in {\"DET\",\"ADJ\",\"PRON\",\"ADP\"}:\n",
    "                        j += 1\n",
    "                    if j < len(lemmas):\n",
    "                        norm = _normalize_transport(toks[j])\n",
    "                        if norm:\n",
    "                            return norm, True\n",
    "\n",
    "            # generic skip of det/adj/pron/adp before the candidate vehicle\n",
    "            while j < len(lemmas) and pos[j] in {\"DET\",\"ADJ\",\"PRON\",\"ADP\"}:\n",
    "                j += 1\n",
    "            if j < len(lemmas):\n",
    "                norm = _normalize_transport(toks[j])\n",
    "                if norm:\n",
    "                    return norm, has_move\n",
    "        i += 1\n",
    "\n",
    "    # VERB(trigger) + * + VEHICLE\n",
    "    for i, (w, p) in enumerate(zip(lemmas, pos)):\n",
    "        if p == \"VERB\" and w in VERB_TRIGGERS:\n",
    "            j = i + 1\n",
    "            while j < len(lemmas) and pos[j] in {\"DET\",\"ADJ\",\"PRON\",\"ADP\"}:\n",
    "                j += 1\n",
    "            if j < len(lemmas):\n",
    "                norm = _normalize_transport(toks[j])\n",
    "                if norm:\n",
    "                    return norm, True\n",
    "\n",
    "    # bare vehicle token if a movement verb exists\n",
    "    if has_move:\n",
    "        for w in toks:\n",
    "            norm = _normalize_transport(w)\n",
    "            if norm:\n",
    "                return norm, True\n",
    "\n",
    "    # regex fallback (covers a bordo de / on board / broader preps)\n",
    "    m = _TRANSPORT_FALLBACK.search(sentence)\n",
    "    if m:\n",
    "        s = m.group(0).lower()\n",
    "        if \"poderosa\" in s:\n",
    "            return \"motorcycle\", True\n",
    "        for g in m.groups():\n",
    "            if g:\n",
    "                g = g.lower()\n",
    "                return TRANSPORT_MAP.get(g, {\"motorbike\":\"motorcycle\"}.get(g, g)), True\n",
    "\n",
    "    return None, has_move\n",
    "\n",
    "# ---------------------- Sentence docs & context ----------------------\n",
    "DOCS = {sid: nlp(text) for sid, text in SENTENCES_FOR_NER}  # noqa: F821\n",
    "\n",
    "persons_by_sentence = {\n",
    "    sid: [ent.text for ent in DOCS[sid].ents if ent.label_ == \"PERSON\"]\n",
    "    for sid, _ in SENTENCES_FOR_NER\n",
    "}\n",
    "\n",
    "GLOBAL_PERSONS: Set[str] = {p.lower().strip() for plist in persons_by_sentence.values() for p in plist}\n",
    "GLOBAL_PERSON_HEADS: Set[str] = {p.split()[-1] for p in GLOBAL_PERSONS if p}\n",
    "GLOBAL_PERSON_HEADS_L: Set[str] = {h.lower() for h in GLOBAL_PERSON_HEADS}\n",
    "PERSON_BLACKLIST: Set[str] = {\"alberto granado\",\"granado\",\"ernesto\",\"ernesto guevara\",\"che\",\"guevara\"}\n",
    "\n",
    "def _looks_like_person_here(ent_text: str, sid: int) -> bool:\n",
    "    cand = ent_text.lower().strip()\n",
    "    persons = [p.lower() for p in persons_by_sentence.get(sid, [])]\n",
    "    if cand in persons:\n",
    "        return True\n",
    "    return any(fuzz.token_set_ratio(cand, p) >= 90 for p in persons)\n",
    "\n",
    "# country lexicon (EN; extended with common variants)\n",
    "COUNTRY_ISO = {\n",
    "    \"argentina\":\"AR\",\"chile\":\"CL\",\"peru\":\"PE\",\"colombia\":\"CO\",\"venezuela\":\"VE\",\n",
    "    \"bolivia\":\"BO\",\"ecuador\":\"EC\",\"panama\":\"PA\",\"costa rica\":\"CR\",\"guatemala\":\"GT\",\n",
    "    \"mexico\":\"MX\",\"cuba\":\"CU\",\"brazil\":\"BR\",\"guyana\":\"GY\",\"paraguay\":\"PY\",\n",
    "    \"suriname\":\"SR\",\"uruguay\":\"UY\",\"honduras\":\"HN\",\"el salvador\":\"SV\",\"nicaragua\":\"NI\",\n",
    "    \"french guiana\":\"GF\",\n",
    "    # extended\n",
    "    \"united states\":\"US\",\"the united states\":\"US\",\"usa\":\"US\",\"u.s.\":\"US\",\"u.s.a.\":\"US\",\n",
    "    \"haiti\":\"HT\",\"dominican republic\":\"DO\",\"the dominican republic\":\"DO\",\"bahamas\":\"BS\",\n",
    "    \"trinidad and tobago\":\"TT\",\"jamaica\":\"JM\"\n",
    "}\n",
    "\n",
    "countries_by_sentence = {\n",
    "    sid: [t.text.lower() for t in DOCS[sid] if t.text.lower() in COUNTRY_ISO]\n",
    "    for sid, _ in SENTENCES_FOR_NER\n",
    "}\n",
    "\n",
    "def _is_bare_country(ent_text: str) -> bool:\n",
    "    return ent_text.lower().strip() in COUNTRY_ISO\n",
    "\n",
    "# ---------------------- Main extraction (NER + Gazetteer) ----------------------\n",
    "def _effective_label(label: str, ent_text: str) -> str:\n",
    "    if label == \"GAZETTEER\":\n",
    "        return \"COUNTRY\" if ent_text.lower() in COUNTRY_ISO else \"GPE\"\n",
    "    if label in {\"CITY\",\"STATE_OR_PROVINCE\",\"COUNTRY\"}:\n",
    "        return \"GPE\" if label != \"COUNTRY\" else \"COUNTRY\"\n",
    "    return label\n",
    "\n",
    "def combine_ner_gazetteer(sentences: List[Tuple[int, str]], gazetteer_set: Set[str]) -> List[Dict]:\n",
    "    allowed = {\"GPE\",\"LOC\",\"GAZETTEER\",\"FAC\",\"CITY\",\"STATE_OR_PROVINCE\",\"COUNTRY\"}\n",
    "    results: List[Dict] = []\n",
    "\n",
    "    for sid, text in tqdm(sentences, desc=\"NER + Gazetteer\"):\n",
    "        ents: List[Tuple[str,str,int,int]] = []\n",
    "        # NER (spaCy)\n",
    "        ents += [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in DOCS[sid].ents]\n",
    "        # Optional Stanza\n",
    "        if USE_STANZA and stanza_pipeline is not None:\n",
    "            try:\n",
    "                for sent in stanza_pipeline(text).sentences:\n",
    "                    for ent in getattr(sent, \"ents\", []):\n",
    "                        ents.append((ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Gazetteer spans\n",
    "        ents += match_gazetteer_safe(text)\n",
    "\n",
    "        keep = []\n",
    "        for ent_text, label, start, end in ents:\n",
    "            if label not in allowed:\n",
    "                continue\n",
    "            if not _valid_toponym(ent_text):\n",
    "                continue\n",
    "\n",
    "            eff_label = _effective_label(label, ent_text)\n",
    "\n",
    "            # drop pure country-name mentions entirely\n",
    "            if eff_label == \"COUNTRY\" and _is_bare_country(ent_text):\n",
    "                continue\n",
    "\n",
    "            # NER spans get narrative cue gate; gazetteer spans skip here to preserve recall\n",
    "            if label != \"GAZETTEER\" and not heuristic_accept(ent_text, DOCS[sid]):\n",
    "                continue\n",
    "\n",
    "            # person prefixes (e.g., \"Dr. Montoya\")\n",
    "            if _person_prefix_rule(text, ent_text):\n",
    "                continue\n",
    "\n",
    "            low = ent_text.lower().strip()\n",
    "\n",
    "            # suppress gazetteer hits that are actually people\n",
    "            if label == \"GAZETTEER\":\n",
    "                if _looks_like_person_here(ent_text, sid):\n",
    "                    continue\n",
    "                if low in GLOBAL_PERSONS or low in GLOBAL_PERSON_HEADS or low in PERSON_BLACKLIST:\n",
    "                    continue\n",
    "\n",
    "            # near-PERSON similarity: only suppress for gazetteer; keep NER\n",
    "            if any(fuzz.token_set_ratio(ent_text, p) >= 90 for p in persons_by_sentence.get(sid, [])):\n",
    "                if label == \"GAZETTEER\":\n",
    "                    continue\n",
    "\n",
    "            # drop single-token NER spans that match a global person head (surname)\n",
    "            if label != \"GAZETTEER\":\n",
    "                toks = re.findall(r\"[A-Za-zÀ-ÿ]+\", ent_text.strip())\n",
    "                if len(toks) == 1 and toks[0].lower() in GLOBAL_PERSON_HEADS_L:\n",
    "                    continue\n",
    "\n",
    "            if is_named_object(ent_text, text):\n",
    "                continue\n",
    "\n",
    "            # singleton ambiguity rule (gazetteer only)\n",
    "            if label == \"GAZETTEER\":\n",
    "                toks = re.findall(r\"[A-Za-zÀ-ÿ]+\", ent_text.strip())\n",
    "                if len(toks) == 1:\n",
    "                    head = toks[0].lower()\n",
    "                    if head in {\"sierra\",\"villa\",\"serra\",\"rio\"} and (head not in gazetteer_set):\n",
    "                        continue\n",
    "\n",
    "            # metonymy filter now applies with effective label\n",
    "            if is_probable_metonymy(ent_text, text, eff_label):\n",
    "                continue\n",
    "\n",
    "            keep.append((ent_text, eff_label, start, end))\n",
    "\n",
    "        # longest-nonoverlapping spans + de-dup by near-duplicate text (diacritics-aware)\n",
    "        spans = []\n",
    "        if keep:\n",
    "            dfk = pd.DataFrame([{\"t\":t, \"l\":l, \"s\":s, \"e\":e, \"len\": e-s} for t,l,s,e in keep]).sort_values(\"len\", ascending=False)\n",
    "            for r in dfk.itertuples():\n",
    "                if not any(not (r.e <= s or r.s >= e) for _,_,s,e in spans):\n",
    "                    t_norm = strip_diacritics(r.t).lower()\n",
    "                    if any(fuzz.token_set_ratio(t_norm, strip_diacritics(t0).lower()) >= 95 for t0,_,_,_ in spans):\n",
    "                        continue\n",
    "                    spans.append((r.t, r.l, r.s, r.e))\n",
    "\n",
    "        for ent_text, eff_label, start, end in spans:\n",
    "            results.append({\n",
    "                \"sentence_id\": sid,\n",
    "                \"entity\": ent_text,\n",
    "                \"entity_norm\": ent_text.lower().strip(),\n",
    "                \"label\": eff_label,\n",
    "                \"start_char\": start,\n",
    "                \"end_char\": end,\n",
    "                \"sentence\": text,\n",
    "                \"persons_in_sentence\": persons_by_sentence.get(sid, []),\n",
    "            })\n",
    "    return results\n",
    "\n",
    "print(\"🔎 Running ensemble NER + gazetteer matching (with metonymy & filters)…\")\n",
    "entities_combined = combine_ner_gazetteer(SENTENCES_FOR_NER, _PLACES)  # noqa: F821\n",
    "df_combined = pd.DataFrame(entities_combined)\n",
    "\n",
    "# --- keep schema even if empty; avoid ValueError down-pipeline ---\n",
    "REQUIRED_COLS = [\"sentence_id\",\"entity\",\"entity_norm\",\"label\",\"start_char\",\"end_char\",\"sentence\",\"persons_in_sentence\"]\n",
    "for c in REQUIRED_COLS:\n",
    "    if c not in df_combined.columns:\n",
    "        df_combined[c] = pd.Series(dtype=object)\n",
    "\n",
    "if not df_combined.empty:\n",
    "    df_combined = remove_overlapping_shorter(df_combined)\n",
    "else:\n",
    "    print(\"ℹ️ No entities to overlap-filter; continuing with empty frame.\")\n",
    "\n",
    "# ---------------------- Composite-head expansion & aliases ----------------------\n",
    "# extend expansion set with laguna/lago/isla + coastal/geomorph heads\n",
    "COMPOSITE_HEADS_EXPAND = {\n",
    "    \"sierra\",\"rio\",\"río\",\"cerro\",\"san\",\"santa\",\"santo\",\"villa\",\"puerto\",\n",
    "    \"bahía\",\"bahia\",\"laguna\",\"lago\",\"isla\",\"punta\",\"playa\",\"quebrada\",\"arroyo\",\"valle\",\"valley\"\n",
    "}\n",
    "\n",
    "def _grow_composite_token(ent_text: str, sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    If entity is a single composite head, try to grow to next capitalized chunk(s).\n",
    "    Handles 'Sierra Maestra', 'Río de la Plata', 'San Martín', etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(ent_text, str) or not ent_text.strip():\n",
    "        return ent_text\n",
    "    head = ent_text.strip().lower()\n",
    "    if \" \" in ent_text or head not in COMPOSITE_HEADS_EXPAND:\n",
    "        return ent_text\n",
    "    m = re.search(\n",
    "        rf\"\\b{re.escape(ent_text)}\\s+((?:[A-ZÀ-Ý][\\wÀ-ÿ-]+)(?:\\s+(?:de|del|la|las|los)\\s+[A-ZÀ-Ý][\\wÀ-ÿ-]+)?)\",\n",
    "        sentence\n",
    "    )\n",
    "    return f\"{ent_text} {m.group(1)}\" if m else ent_text\n",
    "\n",
    "if not df_combined.empty:\n",
    "    df_combined[\"entity_expanded\"] = df_combined.apply(\n",
    "        lambda r: _grow_composite_token(r[\"entity\"], r[\"sentence\"]), axis=1\n",
    "    )\n",
    "    mask_changed = df_combined[\"entity_expanded\"].str.lower() != df_combined[\"entity\"].str.lower()\n",
    "    df_combined.loc[mask_changed, \"entity\"] = df_combined.loc[mask_changed, \"entity_expanded\"]\n",
    "    df_combined.drop(columns=[\"entity_expanded\"], inplace=True, errors=\"ignore\")\n",
    "else:\n",
    "    print(\"ℹ️ Empty df_combined; skipping composite expansion.\")\n",
    "\n",
    "# --- Normalization: squash whitespace, EN→ES surface rewrites, aliases ---\n",
    "def _squash_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def _en_to_es_surface(s: str) -> str:\n",
    "    t = s.lower()\n",
    "    t = re.sub(r\"^lake\\s+\", \"lago \", t)\n",
    "    t = re.sub(r\"^river\\s+\", \"río \", t)\n",
    "    t = re.sub(r\"^(mount|mt\\.?)\\s+\", \"cerro \", t)\n",
    "    return t\n",
    "\n",
    "def _es_to_en_surface(s: str) -> str:\n",
    "    t = s.lower()\n",
    "    t = re.sub(r\"^lago\\s+\", \"lake \", t)\n",
    "    t = re.sub(r\"^(río|rio)\\s+\", \"river \", t)\n",
    "    t = re.sub(r\"^cerro\\s+\", \"mount \", t)\n",
    "    return t\n",
    "\n",
    "ALIASES = {\n",
    "    \"cuzco\":\"cusco\",\n",
    "    \"easter island\":\"isla de pascua\",\n",
    "    \"easter-island\":\"isla de pascua\",\n",
    "    \"rapa nui\":\"isla de pascua\",\n",
    "}\n",
    "\n",
    "df_combined[\"entity_lower\"] = (\n",
    "    df_combined[\"entity\"].map(_squash_ws).map(_en_to_es_surface).str.lower().map(lambda x: ALIASES.get(x, x))\n",
    ")\n",
    "\n",
    "# remove \"Poderosa\" as an entity (keep as transport signal only)\n",
    "df_combined = df_combined[~df_combined[\"entity_lower\"].str.contains(\"poderosa\", na=False)].copy()\n",
    "\n",
    "# --- Remove country-only mentions defensively (in case any slipped through) ---\n",
    "before_c = len(df_combined)\n",
    "df_combined = df_combined[~df_combined[\"entity_lower\"].isin(set(COUNTRY_ISO.keys()))].copy()\n",
    "print(f\"↪️ dropped country-only mentions (defensive): {before_c - len(df_combined)}\")\n",
    "\n",
    "# --- Region/ocean pruning (keep only explicit '... Ocean') ---\n",
    "OCEAN_BASINS = {\"atlantic\",\"pacific\",\"indian\",\"arctic\",\"southern\"}\n",
    "NON_TOPONYMS = {\"americas\",\"south america\",\"north america\",\"europe\",\"africa\",\"mediterranean\"}\n",
    "mask_ocean_bare = df_combined[\"entity_lower\"].isin(OCEAN_BASINS) & ~df_combined[\"entity_lower\"].str.contains(\"ocean\", na=False)\n",
    "mask_regions = df_combined[\"entity_lower\"].isin(NON_TOPONYMS)\n",
    "before_r = len(df_combined)\n",
    "df_combined = df_combined[~(mask_ocean_bare | mask_regions)].copy()\n",
    "print(f\"↪️ pruned oceans/regions: {before_r - len(df_combined)}\")\n",
    "\n",
    "# ---------------------- Sentence-level time & transport (+/- 1 borrowing) ----------------------\n",
    "_sent_df = pd.DataFrame(SENTENCES_FOR_NER, columns=[\"sentence_id\",\"sentence\"]).drop_duplicates()\n",
    "\n",
    "_sid_to_date, _sid_to_year, _sid_to_gran = {}, {}, {}\n",
    "for sid, sent in _sent_df.itertuples(index=False):\n",
    "    dnorm, y, gran = _extract_first_date(sent)\n",
    "    _sid_to_date[int(sid)] = dnorm\n",
    "    _sid_to_year[int(sid)] = y\n",
    "    _sid_to_gran[int(sid)] = gran\n",
    "\n",
    "_sid_to_transport, _sid_to_moveverb = {}, {}\n",
    "for sid, sent in _sent_df.itertuples(index=False):\n",
    "    tnorm, has_moveverb = extract_transport_spans(sent)\n",
    "    if not tnorm:\n",
    "        # POS-free safety net (regex)\n",
    "        m = _TRANSPORT_FALLBACK.search(sent or \"\")\n",
    "        if m:\n",
    "            s = m.group(0).lower()\n",
    "            if \"poderosa\" in s:\n",
    "                tnorm = \"motorcycle\"\n",
    "            else:\n",
    "                for g in m.groups():\n",
    "                    if g:\n",
    "                        g = g.lower()\n",
    "                        tnorm = TRANSPORT_MAP.get(g, {\"motorbike\":\"motorcycle\"}.get(g, g))\n",
    "                        break\n",
    "    _sid_to_transport[int(sid)] = tnorm\n",
    "    _sid_to_moveverb[int(sid)] = bool(has_moveverb)\n",
    "\n",
    "def _nearest_sentence_value(sid: int, mapping: dict[int, object]) -> object|None:\n",
    "    return mapping.get(sid) or mapping.get(sid-1) or mapping.get(sid+1)\n",
    "\n",
    "df_combined[\"date_norm\"] = df_combined[\"sentence_id\"].apply(lambda s: _nearest_sentence_value(int(s), _sid_to_date))\n",
    "df_combined[\"year\"] = df_combined[\"sentence_id\"].apply(lambda s: _nearest_sentence_value(int(s), _sid_to_year))\n",
    "df_combined[\"date_granularity\"] = df_combined[\"sentence_id\"].apply(lambda s: _nearest_sentence_value(int(s), _sid_to_gran))\n",
    "df_combined[\"transport\"] = df_combined[\"sentence_id\"].apply(lambda s: _nearest_sentence_value(int(s), _sid_to_transport))\n",
    "df_combined[\"movement_verb_present\"] = df_combined[\"sentence_id\"].apply(lambda s: bool(_nearest_sentence_value(int(s), _sid_to_moveverb)))\n",
    "\n",
    "# ---------------------- Enrichment with disambiguation scoring ----------------------\n",
    "# flatten gazetteer into a candidates table (DO NOT dedupe by name)\n",
    "gaz_rows = []\n",
    "for n, geo in gazetteer.items():\n",
    "    gaz_rows.append({\n",
    "        \"name_lower\":          n,\n",
    "        \"name_stripped\":       strip_diacritics(n),\n",
    "        \"lat\":                 geo.get(\"lat\"),\n",
    "        \"lon\":                 geo.get(\"lon\"),\n",
    "        \"country\":             geo.get(\"country\"),\n",
    "        \"country_code\":        geo.get(\"country_code\"),\n",
    "        \"feature_class\":       geo.get(\"feature_class\"),\n",
    "        \"feature_code\":        geo.get(\"feature_code\"),\n",
    "        \"admin1\":              geo.get(\"admin1\"),\n",
    "        \"admin2\":              geo.get(\"admin2\"),\n",
    "    })\n",
    "gaz_df = pd.DataFrame(gaz_rows)\n",
    "\n",
    "# name cleaning to help island/parenthetical variants\n",
    "def _clean_name(s: str) -> str:\n",
    "    s = re.sub(r\"\\(.*?\\)\", \" \", s or \"\")\n",
    "    s = re.sub(r\"[^A-Za-zÀ-ÿ\\s-]\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "\n",
    "gaz_df[\"name_clean\"] = gaz_df[\"name_lower\"].map(_clean_name)\n",
    "\n",
    "# build quick indices\n",
    "idx_by_name_lower = defaultdict(list)\n",
    "idx_by_name_stripped = defaultdict(list)\n",
    "idx_by_name_clean = defaultdict(list)\n",
    "for i, r in gaz_df.iterrows():\n",
    "    idx_by_name_lower[r[\"name_lower\"]].append(i)\n",
    "    idx_by_name_stripped[r[\"name_stripped\"]].append(i)\n",
    "    idx_by_name_clean[r[\"name_clean\"]].append(i)\n",
    "gaz_names_all = list(idx_by_name_lower.keys())\n",
    "gaz_names_clean = list(gaz_df[\"name_clean\"].unique())\n",
    "\n",
    "# context hint: ±2 sentence window\n",
    "def _sentence_country_hint_window(sid: int) -> Optional[str]:\n",
    "    window = range(int(sid) - 2, int(sid) + 3)\n",
    "    for s in window:\n",
    "        row = _sent_df.loc[_sent_df[\"sentence_id\"] == s]\n",
    "        if not row.empty:\n",
    "            toks = re.findall(r\"[A-Za-zÀ-ÿ]+\", str(row.iloc[0][\"sentence\"]).lower())\n",
    "            for tok in toks:\n",
    "                if tok in COUNTRY_ISO:\n",
    "                    return COUNTRY_ISO[tok]\n",
    "    return None\n",
    "\n",
    "# add a few hydro/relief tokens for better surface hints\n",
    "_WATER_TOKENS = {\"lake\",\"lago\",\"river\",\"río\",\"laguna\",\"lagoon\",\"canal\",\"arroyo\",\"quebrada\"}\n",
    "_RELIEF_TOKENS = {\"cerro\",\"sierra\",\"cordillera\",\"monte\",\"valle\",\"valley\",\"punta\"}\n",
    "\n",
    "def _candidate_score(row, cand) -> float:\n",
    "    \"\"\"\n",
    "    Score a gazetteer candidate 'cand' (Series) for entity row.\n",
    "    Higher is better. Combines exactness, similarity, feature class, and context.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    ent = row[\"entity_lower\"]\n",
    "    ent_stripped = strip_diacritics(ent)\n",
    "    name_l = cand[\"name_lower\"]\n",
    "    name_s = cand[\"name_stripped\"]\n",
    "\n",
    "    if ent == name_l: score += 50\n",
    "    if ent_stripped == name_s: score += 35\n",
    "\n",
    "    # string similarity\n",
    "    score += 0.20 * fuzz.WRatio(ent, name_l)\n",
    "\n",
    "    # prefer populated/admin for GPE; allow islands (T/ISL) to compete\n",
    "    if row[\"label\"] in {\"GPE\",\"LOC\"}:\n",
    "        if cand.get(\"feature_class\") == \"P\": score += 12\n",
    "        elif cand.get(\"feature_class\") == \"A\": score += 7\n",
    "        elif cand.get(\"feature_class\") == \"T\" and cand.get(\"feature_code\") == \"ISL\": score += 9\n",
    "\n",
    "    # hydro/relief boosts when hinted by surface form\n",
    "    if cand.get(\"feature_class\") == \"H\" and any(w in ent for w in _WATER_TOKENS):  # hydro\n",
    "        score += 10\n",
    "    if cand.get(\"feature_class\") == \"T\" and any(w in ent for w in _RELIEF_TOKENS): # terrain\n",
    "        score += 10\n",
    "\n",
    "    # ±2 sentence context boost (rebalanced to reduce score saturation)\n",
    "    hint_cc = _sentence_country_hint_window(row[\"sentence_id\"])\n",
    "    if hint_cc and cand.get(\"country_code\") == hint_cc:\n",
    "        score += 35  # tuned from +45\n",
    "\n",
    "    # if gazetteer lacks feature metadata (city-only build), don't penalize\n",
    "    if not cand.get(\"feature_class\"):\n",
    "        score += 0\n",
    "\n",
    "    # light preference for configured countries\n",
    "    allowed = set(gcfg.get(\"countries\", [])) or set(COUNTRY_ISO.values())\n",
    "    if cand.get(\"country_code\") in allowed:\n",
    "        score += 5\n",
    "\n",
    "    return score\n",
    "\n",
    "def _best_non_country_row(row) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Pick best candidate among exact/stripped/clean/fuzzy matches using _candidate_score.\n",
    "    \"\"\"\n",
    "    ent_base = _squash_ws(row[\"entity\"]).lower()\n",
    "    variants = {ent_base, _en_to_es_surface(ent_base), _es_to_en_surface(ent_base)}\n",
    "    cand_idx: Set[int] = set()\n",
    "    for ent in variants:\n",
    "        ent_s = strip_diacritics(ent)\n",
    "        ent_c = _clean_name(ent)\n",
    "        cand_idx.update(idx_by_name_lower.get(ent, []))\n",
    "        cand_idx.update(idx_by_name_stripped.get(ent_s, []))\n",
    "        cand_idx.update(idx_by_name_clean.get(ent_c, []))\n",
    "\n",
    "    # if no direct candidates, try fuzzy names\n",
    "    if not cand_idx:\n",
    "        for ent in variants:\n",
    "            fuzzy = process.extract(ent, gaz_names_all, scorer=fuzz.WRatio, limit=5)\n",
    "            for name, sim, _ in fuzzy:\n",
    "                if sim >= 88:\n",
    "                    cand_idx.update(idx_by_name_lower.get(name, []))\n",
    "            ent_c = _clean_name(ent)\n",
    "            fuzzy_c = process.extract(ent_c, gaz_names_clean, scorer=fuzz.WRatio, limit=5)\n",
    "            for name, sim, _ in fuzzy_c:\n",
    "                if sim >= 82:\n",
    "                    cand_idx.update(idx_by_name_clean.get(name, []))\n",
    "\n",
    "    if not cand_idx:\n",
    "        return {\"country_code\": None, \"country\": None, \"lat\": None, \"lon\": None,\n",
    "                \"feature_class\": None, \"feature_code\": None, \"match_source\": \"none\", \"disamb_score\": 0.0}\n",
    "\n",
    "    best, best_score = None, float(\"-inf\")\n",
    "    for i in cand_idx:\n",
    "        cand = gaz_df.loc[i]\n",
    "        sc = _candidate_score(row, cand)\n",
    "        if sc > best_score:\n",
    "            best, best_score = cand, sc\n",
    "\n",
    "    return {\n",
    "        \"country_code\": best.get(\"country_code\"),\n",
    "        \"country\": best.get(\"country\"),\n",
    "        \"lat\": best.get(\"lat\"),\n",
    "        \"lon\": best.get(\"lon\"),\n",
    "        \"feature_class\": best.get(\"feature_class\"),\n",
    "        \"feature_code\": best.get(\"feature_code\"),\n",
    "        \"match_source\": \"exact/stripped/fuzzy\",\n",
    "        \"disamb_score\": float(best_score),\n",
    "    }\n",
    "\n",
    "def enrich_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"country_code\",\"country\",\"lat\",\"lon\",\"feature_class\",\"feature_code\",\"match_source\",\"disamb_score\"]\n",
    "    out = {c: [] for c in cols}\n",
    "    for r in tqdm(df.itertuples(index=False), total=len(df), desc=\"Enrich + disambiguate\"):\n",
    "        row = r._asdict() if hasattr(r, \"_asdict\") else dict(r._asdict())\n",
    "        info = _best_non_country_row(row)  # country-only rows were dropped upstream\n",
    "        for c in cols:\n",
    "            out[c].append(info.get(c))\n",
    "    for c in cols:\n",
    "        df[c] = out[c]\n",
    "    return df\n",
    "\n",
    "# --- Composite cleanup applied; now enrich ---\n",
    "df_enriched = enrich_rows(df_combined.copy())\n",
    "\n",
    "# serialize persons_in_sentence as JSON for CSV safety\n",
    "df_enriched[\"persons_in_sentence\"] = df_enriched[\"persons_in_sentence\"].apply(\n",
    "    lambda xs: xs if isinstance(xs, str) and xs.startswith(\"[\") else json.dumps(xs, ensure_ascii=False)\n",
    ")\n",
    "\n",
    "# ---------------------- Phrase stoplist + score floor cleanup ----------------------\n",
    "PHRASE_STOPLIST = {\n",
    "    \"latin america\",\"north america\",\"america\",\"the americas\",\"americas\",\"south america\",\n",
    "    \"amazon\",\"rising sun\",\"two friends\",\"alliance\",\"north\",\"europe\",\"africa\",\"mediterranean\"\n",
    "}\n",
    "SCORE_FLOOR = 58  # slightly lower to recover legit hydro/relief matches\n",
    "\n",
    "mask_phrase = df_enriched[\"entity_lower\"].isin(PHRASE_STOPLIST)\n",
    "rows_before = len(df_enriched)\n",
    "df_enriched = df_enriched[~mask_phrase].copy()\n",
    "dropped_phrase = rows_before - len(df_enriched)\n",
    "\n",
    "mask_low = (df_enriched[\"disamb_score\"].fillna(0) < SCORE_FLOOR)\n",
    "df_enriched.loc[mask_low, [\"lat\",\"lon\",\"country\",\"country_code\"]] = [None, None, None, None]\n",
    "df_enriched.loc[mask_low, \"match_source\"] = \"rejected_low_evidence\"\n",
    "\n",
    "# --- Facility head cleanup (guarded) ---\n",
    "# non-capturing group avoids pandas \"match groups\" warning\n",
    "_FAC_HEADS = re.compile(r\"\\b(?:hospital|station|university|college|bridge|airport)\\b\", re.I)\n",
    "mask_fac_bad = (\n",
    "    df_enriched[\"entity\"].astype(str).str.contains(_FAC_HEADS, na=False)\n",
    "    & (\n",
    "        (df_enriched[\"match_source\"].isin({\"none\",\"rejected_low_evidence\"}))\n",
    "        | (df_enriched[\"disamb_score\"].fillna(0) < SCORE_FLOOR)\n",
    "    )\n",
    ")\n",
    "dropped_fac = int(mask_fac_bad.sum())\n",
    "if dropped_fac:\n",
    "    df_enriched = df_enriched[~mask_fac_bad].copy()\n",
    "    print(f\"↪️ dropped unresolved facilities: {dropped_fac}\")\n",
    "\n",
    "# OPTIONAL: SA viz flag (non-destructive)\n",
    "SOUTH_AM = {\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"BO\",\"EC\",\"BR\",\"PY\",\"UY\",\"GY\",\"SR\",\"GF\"}\n",
    "df_enriched[\"in_south_america\"] = df_enriched[\"country_code\"].isin(SOUTH_AM)\n",
    "\n",
    "# Export\n",
    "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "out_csv = Path(\"outputs/geoparsing_ner_ensemble.csv\")\n",
    "df_enriched.to_csv(out_csv, index=False)\n",
    "print(f\"Saved: {out_csv}  ({len(df_enriched)} rows) | pruned phrases: {dropped_phrase} | low-evidence nulled: {int(mask_low.sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f112e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transport filled (helpers): +18 (now 57/231)\n",
      "Enriched data saved → outputs/geoparsing_final_enriched.csv  (231 rows)\n"
     ]
    }
   ],
   "source": [
    "# === Block 8: Symbolic Enrichment (modular, using helpers) ===\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from geoparser.nlp_helpers import init_nlp\n",
    "from geoparser.enrichment_helpers import (\n",
    "    load_gazetteer_compat,\n",
    "    normalize_entity_surface,\n",
    "    gaz_lookup_latlon,\n",
    "    safe_person_list,\n",
    "    is_named_object_context,\n",
    "    movement_verb_present,\n",
    "    symbolic_context,\n",
    "    metonymy_flag,\n",
    "    build_sid_to_transport,\n",
    "    infer_transport_for_row,\n",
    "    extract_year_regex,\n",
    "    final_label_decision,\n",
    "    reorder_columns,\n",
    ")\n",
    "\n",
    "# ---------------------- NLP ----------------------\n",
    "try:\n",
    "    nlp  # noqa: F821\n",
    "except NameError:\n",
    "    nlp, _ = init_nlp(lang=\"en\", prefer=[\"en_core_web_md\", \"en_core_web_sm\"])\n",
    "\n",
    "# ---------------------- Paths ----------------------\n",
    "outputs = Path(\"outputs\")\n",
    "in_path = outputs / \"geoparsing_ner_ensemble.csv\"    # Block 7 output\n",
    "gaz_path = outputs / \"geonames_cache.json\"\n",
    "\n",
    "# ---------------------- Load data ----------------------\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "# persons_in_sentence parsing (robust)\n",
    "if \"persons_in_sentence\" in df.columns:\n",
    "    df[\"persons_in_sentence\"] = df[\"persons_in_sentence\"].apply(safe_person_list)\n",
    "else:\n",
    "    df[\"persons_in_sentence\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "# Normalized surface (match Block 7)\n",
    "df[\"entity_lower\"] = df.get(\"entity\", \"\").astype(str).map(normalize_entity_surface)\n",
    "\n",
    "# Gazetteer cache (Block-7 compatible)\n",
    "rows, by_lower, by_stripped, by_clean, gaz_set = load_gazetteer_compat(gaz_path)\n",
    "\n",
    "# Fill lat/lon if missing (non-clobbering)\n",
    "if \"lat\" not in df.columns: df[\"lat\"] = pd.NA\n",
    "if \"lon\" not in df.columns: df[\"lon\"] = pd.NA\n",
    "\n",
    "lat_fill = df[\"entity_lower\"].map(lambda k: gaz_lookup_latlon(k, by_lower, by_stripped, by_clean)[0])\n",
    "lon_fill = df[\"entity_lower\"].map(lambda k: gaz_lookup_latlon(k, by_lower, by_stripped, by_clean)[1])\n",
    "\n",
    "df[\"lat\"] = df[\"lat\"].where(df[\"lat\"].notna(), lat_fill)\n",
    "df[\"lon\"] = df[\"lon\"].where(df[\"lon\"].notna(), lon_fill)\n",
    "\n",
    "# country_valid: prefer Block 7 signal; else gaz membership/coords\n",
    "if \"country_code\" in df.columns:\n",
    "    df[\"country_valid\"] = df[\"country_code\"].notna()\n",
    "else:\n",
    "    df[\"country_valid\"] = df[\"entity_lower\"].isin(gaz_set) | df[\"lat\"].notna()\n",
    "\n",
    "# ---------------------- Enrichment flags ----------------------\n",
    "# named-object (boats etc.)\n",
    "df[\"named_object_flag\"] = df.apply(lambda r: is_named_object_context(nlp, r.get(\"entity\",\"\"), r.get(\"sentence\",\"\")), axis=1)\n",
    "\n",
    "# movement / symbolic (coalesce with existing columns if present)\n",
    "if \"movement_verb_present\" not in df.columns:\n",
    "    df[\"movement_verb_present\"] = df.apply(lambda r: movement_verb_present(nlp, r.get(\"sentence\",\"\"), r.get(\"entity\",\"\"), r.get(\"persons_in_sentence\",[])), axis=1)\n",
    "else:\n",
    "    df[\"movement_verb_present\"] = df[\"movement_verb_present\"].astype(bool)\n",
    "\n",
    "df[\"symbolic_context\"] = df.apply(lambda r: symbolic_context(nlp, r.get(\"sentence\",\"\"), r.get(\"entity\",\"\"), r.get(\"persons_in_sentence\",[])), axis=1)\n",
    "\n",
    "# metonymy (keep if present)\n",
    "if \"metonymy_flagged\" not in df.columns:\n",
    "    df[\"metonymy_flagged\"] = df.apply(lambda r: metonymy_flag(nlp, r.get(\"entity\",\"\"), r.get(\"sentence\",\"\")), axis=1)\n",
    "else:\n",
    "    df[\"metonymy_flagged\"] = df[\"metonymy_flagged\"].astype(bool)\n",
    "\n",
    "# ---------------------- Dates ----------------------\n",
    "# Keep Block 7's date_norm/year/date_granularity if present; else regex year fallback\n",
    "if \"year\" not in df.columns:\n",
    "    df[\"year\"] = df.get(\"sentence\",\"\").apply(extract_year_regex)\n",
    "\n",
    "# ---------------------- Transport (WordNet/regex + context backfill) ----------------------\n",
    "if \"transport\" not in df.columns:\n",
    "    df[\"transport\"] = pd.NA\n",
    "\n",
    "sid2t = build_sid_to_transport(df)\n",
    "prev_t = int(df[\"transport\"].notna().sum())\n",
    "df[\"transport\"] = df.apply(lambda r: infer_transport_for_row(r, sid2t, enable_wordnet=True), axis=1)\n",
    "now_t = int(df[\"transport\"].notna().sum())\n",
    "print(f\"Transport filled (helpers): +{now_t - prev_t} (now {now_t}/{len(df)})\")\n",
    "\n",
    "# ---------------------- Final label ----------------------\n",
    "df[\"final_label\"] = df.apply(final_label_decision, axis=1)\n",
    "\n",
    "# ---------------------- People summary ----------------------\n",
    "df[\"people_involved\"] = df[\"persons_in_sentence\"].apply(lambda xs: \", \".join(xs) if xs else None)\n",
    "\n",
    "# ---------------------- Column order ----------------------\n",
    "preferred_cols = [\n",
    "    \"sentence_id\",\"entity\",\"entity_norm\",\"entity_lower\",\"label\",\n",
    "    \"lat\",\"lon\",\"country\",\"country_code\",\"country_valid\",\n",
    "    \"symbolic_context\",\"movement_verb_present\",\"metonymy_flagged\",\"named_object_flag\",\n",
    "    \"final_label\",\"date_norm\",\"date_granularity\",\"year\",\"transport\",\n",
    "    \"start_char\",\"end_char\",\"sentence\",\"persons_in_sentence\",\"people_involved\",\n",
    "    \"feature_class\",\"feature_code\",\"disamb_score\",\"match_source\"\n",
    "]\n",
    "df = reorder_columns(df, preferred_cols)\n",
    "\n",
    "# ---------------------- Save ----------------------\n",
    "out_path = outputs / \"geoparsing_final_enriched.csv\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"Enriched data saved → {out_path}  ({len(df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f3fbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts BEFORE split:\n",
      " final_label\n",
      "LITERAL    150\n",
      "NOISE       80\n",
      "Name: count, dtype: int64\n",
      "Best NOISE threshold on val (by F1): 0.180  |  P=1.00 R=1.00 F1=1.00\n",
      "\n",
      "Classification Report (thresholded):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     LITERAL       1.00      1.00      1.00        30\n",
      "       NOISE       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        46\n",
      "   macro avg       1.00      1.00      1.00        46\n",
      "weighted avg       1.00      1.00      1.00        46\n",
      "\n",
      "Confusion Matrix (thresholded):\n",
      "[[30  0]\n",
      " [ 0 16]]\n",
      " Saved test predictions → outputs/geoparser_ml_predictions_thresholded.csv\n"
     ]
    }
   ],
   "source": [
    "# === Block 10: ML Classification (balanced LR + NOISE-threshold calibration) ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0) Load data (binary LITERAL vs NOISE; keep negatives; no country filter)\n",
    "# ---------------------------------------------------------\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "df = df[df[\"final_label\"].isin([\"LITERAL\", \"NOISE\"])].copy()\n",
    "# Guard: LITERAL must have coords\n",
    "df = df[~((df[\"final_label\"]==\"LITERAL\") & (df[\"lat\"].isna() | df[\"lon\"].isna()))].copy()\n",
    "print(\"Class counts BEFORE split:\\n\", df[\"final_label\"].value_counts())\n",
    "\n",
    "# Persons column may be serialized -> normalize to list (not used here, but safe)\n",
    "def _as_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str) and x.strip().startswith(\"[\"):\n",
    "        import ast\n",
    "        try: return [str(t) for t in ast.literal_eval(x)]\n",
    "        except Exception: return []\n",
    "    return [] if pd.isna(x) else [str(x)]\n",
    "if \"persons_in_sentence\" in df.columns:\n",
    "    df[\"persons_in_sentence\"] = df[\"persons_in_sentence\"].apply(_as_list)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Gazetteer (optional cue; robust load)\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    with open(\"outputs/geonames_cache.json\",\"r\") as f:\n",
    "        _gj = json.load(f)\n",
    "    gazetteer = {r[\"name\"].lower() for r in _gj if isinstance(r, dict) and \"name\" in r}\n",
    "except Exception:\n",
    "    gazetteer = set()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Lexicons / regex\n",
    "# ---------------------------------------------------------\n",
    "MONTHS = {\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"}\n",
    "YEAR_RE = re.compile(r\"\\b(1[89]\\d{2}|20\\d{2})\\b\")\n",
    "\n",
    "def _nlp_has_vectors(nlp_obj) -> bool:\n",
    "    try:\n",
    "        return hasattr(nlp_obj, \"vocab\") and any(w.has_vector for w in nlp_obj.vocab)\n",
    "    except Exception:\n",
    "        return False\n",
    "_HAS_VECS = _nlp_has_vectors(nlp)  # uses `nlp` initialized earlier\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Feature extractor (interpretable, leak-free)\n",
    "# ---------------------------------------------------------\n",
    "def extract_features_row(row) -> dict:\n",
    "    entity   = str(row[\"entity\"])\n",
    "    sentence = str(row[\"sentence\"])\n",
    "    start    = int(row.get(\"start_char\", 0))\n",
    "\n",
    "    doc = nlp(sentence)  # from earlier blocks\n",
    "\n",
    "    ent = entity.strip()\n",
    "    L   = len(ent)\n",
    "    feats = {\n",
    "        # surface\n",
    "        \"entity_len\": L,\n",
    "        \"token_count\": len(ent.split()),\n",
    "        \"entity_capital_ratio\": sum(c.isupper() for c in ent) / (L or 1),\n",
    "        \"starts_with_cap\": ent[:1].isupper(),\n",
    "        \"has_digits\": any(c.isdigit() for c in ent),\n",
    "        \"has_hyphen\": \"-\" in ent,\n",
    "        \"has_apos\": (\"'\" in ent) or (\"’\" in ent),\n",
    "        \"is_ascii\": ent.isascii(),\n",
    "        # context-lite\n",
    "        \"entity_position_ratio\": start / (len(sentence) or 1),\n",
    "        \"mentions_year\": int(bool(YEAR_RE.search(sentence))),\n",
    "        \"mentions_month\": int(any(m in sentence.lower() for m in MONTHS)),\n",
    "        # optional gazetteer cue\n",
    "        \"gazetteer_match\": int(ent.lower() in gazetteer),\n",
    "    }\n",
    "\n",
    "    # vectors-safe similarity\n",
    "    if _HAS_VECS:\n",
    "        try:\n",
    "            feats[\"entity_sentence_sim\"] = doc.similarity(nlp(ent))\n",
    "        except Exception:\n",
    "            feats[\"entity_sentence_sim\"] = 0.0\n",
    "    else:\n",
    "        feats[\"entity_sentence_sim\"] = 0.0\n",
    "\n",
    "    return feats\n",
    "\n",
    "X_dict = [extract_features_row(r) for _, r in df.iterrows()]\n",
    "y      = df[\"final_label\"].reset_index(drop=True)\n",
    "\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X   = vec.fit_transform(X_dict)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Split → Train/Val/Test (stratified)\n",
    "#    We’ll tune threshold on a validation split, then evaluate on test.\n",
    "# ---------------------------------------------------------\n",
    "# main split (train+val vs test)\n",
    "X_trv, X_test, y_trv, y_test, df_trv, df_test = train_test_split(\n",
    "    X, y, df, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "# train vs val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trv, y_trv, test_size=0.25, stratify=y_trv, random_state=42\n",
    ")  # 0.25 of 0.8 = 0.2 → so final: 60/20/20 split\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc   = le.transform(y_val)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "classes     = list(le.classes_)\n",
    "assert set(classes) == {\"LITERAL\",\"NOISE\"}, f\"Expecting binary classes; got {classes}\"\n",
    "NOISE_IDX   = list(classes).index(\"NOISE\")  # index in predict_proba columns\n",
    "\n",
    "# model\n",
    "clf = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=400,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train_enc)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Threshold calibration on validation set (optimize NOISE F1)\n",
    "# ---------------------------------------------------------\n",
    "proba_val = clf.predict_proba(X_val)[:, NOISE_IDX]\n",
    "\n",
    "def _f1_for_threshold(y_true_enc, proba_pos, thresh: float):\n",
    "    pred_enc = (proba_pos >= thresh).astype(int)  # 1 = NOISE\n",
    "    # remap to string labels for metrics\n",
    "    y_true = le.inverse_transform(y_true_enc)\n",
    "    y_pred = le.inverse_transform(pred_enc)\n",
    "    P, R, F1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[\"NOISE\"], zero_division=0)\n",
    "    return P[0], R[0], F1[0]\n",
    "\n",
    "grid = np.linspace(0.01, 0.99, 99)\n",
    "scores = [(_t, *_f1_for_threshold(y_val_enc, proba_val, _t)) for _t in grid]\n",
    "best_t, best_p, best_r, best_f1 = max(scores, key=lambda x: x[3])\n",
    "\n",
    "print(f\"Best NOISE threshold on val (by F1): {best_t:.3f}  |  P={best_p:.2f} R={best_r:.2f} F1={best_f1:.2f}\")\n",
    "\n",
    "# retrain on full train+val with same hyperparams\n",
    "clf.fit(X_trv, le.transform(y_trv))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Evaluate on held-out test with calibrated threshold\n",
    "# ---------------------------------------------------------\n",
    "proba_test = clf.predict_proba(X_test)[:, NOISE_IDX]\n",
    "pred_test_enc = (proba_test >= best_t).astype(int)  # 1 = NOISE, 0 = LITERAL\n",
    "y_pred = le.inverse_transform(pred_test_enc)\n",
    "\n",
    "print(\"\\nClassification Report (thresholded):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix (thresholded):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Save test predictions for visualization\n",
    "# ---------------------------------------------------------\n",
    "df_out = df_test.copy()\n",
    "df_out[\"proba_noise\"] = proba_test\n",
    "df_out[\"ml_prediction\"] = y_pred\n",
    "out_path = \"outputs/geoparser_ml_predictions_thresholded.csv\"\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\" Saved test predictions → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63008e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive map saved → outputs/interactive_geoparsing_map.html\n"
     ]
    }
   ],
   "source": [
    "# === Block 12: Interactive Map (thresholded predictions; robust & pretty) ===\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import AntPath\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0) Load predictions\n",
    "# ---------------------------------------------------------\n",
    "pred_path = Path(\"outputs/geoparser_ml_predictions_thresholded.csv\")\n",
    "if not pred_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing {pred_path}. Run Block 10 first.\")\n",
    "\n",
    "df = pd.read_csv(pred_path)\n",
    "\n",
    "# defensively fill expected metadata columns, if absent\n",
    "for col in [\"people_involved\",\"transport\",\"year\",\"sentence_id\",\"entity\",\"lat\",\"lon\",\"ml_prediction\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "# keep only literal points with coordinates\n",
    "df_map = df[(df[\"ml_prediction\"]==\"LITERAL\") & df[\"lat\"].notna() & df[\"lon\"].notna()].copy()\n",
    "if df_map.empty:\n",
    "    raise ValueError(\"No LITERAL rows with coordinates to plot. Check thresholds or upstream steps.\")\n",
    "\n",
    "# order: if you have sentence_id keep its order, else keep current index\n",
    "if \"sentence_id\" in df_map.columns and df_map[\"sentence_id\"].notna().any():\n",
    "    df_map = df_map.sort_values(by=[\"sentence_id\"]).reset_index(drop=True)\n",
    "else:\n",
    "    df_map = df_map.reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Hover text builder\n",
    "# ---------------------------------------------------------\n",
    "def generate_hover_info(row):\n",
    "    parts=[]\n",
    "    p = str(row.get(\"people_involved\") or \"\").strip()\n",
    "    t = str(row.get(\"transport\") or \"\").strip()\n",
    "    y = str(row.get(\"year\") or \"\").strip()\n",
    "    if p: parts.append(f\"👥 {p}\")\n",
    "    if t: parts.append(f\"🚗 {t}\")\n",
    "    if y and y.lower() != \"none\": parts.append(f\"📅 {y}\")\n",
    "    return \"<br>\".join(parts) if parts else \"ℹ️ No metadata\"\n",
    "\n",
    "df_map[\"hover_text\"] = df_map.apply(generate_hover_info, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Build map\n",
    "# ---------------------------------------------------------\n",
    "route_coords = df_map[[\"lat\",\"lon\"]].values.tolist()\n",
    "start_lat, start_lon = route_coords[0]\n",
    "\n",
    "m = folium.Map(location=[start_lat, start_lon], zoom_start=4, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Add animated route if there are ≥2 points\n",
    "if len(route_coords) >= 2:\n",
    "    AntPath(route_coords, color=\"red\", weight=3, delay=1000).add_to(m)\n",
    "\n",
    "# Add markers\n",
    "for i, row in df_map.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"lat\"], row[\"lon\"]],\n",
    "        radius=6,\n",
    "        color=\"blue\",\n",
    "        fill=True,\n",
    "        fill_opacity=0.85,\n",
    "        popup=folium.Popup(row[\"hover_text\"], max_width=320),\n",
    "        tooltip=f\"#{i+1}: {row.get('entity','(unknown)')}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Save\n",
    "# ---------------------------------------------------------\n",
    "out_html = Path(\"outputs/interactive_geoparsing_map.html\")\n",
    "out_html.parent.mkdir(parents=True, exist_ok=True)\n",
    "m.save(str(out_html))\n",
    "print(f\"Interactive map saved → {out_html}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe90057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVG saved to: geo_pipeline_flowchart_pretty_svg.svg\n",
      "PNG saved to: geo_pipeline_flowchart_pretty_png.png\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def stage_node(g, node_id, title, subtitle, emoji, colors):\n",
    "    start, end = colors\n",
    "    label = f\"<<TABLE BORDER='0' CELLBORDER='0' CELLSPACING='0'>\\n\" \\\n",
    "            f\"  <TR><TD ALIGN='LEFT'><FONT POINT-SIZE='20'>{emoji}</FONT>  <B><FONT FACE='Inter,Helvetica' POINT-SIZE='14'>{title}</FONT></B></TD></TR>\\n\" \\\n",
    "            f\"  <TR><TD ALIGN='LEFT'><FONT FACE='Inter,Helvetica' POINT-SIZE='11' COLOR='#374151'>{subtitle}</FONT></TD></TR>\\n\" \\\n",
    "            f\"</TABLE>>\"\n",
    "    g.node(\n",
    "        node_id,\n",
    "        label=label,\n",
    "        shape='box',\n",
    "        style='rounded,filled',\n",
    "        gradientangle='90',\n",
    "        fillcolor=f\"{start}:{end}\",\n",
    "        color='#111827',\n",
    "        penwidth='1.6'\n",
    "    )\n",
    "\n",
    "# ---------- diagram ----------\n",
    "flow = Digraph('GeoPipelinePretty', format='svg')\n",
    "flow.attr(rankdir='TB', nodesep='0.7', ranksep='0.9', splines='spline')\n",
    "flow.attr('edge', color='#9CA3AF', penwidth='1.4', arrowsize='0.9')\n",
    "\n",
    "# palette (start, end)\n",
    "CLOUD = ('#F3F4F6', '#E5E7EB')\n",
    "BLUE = ('#DBEAFE', '#BFDBFE')\n",
    "AMBER = ('#FDE68A', '#FCD34D')\n",
    "GREEN = ('#BBF7D0', '#86EFAC')\n",
    "ROSE = ('#FECACA', '#FDA4AF')\n",
    "VIOLET = ('#E9D5FF', '#C4B5FD')\n",
    "PINK = ('#F5D0FE', '#FBCFE8')\n",
    "\n",
    "# clusters for grouping\n",
    "with flow.subgraph(name='cluster_input') as c:\n",
    "    c.attr(label='📥 Input', style='dashed', color='#D1D5DB')\n",
    "    stage_node(c, 'input', 'PDF Input', '(source documents)', '📄', CLOUD)\n",
    "\n",
    "with flow.subgraph(name='cluster_processing') as c:\n",
    "    c.attr(label='⚙️ Processing', style='dashed', color='#D1D5DB')\n",
    "    stage_node(c, 'pre', 'Preprocessing', 'cleaning, OCR fixes, stopwords, sentence splitting', '🧹', BLUE)\n",
    "    stage_node(c, 'nlp', 'NLP (NER)', 'spaCy / Stanza for location extraction', '🔍', AMBER)\n",
    "    stage_node(c, 'gaz', 'Gazetteer Matching', 'GeoNames + OSM + fuzzy matching', '🌍', GREEN)\n",
    "    stage_node(c, 'enrich', 'Enrichment', 'add coordinates, country codes, metadata', '📌', ROSE)\n",
    "    stage_node(c, 'route', 'Route Reconstruction', 'context window, transport keywords, symbolic filtering', '🛣️', VIOLET)\n",
    "\n",
    "with flow.subgraph(name='cluster_output') as c:\n",
    "    c.attr(label='📤 Output', style='dashed', color='#D1D5DB')\n",
    "    stage_node(c, 'vis', 'Visualization', 'map output + GeoJSON export', '🗺️', PINK)\n",
    "\n",
    "# edges\n",
    "edges = [('input','pre'), ('pre','nlp'), ('nlp','gaz'), ('gaz','enrich'), ('enrich','route'), ('route','vis')]\n",
    "for a, b in edges:\n",
    "    flow.edge(a, b)\n",
    "\n",
    "# export directly in both formats (avoid Digraph.from_dot_data, which doesn't exist)\n",
    "svg_path = flow.render('geo_pipeline_flowchart_pretty_svg', format='svg', cleanup=True)\n",
    "png_path = flow.render('geo_pipeline_flowchart_pretty_png', format='png', cleanup=True)\n",
    "\n",
    "print('SVG saved to:', svg_path)\n",
    "print('PNG saved to:', png_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf4de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
