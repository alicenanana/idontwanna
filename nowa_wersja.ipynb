{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2367eb",
   "metadata": {},
   "source": [
    "# [Setup]\n",
    "Block 1 (light preprocessing)\n",
    "Block 2 (gazetteer + spacy/stanza)\n",
    "Block 3 (NER functions)\n",
    "Block 7 (WordNet vague terms)\n",
    "Block 8 (motion/transport terms)\n",
    "Block 9 (sentence scoring prep)\n",
    "\n",
    "# [Pipeline]\n",
    "Block 1 (continue with PDF preprocessing)\n",
    "Block 4 (load cleaned sentences)\n",
    "Block 5 (NER + ML filtering)\n",
    "Block 6 (boosting small towns)\n",
    "Block 10 (GeoNames + Wikidata)\n",
    "Block 11 (Wikidata enrichment)\n",
    "Block 12 (clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfdb186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Pre-Block: Downloads & Setup ===\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"framenet_v17\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ccd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:10:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938ce4241e7540eba330e5d2fe24040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:10:56 INFO: Downloaded file to /Users/alicja/stanza_resources/resources.json\n",
      "2025-07-25 11:10:56 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-07-25 11:10:57 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-07-25 11:10:57 WARNING: GPU requested, but is not available!\n",
      "2025-07-25 11:10:57 INFO: Using device: cpu\n",
      "2025-07-25 11:10:57 INFO: Loading: tokenize\n",
      "2025-07-25 11:10:57 INFO: Loading: mwt\n",
      "2025-07-25 11:10:57 INFO: Loading: ner\n",
      "2025-07-25 11:11:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# === Block 1: Imports ===\n",
    "\n",
    "# Standard\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import yaml\n",
    "import json\n",
    "# Third-party\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "import contractions\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords, wordnet as wn, framenet as fn\n",
    "\n",
    "# Helpers\n",
    "from nlp_helpers import (\n",
    "    init_nlp,\n",
    "    get_stopwords,\n",
    "    extract_text_from_pdf,\n",
    "    load_config,\n",
    "    normalize_punctuation,  \n",
    "    clean_light,\n",
    "    preprocess_text,\n",
    "    segment_sentences,\n",
    "    clean_heavy\n",
    ")\n",
    "from gazetteer_helpers import build_gazetteer\n",
    "\n",
    "\n",
    "# === NLP Initialization ===\n",
    "nlp, stanza_pipeline = init_nlp()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d76d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 2: Extract text from PDF using PyMuPDF.  pages 28-148\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str, start_page: int, end_page: int) -> str:\n",
    "    \"\"\"Extracts and returns text from a PDF given a path and page range.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = doc[start_page:end_page]\n",
    "    return \"\\n\".join(page.get_text() for page in pages)\n",
    "\n",
    "\n",
    "#Load configuration from YAML file\n",
    "def load_config(config_path: str = \"config.yaml\") -> dict:\n",
    "    \"\"\"Loads YAML configuration file and returns it as a dictionary.\"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        \n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "raw_text = extract_text_from_pdf(pdf_conf[\"path\"], pdf_conf[\"start_page\"], pdf_conf[\"end_page\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e33e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 3: Text Preprocessing Functions ===\n",
    "\n",
    "from nlp_helpers import (\n",
    "    init_nlp,\n",
    "    get_stopwords,\n",
    "    extract_text_from_pdf,\n",
    "    load_config,\n",
    "    normalize_punctuation,\n",
    "    clean_light,\n",
    "    preprocess_text,\n",
    "    segment_sentences,\n",
    "    clean_heavy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2737feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned versions saved.\n"
     ]
    }
   ],
   "source": [
    "# === Block 4: Clean and Save Tagged + NLP Versions ===\n",
    "\n",
    "config = load_config()\n",
    "pdf_conf = config[\"pdf\"]\n",
    "gaz_conf = config[\"gazetteer\"]\n",
    "\n",
    "# Output filenames based on input PDF\n",
    "base_name = Path(pdf_conf[\"path\"]).stem\n",
    "tagged_path = Path(\"outputs\") / f\"cleaned_{base_name}_geoparsing.txt\"\n",
    "heavy_path = Path(\"outputs\") / f\"cleaned_{base_name}_nlp.txt\"\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Check if cleaned files already exist\n",
    "if tagged_path.exists() and heavy_path.exists():\n",
    "    print(\"üìÇ Found existing cleaned files, skipping processing.\")\n",
    "else:\n",
    "    raw_text = extract_text_from_pdf(\n",
    "        pdf_conf[\"path\"],\n",
    "        pdf_conf[\"start_page\"],\n",
    "        pdf_conf[\"end_page\"]\n",
    "    )\n",
    "\n",
    "    light_cleaned = clean_light(raw_text)\n",
    "    sentences = segment_sentences(light_cleaned, nlp)\n",
    "\n",
    "    # Optional tagging for geoparsing\n",
    "    sentences_with_tags = [f\"[SENT {i+1}] {s}\" for i, s in enumerate(sentences)]\n",
    "\n",
    "    # Heavy-cleaned version for NLP\n",
    "    all_stops = get_stopwords(nlp)\n",
    "    heavy_cleaned = clean_heavy(light_cleaned, nlp, all_stops)\n",
    "\n",
    "    # Save both outputs\n",
    "    with open(tagged_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(sentences_with_tags))\n",
    "    with open(heavy_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(heavy_cleaned)\n",
    "\n",
    "    print(\"‚úÖ Cleaned versions saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0aabe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2476 tagged sentences\n"
     ]
    }
   ],
   "source": [
    "# === Block 5: Load tagged sentences ===\n",
    "with open(\"outputs/cleaned_MotorcycleDiaries_geoparsing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sentence_data = []\n",
    "for line in lines:\n",
    "    match = re.match(r\"\\[SENT (\\d+)\\] (.+)\", line.strip())\n",
    "    if match:\n",
    "        sent_id = int(match.group(1))\n",
    "        sent_text = match.group(2)\n",
    "        sentence_data.append((sent_id, sent_text))\n",
    "\n",
    "print(f\" Loaded {len(sentence_data)} tagged sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41623c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç No gazetteer file found, building...\n",
      "üåç Downloading cities from GeoNames...\n",
      "‚úÖ AR: Loaded 5000 cities.\n",
      "‚úÖ CL: Loaded 5000 cities.\n",
      "‚úÖ PE: Loaded 5000 cities.\n",
      "‚úÖ CO: Loaded 5000 cities.\n",
      "‚úÖ VE: Loaded 5000 cities.\n",
      "‚úÖ BO: Loaded 5000 cities.\n",
      "‚úÖ EC: Loaded 5000 cities.\n",
      "‚úÖ PA: Loaded 5000 cities.\n",
      "‚úÖ CR: Loaded 5000 cities.\n",
      "‚úÖ GT: Loaded 5000 cities.\n",
      "‚úÖ MX: Loaded 5000 cities.\n",
      "‚úÖ CU: Loaded 5000 cities.\n",
      "‚úÖ BR: Loaded 5000 cities.\n",
      "‚úÖ GY: Loaded 906 cities.\n",
      "‚úÖ PY: Loaded 5000 cities.\n",
      "‚úÖ SR: Loaded 548 cities.\n",
      "‚úÖ UY: Loaded 1077 cities.\n",
      "‚úÖ HN: Loaded 5000 cities.\n",
      "‚úÖ SV: Loaded 4824 cities.\n",
      "‚úÖ NI: Loaded 3039 cities.\n",
      "üìå Total unique cities gathered: 50976\n",
      "‚úÖ Gazetteer built and saved.\n"
     ]
    }
   ],
   "source": [
    "# === Block 6: Build Gazetteer ===\n",
    "\n",
    "gazetteer_path = Path(\"outputs/gazetteer_cities.json\")\n",
    "\n",
    "if gazetteer_path.exists():\n",
    "    print(\"üìÇ Found existing gazetteer file, loading...\")\n",
    "    with open(gazetteer_path) as f:\n",
    "        gazetteer = set(json.load(f))\n",
    "else:\n",
    "    print(\"üåç No gazetteer file found, building...\")\n",
    "    gazetteer = build_gazetteer(\n",
    "        username=gaz_conf[\"username\"],\n",
    "        countries=gaz_conf[\"countries\"],\n",
    "        max_rows=gaz_conf[\"max_rows\"]\n",
    "    )\n",
    "    with open(gazetteer_path, \"w\") as f:\n",
    "        json.dump(sorted(gazetteer), f, indent=2)\n",
    "    print(\"‚úÖ Gazetteer built and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70be0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Running ensemble NER + gazetteer matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER + Gazetteer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2476/2476 [38:23<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: outputs/geoparsing_ner_ensemble.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 7: NER + Gazetteer with optional Stanza and progress bar (with short-name filtering)\n",
    "\n",
    "\n",
    "# Toggle Stanza use (set to False to avoid slowdowns)\n",
    "USE_STANZA = False\n",
    "\n",
    "def extract_entities_spacy(text):\n",
    "    doc = nlp_spacy(text)\n",
    "    return [(ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "            for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "\n",
    "def extract_entities_stanza(text):\n",
    "    doc = stanza_pipeline(text)\n",
    "    results = []\n",
    "    for sent in doc.sentences:\n",
    "        for ent in sent.ents:\n",
    "            if ent.type in {\"GPE\", \"LOC\"}:\n",
    "                results.append((ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "    return results\n",
    "\n",
    "def match_gazetteer(text, known_places):\n",
    "    \"\"\"\n",
    "    gazetteer matching using word-boundary regex to reduce false positives.\n",
    "    Matches longer place names first to avoid substring collisions.\n",
    "    filters out short entries (‚â§3 chars) to reduce noise.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    matches = []\n",
    "\n",
    "    # Ignore very short place names (common source of noise)\n",
    "    known_places_sorted = sorted([p for p in known_places if len(p) > 3], key=len, reverse=True)\n",
    "\n",
    "    for place in known_places_sorted:\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(place.lower()))\n",
    "        for m in re.finditer(pattern, text_lower):\n",
    "            start, end = m.start(), m.end()\n",
    "            match_text = text[start:end]\n",
    "            matches.append((match_text, \"GAZETTEER\", start, end))\n",
    "    return matches\n",
    "\n",
    "def combine_ner_gazetteer(sentences, gazetteer):\n",
    "    results = []\n",
    "    for sent_id, text in tqdm(sentences, desc=\"NER + Gazetteer\"):\n",
    "        try:\n",
    "            ents_spacy = extract_entities_spacy(text)\n",
    "            ents_stanza = extract_entities_stanza(text) if USE_STANZA else []\n",
    "            ents_gazetteer = match_gazetteer(text, gazetteer)\n",
    "\n",
    "            all_ents = ents_spacy + ents_stanza + ents_gazetteer\n",
    "            seen = set()\n",
    "            for ent_text, label, start, end in all_ents:\n",
    "                norm = ent_text.lower()\n",
    "                if not any(fuzz.ratio(norm, s) > 90 for s in seen):\n",
    "                    seen.add(norm)\n",
    "                    results.append({\n",
    "                        \"sentence_id\": sent_id,\n",
    "                        \"entity\": ent_text,\n",
    "                        \"entity_norm\": norm,\n",
    "                        \"label\": label,\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end,\n",
    "                        \"sentence\": text\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\" Error in sentence {sent_id}: {e}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"üîé Running ensemble NER + gazetteer matching...\")\n",
    "entities_combined = combine_ner_gazetteer(sentence_data, gazetteer)\n",
    "\n",
    "df_combined = pd.DataFrame(entities_combined)\n",
    "df_combined.to_csv(\"outputs/geoparsing_ner_ensemble.csv\", index=False)\n",
    "print(\" Saved: outputs/geoparsing_ner_ensemble.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       198\n",
      "           1       1.00      1.00      1.00        35\n",
      "\n",
      "    accuracy                           1.00       233\n",
      "   macro avg       1.00      1.00      1.00       233\n",
      "weighted avg       1.00      1.00      1.00       233\n",
      "\n",
      " ML filtering complete\n"
     ]
    }
   ],
   "source": [
    "# Block 8: Train ML model to filter real geographic entities\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load ensemble NER output\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble.csv\")\n",
    "\n",
    "# Create features\n",
    "df[\"label_GPE\"] = (df[\"label\"] == \"GPE\").astype(int)\n",
    "df[\"label_LOC\"] = (df[\"label\"] == \"LOC\").astype(int)\n",
    "\n",
    "symbolic_keywords = [\"freedom\", \"struggle\", \"liberation\", \"future\", \"dream\", \"cause\", \"revolution\", \"hope\", \"people\"]\n",
    "df[\"symbolic_flagged\"] = df[\"sentence\"].str.contains(\"|\".join(symbolic_keywords), flags=re.IGNORECASE, na=False)\n",
    "\n",
    "expected_countries = [\"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\", \"venezuela\"]\n",
    "df[\"country_valid\"] = df[\"sentence\"].str.lower().apply(\n",
    "    lambda x: int(any(country in x for country in expected_countries))\n",
    ")\n",
    "\n",
    "df[\"fuzzy_score\"] = df.apply(\n",
    "    lambda row: fuzz.ratio(str(row[\"entity\"]).lower(), str(row[\"entity_norm\"]).lower()), axis=1\n",
    ")\n",
    "df[\"fuzzy_score_scaled\"] = df[\"fuzzy_score\"] / 100.0\n",
    "\n",
    "df[\"auto_label\"] = ((df[\"country_valid\"] == 1) & (~df[\"symbolic_flagged\"])).astype(int)\n",
    "\n",
    "# Train/test split\n",
    "features = df[[\"label_GPE\", \"label_LOC\", \"symbolic_flagged\", \"country_valid\", \"fuzzy_score_scaled\"]]\n",
    "target = df[\"auto_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, stratify=target, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(\" Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict on all\n",
    "X_all_scaled = scaler.transform(features)\n",
    "df[\"geo_confidence\"] = clf.predict_proba(X_all_scaled)[:, 1]\n",
    "df[\"filtered_out_ml\"] = df[\"geo_confidence\"] < 0.5\n",
    "\n",
    "# Save outputs\n",
    "df_filtered = df[~df[\"filtered_out_ml\"]].copy()\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_flagged_with_ml.csv\", index=False)\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\", index=False)\n",
    "\n",
    "print(\" ML filtering complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Could not load enriched data: \"['country'] not in index\"\n",
      "Saved: geoparsing_ensemble_final_ml_boosted.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 9: Boost confidence for small towns in South America\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\")\n",
    "\n",
    "# Define target countries\n",
    "target_countries = [\n",
    "    \"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\",\n",
    "    \"venezuela\", \"ecuador\", \"brazil\", \"uruguay\", \"paraguay\"\n",
    "]\n",
    "\n",
    "# Prepare for matching\n",
    "df[\"entity_norm_lower\"] = df[\"entity_norm\"].str.lower()\n",
    "\n",
    "# Try to load population-enriched data\n",
    "try:\n",
    "    enriched = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "    enriched[\"entity_norm_lower\"] = enriched[\"entity_norm\"].str.lower()\n",
    "\n",
    "    df = df.merge(\n",
    "        enriched[[\"entity_norm_lower\", \"country\", \"population\"]],\n",
    "        on=\"entity_norm_lower\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Mark small towns\n",
    "    df[\"boost_small_town\"] = (\n",
    "        df[\"population\"].fillna(0).lt(50000) &\n",
    "        df[\"country\"].str.lower().isin(target_countries)\n",
    "    )\n",
    "    print(f\" Boosted {df['boost_small_town'].sum()} small towns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Could not load enriched data: {e}\")\n",
    "    df[\"boost_small_town\"] = False\n",
    "\n",
    "# Apply confidence boost\n",
    "df[\"geo_confidence_boosted\"] = df[\"geo_confidence\"]\n",
    "df.loc[df[\"boost_small_town\"], \"geo_confidence_boosted\"] = 0.9\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\", index=False)\n",
    "print(\"Saved: geoparsing_ensemble_final_ml_boosted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b88ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ML-learned vague terms: 0\n",
      " WordNet vague terms: 1786\n",
      " Removed 0 entities (ML or WordNet flagged)\n",
      " 'country' column not found. Cannot apply region filter.\n",
      " Saved: geoparsing_ner_ensemble_filtered_southamerica.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Block 10: vague term filtering (ML + WordNet) + South America region restriction\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\")\n",
    "\n",
    "#  Normalize entity name \n",
    "df[\"entity_norm\"] = df[\"entity_norm\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# === STEP 1: ML-learned vague terms ===\n",
    "term_stats = df.groupby(\"entity_norm\").agg({\n",
    "    \"geo_confidence_boosted\": \"mean\",\n",
    "    \"entity\": \"count\"\n",
    "}).rename(columns={\n",
    "    \"entity\": \"freq\",\n",
    "    \"geo_confidence_boosted\": \"avg_conf\"\n",
    "}).reset_index()\n",
    "\n",
    "learned_vague = term_stats[\n",
    "    (term_stats[\"freq\"] >= 3) &\n",
    "    (term_stats[\"avg_conf\"] < 0.35)\n",
    "][\"entity_norm\"].tolist()\n",
    "print(f\" ML-learned vague terms: {len(learned_vague)}\")\n",
    "\n",
    "# === STEP 2: WordNet vague terms ===\n",
    "location_synsets = [\n",
    "    wn.synset(\"location.n.01\"),\n",
    "    wn.synset(\"region.n.01\"),\n",
    "    wn.synset(\"area.n.01\"),\n",
    "    wn.synset(\"place.n.01\"),\n",
    "    wn.synset(\"territory.n.01\")\n",
    "]\n",
    "\n",
    "vague_terms_wordnet = set()\n",
    "for syn in location_synsets:\n",
    "    for hypo in syn.closure(lambda s: s.hyponyms()):\n",
    "        for lemma in hypo.lemmas():\n",
    "            vague_terms_wordnet.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "print(f\" WordNet vague terms: {len(vague_terms_wordnet)}\")\n",
    "\n",
    "# === STEP 3: Combined vague term filtering ===\n",
    "df[\"is_vague_combined\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"entity_norm\"] in learned_vague or\n",
    "        (row[\"label\"] == \"LOC\" and row[\"entity_norm\"] in vague_terms_wordnet)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_filtered = df[~df[\"is_vague_combined\"]].copy()\n",
    "print(f\" Removed {df['is_vague_combined'].sum()} entities (ML or WordNet flagged)\")\n",
    "\n",
    "# === STEP 4: South America region restriction ===\n",
    "sa_countries = {\n",
    "    \"argentina\", \"bolivia\", \"brazil\", \"chile\", \"colombia\",\n",
    "    \"ecuador\", \"guyana\", \"paraguay\", \"peru\", \"suriname\", \"uruguay\", \"venezuela\"\n",
    "}\n",
    "\n",
    "if \"country\" in df_filtered.columns:\n",
    "    df_filtered[\"country\"] = df_filtered[\"country\"].astype(str).str.lower()\n",
    "    df_filtered = df_filtered[df_filtered[\"country\"].isin(sa_countries)].copy()\n",
    "    print(f\"üåé After SA region filter: {len(df_filtered)} rows\")\n",
    "else:\n",
    "    print(\" 'country' column not found. Cannot apply region filter.\")\n",
    "\n",
    "# === Final Save ===\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ner_ensemble_filtered_southamerica.csv\", index=False)\n",
    "print(\" Saved: geoparsing_ner_ensemble_filtered_southamerica.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('personnel_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_vehicle.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('weapons_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warplane.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warship.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('tank.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('half_track.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('bomber.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('fighter.n.02') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_plane.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('technical.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('minivan.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "Scoring entities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:15<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Saved: outputs/geoparsing_scored_candidates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 11 : Contextual Scoring with enhanced inputs\n",
    "import spacy\n",
    "import re\n",
    "import dateparser.search\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn, framenet as fn\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === Load motion verbs ===\n",
    "motion_frames = ['Motion', 'Travel', 'Self_motion', 'Arriving', 'Departing']\n",
    "motion_verbs = set()\n",
    "for frame in motion_frames:\n",
    "    try:\n",
    "        for lu in fn.frame_by_name(frame).lexUnit.values():\n",
    "            if lu['name'].endswith('.v'):\n",
    "                motion_verbs.add(lu['name'].split('.')[0].lower())\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# === Load transport terms ===\n",
    "vehicle_syn = wn.synset('vehicle.n.01')\n",
    "transport_terms = set()\n",
    "for syn in vehicle_syn.closure(lambda s: s.hyponyms()):\n",
    "    for lemma in syn.lemmas():\n",
    "        transport_terms.add(lemma.name().lower().replace('_', ' '))\n",
    "\n",
    "# === Load filtered file after SA restriction ===\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble_filtered_southamerica.csv\")\n",
    "\n",
    "# === Load sentence map ===\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "sentence_map = {}\n",
    "for line in lines:\n",
    "    if line.strip().startswith(\"[SENT\"):\n",
    "        sid = int(line.split(\"]\")[0].split()[1])\n",
    "        sentence_map[sid] = line.split(\"]\")[1].strip()\n",
    "\n",
    "# === Scoring logic ===\n",
    "scored_rows = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring entities\"):\n",
    "    sid = row[\"sentence_id\"]\n",
    "    entity = row[\"entity\"]\n",
    "    norm = row[\"entity_norm\"]\n",
    "    label = row[\"label\"]\n",
    "    sentence = sentence_map.get(sid, \"\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    score = 0\n",
    "    entity_token = None\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"} and ent.text.lower().strip() == entity.lower().strip():\n",
    "            entity_token = ent.root\n",
    "            break\n",
    "\n",
    "    if not entity_token:\n",
    "        continue\n",
    "\n",
    "    sentence_lower = sentence.lower()\n",
    "    entity_lower = entity.lower()\n",
    "\n",
    "    if any(verb in sentence_lower for verb in motion_verbs):\n",
    "        score += 1\n",
    "    if any(term in sentence_lower for term in transport_terms):\n",
    "        score += 1\n",
    "    if re.search(r'\\b(in|to|at)\\s+' + re.escape(entity_lower) + r'\\b', sentence_lower):\n",
    "        score += 1\n",
    "    if re.search(r\"\\b\" + re.escape(entity_lower) + r\"['‚Äô]s\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bof\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bfor\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if dateparser.search.search_dates(sentence):\n",
    "        score += 1\n",
    "\n",
    "    scored_rows.append({\n",
    "        \"sentence_id\": sid,\n",
    "        \"entity\": entity,\n",
    "        \"entity_norm\": norm,\n",
    "        \"label\": label,\n",
    "        \"sentence\": sentence,\n",
    "        \"score\": score,\n",
    "        \"latitude\": row.get(\"latitude\"),\n",
    "        \"longitude\": row.get(\"longitude\"),\n",
    "        \"country\": row.get(\"country\")\n",
    "    })\n",
    "\n",
    "scored_df = pd.DataFrame(scored_rows)\n",
    "scored_df = scored_df.sort_values(by=[\"score\", \"sentence_id\"], ascending=[False, True])\n",
    "scored_df.to_csv(\"outputs/geoparsing_scored_candidates.csv\", index=False)\n",
    "print(\"üìÑ Saved: outputs/geoparsing_scored_candidates.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ea256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Querying GeoNames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GeoNames queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:55<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Querying Wikidata for missing coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wikidata queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:35<00:00,  1.22it/s]\n",
      "/var/folders/62/b5nr017s2w11vxfg0s2020wh0000gn/T/ipykernel_92610/1796646064.py:69: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"wikidata_lat\"])\n",
      "/var/folders/62/b5nr017s2w11vxfg0s2020wh0000gn/T/ipykernel_92610/1796646064.py:70: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"wikidata_lon\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß≠ Querying OpenStreetMap for unresolved locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OSM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:11<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 12: Enrich entities with coordinates via GeoNames ‚Üí Wikidata ‚Üí OSM\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load scored candidates ===\n",
    "df = pd.read_csv(\"outputs/geoparsing_scored_candidates.csv\")\n",
    "places = df[\"entity_norm\"].dropna().unique()\n",
    "geonames_username = \"alicjab\"\n",
    "\n",
    "geo_data = {}\n",
    "\n",
    "# === Step 1: GeoNames ===\n",
    "print(\"üåç Querying GeoNames...\")\n",
    "for place in tqdm(places, desc=\"GeoNames queries\"):\n",
    "    try:\n",
    "        params = {\"q\": place, \"maxRows\": 1, \"username\": geonames_username}\n",
    "        r = requests.get(\"http://api.geonames.org/searchJSON\", params=params, timeout=10).json()\n",
    "        if not r.get(\"geonames\"):\n",
    "            raise ValueError(\"No result\")\n",
    "        g = r[\"geonames\"][0]\n",
    "        geo_data[place] = {\n",
    "            \"latitude\": float(g[\"lat\"]),\n",
    "            \"longitude\": float(g[\"lng\"]),\n",
    "            \"country\": g.get(\"countryName\"),\n",
    "            \"population\": int(g.get(\"population\", 0))\n",
    "        }\n",
    "    except Exception:\n",
    "        geo_data[place] = {\"latitude\": None, \"longitude\": None, \"country\": None, \"population\": None}\n",
    "    time.sleep(1)  # GeoNames rate limit\n",
    "\n",
    "geo_df = pd.DataFrame.from_dict(geo_data, orient=\"index\")\n",
    "geo_df.index.name = \"entity_norm\"\n",
    "geo_df.reset_index(inplace=True)\n",
    "\n",
    "# === Merge GeoNames results ===\n",
    "df_enriched = df.merge(geo_df, on=\"entity_norm\", how=\"left\")\n",
    "\n",
    "# === Step 2: Wikidata fallback ===\n",
    "print(\"üîÅ Querying Wikidata for missing coordinates...\")\n",
    "if \"latitude\" not in df_enriched.columns:\n",
    "    df_enriched[\"latitude\"] = None\n",
    "    df_enriched[\"longitude\"] = None\n",
    "\n",
    "missing_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_wikidata_coords(place):\n",
    "    try:\n",
    "        search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"search\": place,\n",
    "            \"language\": \"en\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        r = requests.get(search_url, params=search_params, timeout=10).json()\n",
    "        if not r[\"search\"]:\n",
    "            return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "        qid = r[\"search\"][0][\"id\"]\n",
    "        entity = requests.get(f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\").json()\n",
    "        coords = entity[\"entities\"][qid][\"claims\"].get(\"P625\", [{}])[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {})\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": coords.get(\"latitude\"), \"wikidata_lon\": coords.get(\"longitude\")}\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "\n",
    "wikidata_results = pd.DataFrame([query_wikidata_coords(p) for p in tqdm(missing_places, desc=\"Wikidata queries\")])\n",
    "df_enriched = df_enriched.merge(wikidata_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"wikidata_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"wikidata_lon\"])\n",
    "\n",
    "# === Step 3: OSM fallback ===\n",
    "print(\"üß≠ Querying OpenStreetMap for unresolved locations...\")\n",
    "nominatim_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_osm(place):\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://nominatim.openstreetmap.org/search\",\n",
    "            params={\"q\": place, \"format\": \"json\", \"limit\": 1},\n",
    "            headers={\"User-Agent\": \"Geoparser/1.0\"},\n",
    "            timeout=10\n",
    "        ).json()\n",
    "        if not r:\n",
    "            return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "        return {\n",
    "            \"entity_norm\": place,\n",
    "            \"osm_lat\": float(r[0][\"lat\"]),\n",
    "            \"osm_lon\": float(r[0][\"lon\"])\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "\n",
    "osm_results = pd.DataFrame([query_osm(p) for p in tqdm(nominatim_places, desc=\"OSM queries\")])\n",
    "df_enriched = df_enriched.merge(osm_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"osm_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"osm_lon\"])\n",
    "\n",
    "\n",
    "df_enriched.to_csv(\"outputs/geoparsing_final_enriched.csv\", index=False)\n",
    "print(\"‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚Ü©Ô∏è Reweighting outliers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 2712.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final post-reweighting size: 107 rows\n",
      "üåé Filtering by continent...\n",
      "Loading formatted geocoded file...\n",
      "üåç Removed 11 non-South American entries\n",
      "Saved final cleaned + reweighted + region-filtered version: geoparsing_final_scored_clustered.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 13: Geographic Outlier Filtering using DBSCAN (corrected input + centroid-aware reweighting + continent filtering)\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "\n",
    "# Drop rows without coordinates\n",
    "df_geo = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "coords_rad = np.radians(df_geo[[\"latitude\", \"longitude\"]].values)\n",
    "\n",
    "# Cluster with DBSCAN using haversine distance (in radians)\n",
    "clustering = DBSCAN(eps=0.5, min_samples=2, metric='haversine')\n",
    "df_geo[\"geo_cluster\"] = clustering.fit_predict(coords_rad)\n",
    "\n",
    "# === Compute centroids of each valid cluster ===\n",
    "centroids = (\n",
    "    df_geo[df_geo[\"geo_cluster\"] != -1]\n",
    "    .groupby(\"geo_cluster\")[[\"latitude\", \"longitude\"]]\n",
    "    .mean()\n",
    "    .to_dict(\"index\")\n",
    ")\n",
    "\n",
    "# === Reweight scores for outliers ===\n",
    "outliers = df_geo[df_geo[\"geo_cluster\"] == -1].copy()\n",
    "non_outliers = df_geo[df_geo[\"geo_cluster\"] != -1].copy()\n",
    "\n",
    "reweighted = []\n",
    "for _, row in tqdm(outliers.iterrows(), total=len(outliers), desc=\"‚Ü©Ô∏è Reweighting outliers\"):\n",
    "    min_dist_km = float(\"inf\")\n",
    "    entity_point = (row[\"latitude\"], row[\"longitude\"])\n",
    "\n",
    "    for c in centroids.values():\n",
    "        centroid_point = (c[\"latitude\"], c[\"longitude\"])\n",
    "        dist = geodesic(entity_point, centroid_point).km\n",
    "        if dist < min_dist_km:\n",
    "            min_dist_km = dist\n",
    "\n",
    "    # If outlier is <500 km from any cluster, keep it with downgraded score\n",
    "    if min_dist_km < 500:\n",
    "        row[\"geo_cluster\"] = -2  # kept but marked as downgraded outlier\n",
    "        row[\"geo_score_adjusted\"] = row.get(\"score\", 0) - 1\n",
    "        reweighted.append(row)\n",
    "\n",
    "# Combine cleaned + downgraded outliers\n",
    "final_df = pd.concat([non_outliers, pd.DataFrame(reweighted)], ignore_index=True)\n",
    "print(f\" Final post-reweighting size: {len(final_df)} rows\")\n",
    "\n",
    "# === Filter by continent (keep only South America) ===\n",
    "def get_continent(lat, lon):\n",
    "    try:\n",
    "        results = rg.search((lat, lon), mode=1)\n",
    "        cc = results[0]['cc']\n",
    "        # ISO country codes in South America\n",
    "        south_america = {\n",
    "            \"AR\", \"BO\", \"BR\", \"CL\", \"CO\", \"EC\", \"GY\", \"PY\", \"PE\", \"SR\", \"UY\", \"VE\",  # South America\n",
    "             \"MX\", \"GT\", \"HN\", \"SV\", \"NI\", \"CR\", \"PA\"  # Central America / Mesoamerica\n",
    "        }\n",
    "        return 'South America' if cc in south_america else 'Other'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "print(\"üåé Filtering by continent...\")\n",
    "final_df[\"continent\"] = final_df.apply(\n",
    "    lambda row: get_continent(row[\"latitude\"], row[\"longitude\"]), axis=1\n",
    ")\n",
    "\n",
    "before_filter = len(final_df)\n",
    "final_df = final_df[final_df[\"continent\"] == \"South America\"].copy()\n",
    "print(f\"üåç Removed {before_filter - len(final_df)} non-South American entries\")\n",
    "\n",
    "# Save final output\n",
    "final_df.to_csv(\"outputs/geoparsing_final_scored_clustered.csv\", index=False)\n",
    "print(\"Saved final cleaned + reweighted + region-filtered version: geoparsing_final_scored_clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e8424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "geo": "geo",
         "hovertemplate": "<b>%{hovertext}</b><br><br>entity=%{text}<br>latitude=%{lat}<br>longitude=%{lon}<br>cluster=%{marker.color}<extra></extra>",
         "hovertext": [
          "Antofagasta (cluster 0)",
          "Chile (cluster 0)",
          "Bogot√° (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Osorno (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Peru (cluster 0)",
          "Venezuela (cluster 0)",
          "Venezuela (cluster 0)",
          "Bogot√° (cluster 0)",
          "Colombia (cluster 0)",
          "Caracas (cluster 0)",
          "Chile (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Granado (cluster 0)",
          "Chile (cluster 0)",
          "San Carlos de Bariloche (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Peru (cluster 0)",
          "Chile (cluster 0)",
          "Iquique (cluster 0)",
          "Arica (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Santiago de Chile (cluster 0)",
          "Puno (cluster 0)",
          "Lima (cluster 0)",
          "Peru (cluster 0)",
          "peru (cluster 0)",
          "Argentina (cluster 0)",
          "Manaos (cluster 0)",
          "Peru (cluster 0)",
          "Bogot√° (cluster 0)",
          "Colombia (cluster 0)",
          "Venezuela (cluster 0)",
          "Colombia (cluster 0)",
          "C√≥rdoba (cluster 0)",
          "Baquedano (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Valpara√≠so (cluster 0)",
          "Chile (cluster 0)",
          "Americas (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Arica (cluster 0)",
          "Peru (cluster 0)",
          "Chile (cluster 0)",
          "Chuquicamata (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Andes (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "C√≥rdoba (cluster 0)",
          "Santa Luc√≠a (cluster 0)",
          "Tarat√° (cluster 0)",
          "Peru (cluster 0)",
          "Pucallpa (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Lima (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Lima (cluster 0)",
          "Chile (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "South America (cluster 0)",
          "Bolivia (cluster 0)",
          "Suqu√≠a (cluster 0)",
          "Peru (cluster 0)",
          "Quechua (cluster 0)",
          "Colombia (cluster 0)",
          "Colombia (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)"
         ],
         "lat": {
          "bdata": "i0YCvnmlN8AAAAAAAIBAwDgteNFXcBJAAAAAAAAAQcAAAAAAAABBwHyqqqqqSkTAAAAAAACAQMAAAAAAAIBAwAAAAAAAAEHAzczMzMzMIsAAAAAAAAAgQAAAAAAAACBAOC140VdwEkAAAAAAAAAQQAAAAAAAACVAAAAAAACAQMDNzMzMzMwiwM3MzMzMzCLAYdtHE5TTJ0AAAAAAAIBAwDMzMzMzk0TAAAAAAACAQMAAAAAAAABBwM3MzMzMzCLAAAAAAACAQMDXEvJBzzY0wLIxCt2AejLAzczMzMzMIsAAAAAAAABBwAAAAAAAgEDAAAAAAACAQMAAAAAAAABBwAAAAAAAuEDAdJX8YsmvL8DxvFRszB8owM3MzMzMzCLAzczMzMzMIsAAAAAAAABBwDaEdMKVDQnAzczMzMzMIsA4LXjRV3ASQAAAAAAAABBAAAAAAAAAIEAAAAAAAAAQQAirqqqqaj/A4A55cBzJRsAAAAAAAABBwAAAAAAAgEDAKxoJ+OaFQMAAAAAAAIBAwAAAAAAAADNAAAAAAACAQMAAAAAAAIBAwLIxCt2AejLAzczMzMzMIsAAAAAAAIBAwPjeY9lhSjbAAAAAAACAQMAAAAAAAABBwAXKARZUnxZAAAAAAACAQMAAAAAAAABBwAAAAAAAAEHAAAAAAAAAQcAAAAAAAABBwAAAAAAAgEDACKuqqqpqP8CydlDYpxApQKrXLQJjeTHAzczMzMzMIsDMUvwpn8MgwM3MzMzMzCLAzczMzMzMIsDNzMzMzMwiwAAAAAAAAEHA8bxUbMwfKMDNzMzMzMwiwM3MzMzMzCLA8bxUbMwfKMAAAAAAAIBAwM3MzMzMzCLAAAAAAAAAQcAAAAAAAABBwAAAAAAAgEDAAAAAAACAQMAAAAAAAAA1wKXbvwGPDjHAFyr/Wl5jP8DNzMzMzMwiwDjUlAlkZzvAAAAAAAAAEEAAAAAAAAAQQAAAAAAAAEHAAAAAAAAAQcAAAAAAAIBAwAAAAAAAgEDA",
          "dtype": "f8"
         },
         "legendgroup": "",
         "lon": {
          "bdata": "i0YCvnmZUcAAAAAAAMBRwKwcWmQ7hVLAAAAAAAAAUMAAAAAAAABQwMKqqqqqSlLAAAAAAADAUcAAAAAAAMBRwAAAAAAAAFDAAAAAAAAAU8AAAAAAAMBQwAAAAAAAwFDArBxaZDuFUsAAAAAAAFBSwKS7u7u7u1DAAAAAAADAUcAAAAAAAABTwAAAAAAAAFPApR5YRIN4VcAAAAAAAMBRwDMzMzMz01HAAAAAAADAUcAAAAAAAABQwAAAAAAAAFPAAAAAAADAUcBE+u3rwIlRwEEdRYyNlFHAAAAAAAAAU8AAAAAAAABQwAAAAAAAwFHAAAAAAADAUcAAAAAAAABQwJqZmZmZqVHA0IIt2IKBUcDgXXhvVkJTwAAAAAAAAFPAAAAAAAAAU8AAAAAAAABQwMzXwLHC/U3AAAAAAAAAU8CsHFpkO4VSwAAAAAAAUFLAAAAAAADAUMAAAAAAAFBSwKS7u7u7C1DAV65cFGIEUsAAAAAAAABQwAAAAAAAwFHAIUNlh6nnUcAAAAAAAMBRwAAAAAAAAFjAAAAAAADAUcAAAAAAAMBRwEEdRYyNlFHAAAAAAAAAU8AAAAAAAMBRwPomLHW5OVHAAAAAAADAUcAAAAAAAABQwK299Jsu+FLAAAAAAADAUcAAAAAAAABQwAAAAAAAAFDAAAAAAAAAUMAAAAAAAABQwAAAAAAAwFHApLu7u7sLUMBrc9jMg21VwBEdAkcCglHAAAAAAAAAU8CdTDfkeqJSwAAAAAAAAFPAAAAAAAAAU8AAAAAAAABTwAAAAAAAAFDA4F14b1ZCU8AAAAAAAABTwAAAAAAAAFPA4F14b1ZCU8AAAAAAAMBRwAAAAAAAAFPAAAAAAAAAUMAAAAAAAABQwAAAAAAAwFHAAAAAAADAUcAAAAAAAIBNwDPlIEpwP1DANO5ytqUMUMAAAAAAAABTwD0q/u8IkVHAAAAAAABQUsAAAAAAAFBSwAAAAAAAAFDAAAAAAAAAUMAAAAAAAMBRwAAAAAAAwFHA",
          "dtype": "f8"
         },
         "marker": {
          "color": {
           "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "",
         "showlegend": false,
         "text": [
          "Antofagasta",
          "Chile",
          "Bogot√°",
          "Argentina",
          "Argentina",
          "Osorno",
          "Chile",
          "Chile",
          "Argentina",
          "Peru",
          "Venezuela",
          "Venezuela",
          "Bogot√°",
          "Colombia",
          "Caracas",
          "Chile",
          "Peru",
          "Peru",
          "Granado",
          "Chile",
          "San Carlos de Bariloche",
          "Chile",
          "Argentina",
          "Peru",
          "Chile",
          "Iquique",
          "Arica",
          "Peru",
          "Argentina",
          "Chile",
          "Chile",
          "Argentina",
          "Santiago de Chile",
          "Puno",
          "Lima",
          "Peru",
          "peru",
          "Argentina",
          "Manaos",
          "Peru",
          "Bogot√°",
          "Colombia",
          "Venezuela",
          "Colombia",
          "C√≥rdoba",
          "Baquedano",
          "Argentina",
          "Chile",
          "Valpara√≠so",
          "Chile",
          "Americas",
          "Chile",
          "Chile",
          "Arica",
          "Peru",
          "Chile",
          "Chuquicamata",
          "Chile",
          "Argentina",
          "Andes",
          "Chile",
          "Argentina",
          "Argentina",
          "Argentina",
          "Argentina",
          "Chile",
          "C√≥rdoba",
          "Santa Luc√≠a",
          "Tarat√°",
          "Peru",
          "Pucallpa",
          "Peru",
          "Peru",
          "Peru",
          "Argentina",
          "Lima",
          "Peru",
          "Peru",
          "Lima",
          "Chile",
          "Peru",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile",
          "South America",
          "Bolivia",
          "Suqu√≠a",
          "Peru",
          "Quechua",
          "Colombia",
          "Colombia",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile"
         ],
         "type": "scattergeo"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "geo": {
         "center": {},
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "projection": {
          "type": "natural earth"
         }
        },
        "legend": {
         "title": {
          "text": "Cluster ID"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Clustered Location Mentions from Text"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Block 14: Visualize clustered results using Plotly\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load clustered data\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_scored_clustered.csv\")\n",
    "\n",
    "# Drop NaNs (should already be clean)\n",
    "df = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# Rename cluster column for consistency with Plotly\n",
    "df[\"cluster\"] = df[\"geo_cluster\"]\n",
    "\n",
    "# Create hover label\n",
    "df[\"hover\"] = df[\"entity\"] + \" (cluster \" + df[\"cluster\"].astype(str) + \")\"\n",
    "\n",
    "# Basic scatter geo map\n",
    "fig = px.scatter_geo(\n",
    "    df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    text=\"entity\",\n",
    "    hover_name=\"hover\",\n",
    "    color=\"cluster\",\n",
    "    title=\"Clustered Location Mentions from Text\",\n",
    "    projection=\"natural earth\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.update_layout(legend_title_text='Cluster ID')\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
