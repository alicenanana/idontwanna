{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2367eb",
   "metadata": {},
   "source": [
    "# [Setup]\n",
    "Block 1 (light preprocessing)\n",
    "Block 2 (gazetteer + spacy/stanza)\n",
    "Block 3 (NER functions)\n",
    "Block 7 (WordNet vague terms)\n",
    "Block 8 (motion/transport terms)\n",
    "Block 9 (sentence scoring prep)\n",
    "\n",
    "# [Pipeline]\n",
    "Block 1 (continue with PDF preprocessing)\n",
    "Block 4 (load cleaned sentences)\n",
    "Block 5 (NER + ML filtering)\n",
    "Block 6 (boosting small towns)\n",
    "Block 10 (GeoNames + Wikidata)\n",
    "Block 11 (Wikidata enrichment)\n",
    "Block 12 (clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "092217f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n",
      "2025-07-16 19:02:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77b89a93d2a4fdab379522ecc14f69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 19:02:57 INFO: Downloaded file to /Users/alicja/stanza_resources/resources.json\n",
      "2025-07-16 19:02:57 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-07-16 19:02:58 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-07-16 19:02:58 WARNING: GPU requested, but is not available!\n",
      "2025-07-16 19:02:58 INFO: Using device: cpu\n",
      "2025-07-16 19:02:58 INFO: Loading: tokenize\n",
      "2025-07-16 19:02:58 INFO: Loading: mwt\n",
      "2025-07-16 19:02:58 INFO: Loading: ner\n",
      "2025-07-16 19:03:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Imports and basic NLP setup\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import unicodedata\n",
    "import contractions\n",
    "import nltk\n",
    "import spacy\n",
    "import stanza\n",
    "# Fuzzy matching library\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "# Downloads (run once)\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"framenet_v17\")\n",
    "\n",
    "# Load models\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "# in Block 1 (setup), replace sentencizer init:\n",
    "# Setup for sentence segmentation\n",
    "import spacy\n",
    "sentencizer = spacy.blank(\"en\")\n",
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer.add_pipe(Sentencizer())\n",
    "\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "stanza_pipeline = stanza.Pipeline(lang=\"en\", processors=\"tokenize,ner\", use_gpu=True)\n",
    "\n",
    "# Stopwords\n",
    "nltk_stops = set(stopwords.words(\"english\"))\n",
    "spacy_stops = nlp.Defaults.stop_words\n",
    "all_stops = nltk_stops.union(spacy_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d76d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF text extracted\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extract text from PDF using PyMuPDF\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_path = \"/Users/alicja/Desktop/BA-code/Corpora/MotorcycleDiaries.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# Adjust page range as needed\n",
    "pages_to_read = doc[29:148]\n",
    "raw_text = \"\\n\".join(page.get_text() for page in pages_to_read)\n",
    "\n",
    "print(\"‚úÖ PDF text extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67e33e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Light cleaning (preserve structure)\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    return (\n",
    "        text.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
    "            .replace(\"‚Äô\", \"'\").replace(\"‚Äò\", \"'\")\n",
    "            .replace(\"‚Äî\", \"-\").replace(\"‚Äì\", \"-\")\n",
    "    )\n",
    "\n",
    "def clean_light(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = contractions.fix(text)\n",
    "    text = normalize_punctuation(text)\n",
    "    \n",
    "    # Remove URLs, emails, HTML tags, phone numbers\n",
    "    patterns = [\n",
    "        r'https?://\\S+', r'\\S+@\\S+', r'<.*?>', r'\\+?\\d[\\d\\-\\(\\)\\s]{5,}\\d'\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        text = re.sub(pat, \" \", text)\n",
    "        \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df7a45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Sentence segmentation using spaCy sentencizer\n",
    "def segment_sentences(text: str) -> list:\n",
    "    doc = sentencizer(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e9d3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Heavy NLP cleaning\n",
    "def clean_heavy(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    doc = list(nlp.pipe([text], batch_size=1000, n_process=1))[0]\n",
    "    tokens = [\n",
    "        tok.lemma_ for tok in doc\n",
    "        if tok.lemma_.isalpha()\n",
    "        and tok.lemma_ not in all_stops\n",
    "        and tok.lemma_ != \"-PRON-\"\n",
    "    ]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "396999d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Downloading major cities from GeoNames...\n",
      "‚úÖ Loaded 1000 cities from AR\n",
      "‚úÖ Loaded 1000 cities from CL\n",
      "‚úÖ Loaded 1000 cities from PE\n",
      "‚úÖ Loaded 1000 cities from BO\n",
      "‚úÖ Loaded 1000 cities from CO\n",
      "‚úÖ Loaded 1000 cities from VE\n",
      "üìå Total unique cities in gazetteer: 5311\n"
     ]
    }
   ],
   "source": [
    "# Block 6: Build gazetteer using GeoNames API\n",
    "geonames_username = \"alicjab\"  # <-- Replace if needed\n",
    "countries = [\"AR\", \"CL\", \"PE\", \"BO\", \"CO\", \"VE\"]  # South American countries\n",
    "gazetteer = set()\n",
    "\n",
    "print(\"üåç Downloading major cities from GeoNames...\")\n",
    "for country_code in countries:\n",
    "    try:\n",
    "        url = \"http://api.geonames.org/searchJSON\"\n",
    "        params = {\n",
    "            \"featureClass\": \"P\",  # populated places\n",
    "            \"country\": country_code,\n",
    "            \"maxRows\": 1000,\n",
    "            \"username\": geonames_username\n",
    "        }\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        city_names = [\n",
    "            entry[\"name\"].lower() for entry in data.get(\"geonames\", [])\n",
    "            if \"name\" in entry\n",
    "        ]\n",
    "        gazetteer.update(city_names)\n",
    "        print(f\"‚úÖ Loaded {len(city_names)} cities from {country_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error downloading cities for {country_code}: {e}\")\n",
    "\n",
    "print(f\"üìå Total unique cities in gazetteer: {len(gazetteer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c0aabe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned versions saved\n"
     ]
    }
   ],
   "source": [
    "# Block 7: Apply cleaning and segmentation to raw text\n",
    "light_cleaned = clean_light(raw_text)\n",
    "sentences = segment_sentences(light_cleaned)\n",
    "\n",
    "# Optional tagging for geoparsing\n",
    "sentences_with_tags = [f\"[SENT {i+1}] {s}\" for i, s in enumerate(sentences)]\n",
    "\n",
    "# Heavy-cleaned version for downstream NLP\n",
    "heavy_cleaned = clean_heavy(light_cleaned)\n",
    "\n",
    "# Save outputs\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sentences_with_tags))\n",
    "\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_nlp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(heavy_cleaned)\n",
    "\n",
    "print(\"‚úÖ Cleaned versions saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41623c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 2476 tagged sentences\n"
     ]
    }
   ],
   "source": [
    "# Block 8: Load sentence list with IDs from saved file\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sentence_data = []\n",
    "for line in lines:\n",
    "    match = re.match(r\"\\[SENT (\\d+)\\] (.+)\", line.strip())\n",
    "    if match:\n",
    "        sent_id = int(match.group(1))\n",
    "        sent_text = match.group(2)\n",
    "        sentence_data.append((sent_id, sent_text))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(sentence_data)} tagged sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d70be0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Running ensemble NER + gazetteer matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER + Gazetteer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2476/2476 [00:17<00:00, 140.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: outputs/geoparsing_ner_ensemble.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Block 9: NER + Gazetteer with optional Stanza and progress bar\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Toggle Stanza use (set to False to avoid slowdowns)\n",
    "USE_STANZA = False\n",
    "\n",
    "def extract_entities_spacy(text):\n",
    "    doc = nlp_spacy(text)\n",
    "    return [(ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "            for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "\n",
    "def extract_entities_stanza(text):\n",
    "    doc = stanza_pipeline(text)\n",
    "    results = []\n",
    "    for sent in doc.sentences:\n",
    "        for ent in sent.ents:\n",
    "            if ent.type in {\"GPE\", \"LOC\"}:\n",
    "                results.append((ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "    return results\n",
    "\n",
    "def match_gazetteer(text, known_places):\n",
    "    matches = []\n",
    "    lowered = text.lower()\n",
    "    for place in known_places:\n",
    "        idx = lowered.find(place)\n",
    "        if idx != -1:\n",
    "            matches.append((place, \"GAZETTEER\", idx, idx + len(place)))\n",
    "    return matches\n",
    "\n",
    "def combine_ner_gazetteer(sentences, gazetteer):\n",
    "    results = []\n",
    "    for sent_id, text in tqdm(sentences, desc=\"NER + Gazetteer\"):\n",
    "        try:\n",
    "            ents_spacy = extract_entities_spacy(text)\n",
    "            ents_stanza = extract_entities_stanza(text) if USE_STANZA else []\n",
    "            ents_gazetteer = match_gazetteer(text, gazetteer)\n",
    "\n",
    "            all_ents = ents_spacy + ents_stanza + ents_gazetteer\n",
    "            seen = set()\n",
    "            for ent_text, label, start, end in all_ents:\n",
    "                norm = ent_text.lower()\n",
    "                if not any(fuzz.ratio(norm, s) > 90 for s in seen):\n",
    "                    seen.add(norm)\n",
    "                    results.append({\n",
    "                        \"sentence_id\": sent_id,\n",
    "                        \"entity\": ent_text,\n",
    "                        \"entity_norm\": norm,\n",
    "                        \"label\": label,\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end,\n",
    "                        \"sentence\": text\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error in sentence {sent_id}: {e}\")\n",
    "    return results\n",
    "\n",
    "# üöÄ Run it\n",
    "print(\"üîé Running ensemble NER + gazetteer matching...\")\n",
    "entities_combined = combine_ner_gazetteer(sentence_data, gazetteer)\n",
    "\n",
    "df_combined = pd.DataFrame(entities_combined)\n",
    "df_combined.to_csv(\"outputs/geoparsing_ner_ensemble.csv\", index=False)\n",
    "print(\"‚úÖ Saved: outputs/geoparsing_ner_ensemble.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa8a7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       230\n",
      "           1       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           1.00       267\n",
      "   macro avg       1.00      1.00      1.00       267\n",
      "weighted avg       1.00      1.00      1.00       267\n",
      "\n",
      "‚úÖ ML filtering complete\n"
     ]
    }
   ],
   "source": [
    "# Block 10: Train ML model to filter real geographic entities\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load ensemble NER output\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble.csv\")\n",
    "\n",
    "# Create features\n",
    "df[\"label_GPE\"] = (df[\"label\"] == \"GPE\").astype(int)\n",
    "df[\"label_LOC\"] = (df[\"label\"] == \"LOC\").astype(int)\n",
    "\n",
    "symbolic_keywords = [\"freedom\", \"struggle\", \"liberation\", \"future\", \"dream\", \"cause\", \"revolution\", \"hope\", \"people\"]\n",
    "df[\"symbolic_flagged\"] = df[\"sentence\"].str.contains(\"|\".join(symbolic_keywords), flags=re.IGNORECASE, na=False)\n",
    "\n",
    "expected_countries = [\"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\", \"venezuela\"]\n",
    "df[\"country_valid\"] = df[\"sentence\"].str.lower().apply(\n",
    "    lambda x: int(any(country in x for country in expected_countries))\n",
    ")\n",
    "\n",
    "df[\"fuzzy_score\"] = df.apply(\n",
    "    lambda row: fuzz.ratio(str(row[\"entity\"]).lower(), str(row[\"entity_norm\"]).lower()), axis=1\n",
    ")\n",
    "df[\"fuzzy_score_scaled\"] = df[\"fuzzy_score\"] / 100.0\n",
    "\n",
    "df[\"auto_label\"] = ((df[\"country_valid\"] == 1) & (~df[\"symbolic_flagged\"])).astype(int)\n",
    "\n",
    "# Train/test split\n",
    "features = df[[\"label_GPE\", \"label_LOC\", \"symbolic_flagged\", \"country_valid\", \"fuzzy_score_scaled\"]]\n",
    "target = df[\"auto_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, stratify=target, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict on all\n",
    "X_all_scaled = scaler.transform(features)\n",
    "df[\"geo_confidence\"] = clf.predict_proba(X_all_scaled)[:, 1]\n",
    "df[\"filtered_out_ml\"] = df[\"geo_confidence\"] < 0.5\n",
    "\n",
    "# Save outputs\n",
    "df_filtered = df[~df[\"filtered_out_ml\"]].copy()\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_flagged_with_ml.csv\", index=False)\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ ML filtering complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f1288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not load enriched data: [Errno 2] No such file or directory: 'outputs/geoparsing_final_enriched.csv'\n",
      "‚úÖ Saved: geoparsing_ensemble_final_ml_boosted.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 11: Boost confidence for small towns in South America\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\")\n",
    "\n",
    "# Define target countries\n",
    "target_countries = [\n",
    "    \"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\",\n",
    "    \"venezuela\", \"ecuador\", \"brazil\", \"uruguay\", \"paraguay\"\n",
    "]\n",
    "\n",
    "# Prepare for matching\n",
    "df[\"entity_norm_lower\"] = df[\"entity_norm\"].str.lower()\n",
    "\n",
    "# Try to load population-enriched data\n",
    "try:\n",
    "    enriched = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "    enriched[\"entity_norm_lower\"] = enriched[\"entity_norm\"].str.lower()\n",
    "\n",
    "    df = df.merge(\n",
    "        enriched[[\"entity_norm_lower\", \"country\", \"population\"]],\n",
    "        on=\"entity_norm_lower\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Mark small towns\n",
    "    df[\"boost_small_town\"] = (\n",
    "        df[\"population\"].fillna(0).lt(50000) &\n",
    "        df[\"country\"].str.lower().isin(target_countries)\n",
    "    )\n",
    "    print(f\"‚úÖ Boosted {df['boost_small_town'].sum()} small towns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load enriched data: {e}\")\n",
    "    df[\"boost_small_town\"] = False\n",
    "\n",
    "# Apply confidence boost\n",
    "df[\"geo_confidence_boosted\"] = df[\"geo_confidence\"]\n",
    "df.loc[df[\"boost_small_town\"], \"geo_confidence_boosted\"] = 0.9\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\", index=False)\n",
    "print(\"‚úÖ Saved: geoparsing_ensemble_final_ml_boosted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47b36f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved filtered: geoparsing_ner_ensemble_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Block 12: Filter vague location terms (e.g. 'area', 'region') using WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "location_synsets = [\n",
    "    wn.synset(\"location.n.01\"),\n",
    "    wn.synset(\"region.n.01\"),\n",
    "    wn.synset(\"area.n.01\"),\n",
    "    wn.synset(\"place.n.01\"),\n",
    "    wn.synset(\"territory.n.01\")\n",
    "]\n",
    "\n",
    "vague_terms = set()\n",
    "for syn in location_synsets:\n",
    "    for hypo in syn.closure(lambda s: s.hyponyms()):\n",
    "        for lemma in hypo.lemmas():\n",
    "            vague_terms.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\")\n",
    "\n",
    "# Check against pre-generated vague_terms set (from setup)\n",
    "df[\"is_vague\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"label\"] == \"LOC\"\n",
    "        and isinstance(row[\"entity_norm\"], str)\n",
    "        and row[\"entity_norm\"].lower().strip() in vague_terms\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filter and save\n",
    "df_filtered = df[~df[\"is_vague\"]].copy().drop(columns=[\"is_vague\"])\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ner_ensemble_filtered.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved filtered: geoparsing_ner_ensemble_filtered.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6b3c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('personnel_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_vehicle.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('weapons_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warplane.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warship.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('tank.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('half_track.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('bomber.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('fighter.n.02') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_plane.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('technical.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('minivan.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded spaCy model for scoring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 183/183 [00:16<00:00, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Saved: outputs/geoparsing_scored_candidates_improved.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 13: Score location entities using contextual heuristics\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import dateparser.search\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn, framenet as fn\n",
    "\n",
    "# === REBUILD motion_verbs and transport_terms ===\n",
    "\n",
    "# Load motion verbs from FrameNet\n",
    "motion_frames = ['Motion', 'Travel', 'Self_motion', 'Arriving', 'Departing']\n",
    "motion_verbs = set()\n",
    "for frame in motion_frames:\n",
    "    try:\n",
    "        for lu in fn.frame_by_name(frame).lexUnit.values():\n",
    "            if lu['name'].endswith('.v'):\n",
    "                motion_verbs.add(lu['name'].split('.')[0].lower())\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Load transport nouns from WordNet\n",
    "vehicle_syn = wn.synset('vehicle.n.01')\n",
    "transport_terms = set()\n",
    "for syn in vehicle_syn.closure(lambda s: s.hyponyms()):\n",
    "    for lemma in syn.lemmas():\n",
    "        transport_terms.add(lemma.name().lower().replace('_', ' '))\n",
    "\n",
    "# === Load filtered NER results ===\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble_filtered.csv\")\n",
    "\n",
    "# === Load sentence map ===\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sentence_map = {}\n",
    "for line in lines:\n",
    "    if line.strip().startswith(\"[SENT\"):\n",
    "        sid = int(line.split(\"]\")[0].split()[1])\n",
    "        sentence_map[sid] = line.split(\"]\")[1].strip()\n",
    "\n",
    "# === Load spaCy model ===\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"‚úÖ Loaded spaCy model for scoring\")\n",
    "\n",
    "# === Score logic ===\n",
    "scored_rows = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring entities\"):\n",
    "    sid = row[\"sentence_id\"]\n",
    "    entity = row[\"entity\"]\n",
    "    norm = row[\"entity_norm\"]\n",
    "    label = row[\"label\"]\n",
    "    sentence = sentence_map.get(sid, \"\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    score = 0\n",
    "    entity_token = None\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"} and ent.text.lower().strip() == entity.lower().strip():\n",
    "            entity_token = ent.root\n",
    "            break\n",
    "\n",
    "    if not entity_token:\n",
    "        continue\n",
    "\n",
    "    sentence_lower = sentence.lower()\n",
    "    entity_lower = entity.lower()\n",
    "\n",
    "    if any(verb in sentence_lower for verb in motion_verbs):\n",
    "        score += 1\n",
    "    if any(term in sentence_lower for term in transport_terms):\n",
    "        score += 1\n",
    "    if re.search(r'\\b(in|to|at)\\s+' + re.escape(entity_lower) + r'\\b', sentence_lower):\n",
    "        score += 1\n",
    "    if re.search(r\"\\b\" + re.escape(entity_lower) + r\"['‚Äô]s\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bof\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bfor\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if dateparser.search.search_dates(sentence):\n",
    "        score += 1\n",
    "\n",
    "    scored_rows.append({\n",
    "        \"sentence_id\": sid,\n",
    "        \"entity\": entity,\n",
    "        \"entity_norm\": norm,\n",
    "        \"label\": label,\n",
    "        \"sentence\": sentence,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "# === Save results ===\n",
    "scored_df = pd.DataFrame(scored_rows)\n",
    "scored_df = scored_df.sort_values(by=[\"score\", \"sentence_id\"], ascending=[False, True])\n",
    "scored_df.to_csv(\"outputs/geoparsing_scored_candidates_improved.csv\", index=False)\n",
    "\n",
    "print(\"üìÑ Saved: outputs/geoparsing_scored_candidates_improved.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "093ea256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Querying GeoNames...\n",
      "üîÅ Querying Wikidata for missing coordinates...\n",
      "üß≠ Querying OpenStreetMap for unresolved locations...\n",
      "‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 14: Enrich entities with coordinates via GeoNames ‚Üí Wikidata ‚Üí OSM\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_scored_candidates_improved.csv\")\n",
    "places = df[\"entity_norm\"].dropna().unique()\n",
    "geonames_username = \"alicjab\"\n",
    "\n",
    "geo_data = {}\n",
    "\n",
    "# === Step 1: GeoNames ===\n",
    "print(\"üåç Querying GeoNames...\")\n",
    "for place in places:\n",
    "    try:\n",
    "        params = {\"q\": place, \"maxRows\": 1, \"username\": geonames_username}\n",
    "        r = requests.get(\"http://api.geonames.org/searchJSON\", params=params, timeout=10).json()\n",
    "        if not r.get(\"geonames\"):\n",
    "            raise ValueError(\"No result\")\n",
    "        g = r[\"geonames\"][0]\n",
    "        geo_data[place] = {\n",
    "            \"latitude\": float(g[\"lat\"]),\n",
    "            \"longitude\": float(g[\"lng\"]),\n",
    "            \"country\": g.get(\"countryName\"),\n",
    "            \"population\": int(g.get(\"population\", 0))\n",
    "        }\n",
    "    except Exception:\n",
    "        geo_data[place] = {\"latitude\": None, \"longitude\": None, \"country\": None, \"population\": None}\n",
    "    time.sleep(1)  # API rate limit\n",
    "\n",
    "geo_df = pd.DataFrame.from_dict(geo_data, orient=\"index\")\n",
    "geo_df.index.name = \"entity_norm\"\n",
    "\n",
    "df_enriched = df.merge(geo_df, on=\"entity_norm\", how=\"left\")\n",
    "\n",
    "# === Step 2: Wikidata Fallback ===\n",
    "print(\"üîÅ Querying Wikidata for missing coordinates...\")\n",
    "missing_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_wikidata_coords(place):\n",
    "    try:\n",
    "        search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"search\": place,\n",
    "            \"language\": \"en\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        r = requests.get(search_url, params=search_params, timeout=10).json()\n",
    "        if not r[\"search\"]:\n",
    "            return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "        qid = r[\"search\"][0][\"id\"]\n",
    "        entity = requests.get(f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\").json()\n",
    "        coords = entity[\"entities\"][qid][\"claims\"].get(\"P625\", [{}])[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {})\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": coords.get(\"latitude\"), \"wikidata_lon\": coords.get(\"longitude\")}\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "\n",
    "wikidata_results = pd.DataFrame([query_wikidata_coords(p) for p in missing_places])\n",
    "df_enriched = df_enriched.merge(wikidata_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"wikidata_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"wikidata_lon\"])\n",
    "\n",
    "# === Step 3: OSM (OpenStreetMap) Fallback ===\n",
    "print(\"üß≠ Querying OpenStreetMap for unresolved locations...\")\n",
    "nominatim_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_osm(place):\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://nominatim.openstreetmap.org/search\",\n",
    "            params={\"q\": place, \"format\": \"json\", \"limit\": 1},\n",
    "            headers={\"User-Agent\": \"Geoparser/1.0\"},\n",
    "            timeout=10\n",
    "        ).json()\n",
    "        if not r:\n",
    "            return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "        return {\n",
    "            \"entity_norm\": place,\n",
    "            \"osm_lat\": float(r[0][\"lat\"]),\n",
    "            \"osm_lon\": float(r[0][\"lon\"])\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "\n",
    "osm_results = pd.DataFrame([query_osm(p) for p in nominatim_places])\n",
    "df_enriched = df_enriched.merge(osm_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"osm_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"osm_lon\"])\n",
    "\n",
    "# === Save Final Enriched File ===\n",
    "df_enriched.to_csv(\"outputs/geoparsing_final_enriched.csv\", index=False)\n",
    "print(\"‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9bd58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clustered results saved to: outputs/geoparsing_clustered.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 15: Cluster enriched locations using DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Load geocoded data\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "\n",
    "# Drop entries with missing coordinates\n",
    "df = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# Prepare coordinate matrix\n",
    "coords = df[[\"latitude\", \"longitude\"]].to_numpy()\n",
    "\n",
    "# DBSCAN params: ~100km radius\n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 100 / kms_per_radian  # 100km in radians\n",
    "\n",
    "# Run clustering\n",
    "db = DBSCAN(eps=epsilon, min_samples=2, algorithm='ball_tree', metric='haversine')\n",
    "df[\"cluster\"] = db.fit_predict(np.radians(coords))\n",
    "\n",
    "# Save clustered output\n",
    "df.to_csv(\"outputs/geoparsing_clustered.csv\", index=False)\n",
    "print(\"‚úÖ Clustered results saved to: outputs/geoparsing_clustered.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45e8424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "geo": "geo",
         "hovertemplate": "<b>%{hovertext}</b><br><br>entity=%{text}<br>latitude=%{lat}<br>longitude=%{lon}<br>cluster=%{marker.color}<extra></extra>",
         "hovertext": [
          "Antofagasta (cluster -1)",
          "Chile (cluster 0)",
          "Bogot√° (cluster 1)",
          "Argentina (cluster 2)",
          "Argentina (cluster 2)",
          "Osorno (cluster -1)",
          "Los √Ångeles (cluster -1)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Valley (cluster -1)",
          "Argentina (cluster 2)",
          "Peru (cluster 3)",
          "Venezuela (cluster 4)",
          "Venezuela (cluster 4)",
          "Bogot√° (cluster 1)",
          "Colombia (cluster 5)",
          "Caracas (cluster -1)",
          "Chile (cluster 0)",
          "Peru (cluster 3)",
          "Peru (cluster 3)",
          "Granado (cluster -1)",
          "Chile (cluster 0)",
          "Andean (cluster -1)",
          "San Carlos de Bariloche (cluster -1)",
          "Chile (cluster 0)",
          "Argentina (cluster 2)",
          "Peru (cluster 3)",
          "Chile (cluster 0)",
          "Iquique (cluster -1)",
          "Arica (cluster 6)",
          "Peru (cluster 3)",
          "Argentina (cluster 2)",
          "Chile (cluster 0)",
          "the United States (cluster 7)",
          "Chile (cluster 0)",
          "Argentina (cluster 2)",
          "Santiago de Chile (cluster -1)",
          "Puno (cluster -1)",
          "Pacific (cluster -1)",
          "Lima (cluster 8)",
          "Peru (cluster 3)",
          "peru (cluster 3)",
          "Argentina (cluster 2)",
          "Manaos (cluster -1)",
          "Peru (cluster 3)",
          "Bogot√° (cluster 1)",
          "Colombia (cluster 5)",
          "Venezuela (cluster 4)",
          "Colombia (cluster 5)",
          "C√≥rdoba (cluster 9)",
          "Punta (cluster 10)",
          "√Åguila (cluster -1)",
          "Baquedano (cluster 10)",
          "Argentina (cluster 2)",
          "Chile (cluster 0)",
          "United Latin America (cluster 11)",
          "Valpara√≠so (cluster -1)",
          "Chile (cluster 0)",
          "Americas (cluster -1)",
          "Spain (cluster 12)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Rome (cluster -1)",
          "Arica (cluster 6)",
          "Peru (cluster 3)",
          "Chile (cluster 0)",
          "Chuquicamata (cluster -1)",
          "Chile (cluster 0)",
          "Argentina (cluster 2)",
          "Andes (cluster -1)",
          "Chile (cluster 0)",
          "Argentina (cluster 2)",
          "Argentina (cluster 2)",
          "Argentina (cluster 2)",
          "Argentina (cluster 2)",
          "Chile (cluster 0)",
          "C√≥rdoba (cluster 9)",
          "Santa Luc√≠a (cluster -1)",
          "Tarat√° (cluster -1)",
          "Peru (cluster 3)",
          "Pucallpa (cluster -1)",
          "Peru (cluster 3)",
          "Peru (cluster 3)",
          "Peru (cluster 3)",
          "Argentina (cluster 2)",
          "Lima (cluster 8)",
          "Peru (cluster 3)",
          "United Latin America (cluster 11)",
          "Peru (cluster 3)",
          "Lima (cluster 8)",
          "Spain (cluster 12)",
          "Chile (cluster 0)",
          "Peru (cluster 3)",
          "Argentina (cluster 2)",
          "Argentina (cluster 2)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Easter Island (cluster -1)",
          "South America (cluster 7)",
          "the United States (cluster 7)",
          "Bolivia (cluster -1)",
          "Suqu√≠a (cluster -1)",
          "Peru (cluster 3)",
          "Quechua (cluster -1)",
          "Colombia (cluster 5)",
          "Colombia (cluster 5)",
          "Argentina (cluster 2)",
          "Argentina (cluster 2)",
          "Chile (cluster 0)",
          "Chile (cluster 0)"
         ],
         "lat": {
          "bdata": "gqj7AKSmN8AAAAAAAAA+wDgteNFXcBJAAAAAAAAAQcAAAAAAAABBwAXFjzF3SUTAX+/+eK8GQUAAAAAAAAA+wAAAAAAAAD7Agez17o83MkAAAAAAAABBwAAAAAAAACTAAAAAAAAAIEAAAAAAAAAgQDgteNFXcBJAAAAAAAAAEEDuPVxy3PkkQAAAAAAAAD7AAAAAAAAAJMAAAAAAAAAkwM0Bgjl63CdAAAAAAAAAPsBNvtnmxpQgQK2jqgmikkTAAAAAAAAAPsAAAAAAAABBwAAAAAAAACTAAAAAAAAAPsCCixU1mDY0wAuYwK27eTLAAAAAAAAAJMAAAAAAAABBwAAAAAAAAD7Aofgx5q41LcAAAAAAAAA+wAAAAAAAAEHA/B2KAn26QMBZUYNpGK4vwD0K16NwPSFAvK5fsBsWKMAAAAAAAAAkwAAAAAAAACTAAAAAAAAAQcDiOzHrxdAIwAAAAAAAACTAOC140VdwEkAAAAAAAAAQQAAAAAAAACBAAAAAAAAAEEBjesISD2g/wD+RJ0nXlErAIbByaJGdNUBJnYAmwqJKwAAAAAAAAEHAAAAAAAAAPsAai6azk3FEQPhT46WbhEDAAAAAAAAAPsAAAAAAAAA1QAAAAAAAAERAAAAAAAAAPsAAAAAAAAA+wCApIsMq8kRAC5jArbt5MsAAAAAAAAAkwAAAAAAAAD7A8L+V7NhQNsAAAAAAAAA+wAAAAAAAAEHASbpm8s3WO8AAAAAAAAA+wAAAAAAAAEHAAAAAAAAAQcAAAAAAAABBwAAAAAAAAEHAAAAAAAAAPsBjesISD2g/wMAma9RDxCtAB+v/HOabMcAAAAAAAAAkwIqO5PIfwiDAAAAAAAAAJMAAAAAAAAAkwAAAAAAAACTAAAAAAAAAQcC8rl+wGxYowAAAAAAAACTAGoums5NxREAAAAAAAAAkwLyuX7AbFijAAAAAAAAAREAAAAAAAAA+wAAAAAAAACTAAAAAAAAAQcAAAAAAAABBwAAAAAAAAD7AAAAAAAAAPsAAAAAAAIBGwKH4MeauNS3Aofgx5q41LcAAAAAAAAAxwKEQAYdQRS3AAAAAAAAAJMDp1JXP8lwiwAAAAAAAABBAAAAAAAAAEEAAAAAAAABBwAAAAAAAAEHAAAAAAAAAPsAAAAAAAAA+wA==",
          "dtype": "f8"
         },
         "legendgroup": "",
         "lon": {
          "bdata": "YeC593CZUcAAAAAAAMBRwKwcWmQ7hVLAAAAAAAAAUMAAAAAAAABQwOOqsu+KSFLAHaz/c5iPXcAAAAAAAMBRwAAAAAAAwFHAJV0z+WaHT8AAAAAAAABQwAAAAAAA0FLAAAAAAACAUMAAAAAAAIBQwKwcWmQ7hVLAAAAAAABQUsAYPiKmRLhQwAAAAAAAwFHAAAAAAADQUsAAAAAAANBSwFu21hcJfVXAAAAAAADAUcCgNxWpMIBRwEj+YOC501HAAAAAAADAUcAAAAAAAABQwAAAAAAA0FLAAAAAAADAUcB65A8GnolRwDp15bM8k1HAAAAAAADQUsAAAAAAAABQwAAAAAAAwFHAAAAAAADUTMAAAAAAAMBRwAAAAAAAAFDAlj50QX2pUcChSs0eaIFRwOxRuB6FQ2XAGRwlr85BU8AAAAAAANBSwAAAAAAA0FLAAAAAAAAAUMAzMzMzMwNOwAAAAAAA0FLArBxaZDuFUsAAAAAAAFBSwAAAAAAAgFDAAAAAAABQUsAnFCLgEAxQwKA3FakwulHACW05l+JiU8ASFD/G3IVRwAAAAAAAAFDAAAAAAADAUcAz4Zf6eaNSwCeloNtL6FHAAAAAAADAUcAAAAAAAIBVwAAAAAAAABDAAAAAAADAUcAAAAAAAMBRwGfttgvNBSlAOnXlszyTUcAAAAAAANBSwAAAAAAAwFHA1qiHaHQ7UcAAAAAAAMBRwAAAAAAAAFDAsRafAmDoYUAAAAAAAMBRwAAAAAAAAFDAAAAAAAAAUMAAAAAAAABQwAAAAAAAAFDAAAAAAADAUcAnFCLgEAxQwB0Dste7e07AQfFjzF2BUMAAAAAAANBSwBMPKJtyo1LAAAAAAADQUsAAAAAAANBSwAAAAAAA0FLAAAAAAAAAUMAZHCWvzkFTwAAAAAAA0FLAM+GX+nmjUsAAAAAAANBSwBkcJa/OQVPAAAAAAAAAEMAAAAAAAMBRwAAAAAAA0FLAAAAAAAAAUMAAAAAAAABQwAAAAAAAwFHAAAAAAADAUcAAAAAAAMBcwAAAAAAA1EzAAAAAAADUTMAAAAAAAEBQwLSTwVHyyjVAAAAAAADQUsDqymd5HjJTwAAAAAAAUFLAAAAAAABQUsAAAAAAAABQwAAAAAAAAFDAAAAAAADAUcAAAAAAAMBRwA==",
          "dtype": "f8"
         },
         "marker": {
          "color": {
           "bdata": "/wABAgL//wAA/wIDBAQBBf8AAwP/AP//AAIDAP8GAwIABwAC////CAMDAv8DAQUEBQkK/woCAAv/AP8MAAD/BgMA/wAC/wACAgICAAn//wP/AwMDAggDCwMIDAADAgIAAP8HB///A/8FBQICAAA=",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "",
         "showlegend": false,
         "text": [
          "Antofagasta",
          "Chile",
          "Bogot√°",
          "Argentina",
          "Argentina",
          "Osorno",
          "Los √Ångeles",
          "Chile",
          "Chile",
          "Valley",
          "Argentina",
          "Peru",
          "Venezuela",
          "Venezuela",
          "Bogot√°",
          "Colombia",
          "Caracas",
          "Chile",
          "Peru",
          "Peru",
          "Granado",
          "Chile",
          "Andean",
          "San Carlos de Bariloche",
          "Chile",
          "Argentina",
          "Peru",
          "Chile",
          "Iquique",
          "Arica",
          "Peru",
          "Argentina",
          "Chile",
          "the United States",
          "Chile",
          "Argentina",
          "Santiago de Chile",
          "Puno",
          "Pacific",
          "Lima",
          "Peru",
          "peru",
          "Argentina",
          "Manaos",
          "Peru",
          "Bogot√°",
          "Colombia",
          "Venezuela",
          "Colombia",
          "C√≥rdoba",
          "Punta",
          "√Åguila",
          "Baquedano",
          "Argentina",
          "Chile",
          "United Latin America",
          "Valpara√≠so",
          "Chile",
          "Americas",
          "Spain",
          "Chile",
          "Chile",
          "Rome",
          "Arica",
          "Peru",
          "Chile",
          "Chuquicamata",
          "Chile",
          "Argentina",
          "Andes",
          "Chile",
          "Argentina",
          "Argentina",
          "Argentina",
          "Argentina",
          "Chile",
          "C√≥rdoba",
          "Santa Luc√≠a",
          "Tarat√°",
          "Peru",
          "Pucallpa",
          "Peru",
          "Peru",
          "Peru",
          "Argentina",
          "Lima",
          "Peru",
          "United Latin America",
          "Peru",
          "Lima",
          "Spain",
          "Chile",
          "Peru",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile",
          "Easter Island",
          "South America",
          "the United States",
          "Bolivia",
          "Suqu√≠a",
          "Peru",
          "Quechua",
          "Colombia",
          "Colombia",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile"
         ],
         "type": "scattergeo"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "geo": {
         "center": {},
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "projection": {
          "type": "natural earth"
         }
        },
        "legend": {
         "title": {
          "text": "Cluster ID"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "üó∫Ô∏è Clustered Location Mentions from Text"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìç Visualize clustered results using Plotly\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load clustered data\n",
    "df = pd.read_csv(\"outputs/geoparsing_clustered.csv\")\n",
    "\n",
    "# Drop NaNs (should already be clean)\n",
    "df = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# Create hover label\n",
    "df[\"hover\"] = df[\"entity\"] + \" (cluster \" + df[\"cluster\"].astype(str) + \")\"\n",
    "\n",
    "# Basic scatter geo map\n",
    "fig = px.scatter_geo(\n",
    "    df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    text=\"entity\",\n",
    "    hover_name=\"hover\",\n",
    "    color=\"cluster\",\n",
    "    title=\"üó∫Ô∏è Clustered Location Mentions from Text\",\n",
    "    projection=\"natural earth\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.update_layout(legend_title_text='Cluster ID')\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
