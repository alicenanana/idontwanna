{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2367eb",
   "metadata": {},
   "source": [
    "# [Setup]\n",
    "Block 1 (light preprocessing)\n",
    "Block 2 (gazetteer + spacy/stanza)\n",
    "Block 3 (NER functions)\n",
    "Block 7 (WordNet vague terms)\n",
    "Block 8 (motion/transport terms)\n",
    "Block 9 (sentence scoring prep)\n",
    "\n",
    "# [Pipeline]\n",
    "Block 1 (continue with PDF preprocessing)\n",
    "Block 4 (load cleaned sentences)\n",
    "Block 5 (NER + ML filtering)\n",
    "Block 6 (boosting small towns)\n",
    "Block 10 (GeoNames + Wikidata)\n",
    "Block 11 (Wikidata enrichment)\n",
    "Block 12 (clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092217f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/alicja/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n",
      "2025-07-17 20:25:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcacfd4f1cc4ff09e4c9ed632c246a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 20:25:05 INFO: Downloaded file to /Users/alicja/stanza_resources/resources.json\n",
      "2025-07-17 20:25:05 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-07-17 20:25:05 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-07-17 20:25:05 WARNING: GPU requested, but is not available!\n",
      "2025-07-17 20:25:05 INFO: Using device: cpu\n",
      "2025-07-17 20:25:05 INFO: Loading: tokenize\n",
      "2025-07-17 20:25:06 INFO: Loading: mwt\n",
      "2025-07-17 20:25:06 INFO: Loading: ner\n",
      "2025-07-17 20:25:10 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Imports and basic NLP setup\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import unicodedata\n",
    "import contractions\n",
    "import nltk\n",
    "import spacy\n",
    "import stanza\n",
    "# Fuzzy matching library\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "# Downloads \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"framenet_v17\")\n",
    "\n",
    "# Load models\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "# in Block 1 (setup), replace sentencizer init:\n",
    "# Setup for sentence segmentation\n",
    "import spacy\n",
    "sentencizer = spacy.blank(\"en\")\n",
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer.add_pipe(Sentencizer())\n",
    "\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "stanza_pipeline = stanza.Pipeline(lang=\"en\", processors=\"tokenize,ner\", use_gpu=True)\n",
    "\n",
    "# Stopwords\n",
    "nltk_stops = set(stopwords.words(\"english\"))\n",
    "spacy_stops = nlp.Defaults.stop_words\n",
    "all_stops = nltk_stops.union(spacy_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF text extracted\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extract text from PDF using PyMuPDF\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_path = \"/Users/alicja/Desktop/BA-code/Corpora/MotorcycleDiaries.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# Adjust page range as needed\n",
    "pages_to_read = doc[29:148]\n",
    "raw_text = \"\\n\".join(page.get_text() for page in pages_to_read)\n",
    "\n",
    "print(\" PDF text extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e33e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Light cleaning (preserve structure)\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    return (\n",
    "        text.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
    "            .replace(\"‚Äô\", \"'\").replace(\"‚Äò\", \"'\")\n",
    "            .replace(\"‚Äî\", \"-\").replace(\"‚Äì\", \"-\")\n",
    "    )\n",
    "\n",
    "def clean_light(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = contractions.fix(text)\n",
    "    text = normalize_punctuation(text)\n",
    "    \n",
    "    # Remove URLs, emails, HTML tags, phone numbers\n",
    "    patterns = [\n",
    "        r'https?://\\S+', r'\\S+@\\S+', r'<.*?>', r'\\+?\\d[\\d\\-\\(\\)\\s]{5,}\\d'\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        text = re.sub(pat, \" \", text)\n",
    "        \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7a45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Sentence segmentation using spaCy sentencizer\n",
    "def segment_sentences(text: str) -> list:\n",
    "    doc = sentencizer(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5:  NLP cleaning\n",
    "def clean_heavy(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    doc = list(nlp.pipe([text], batch_size=1000, n_process=1))[0]\n",
    "    tokens = [\n",
    "        tok.lemma_ for tok in doc\n",
    "        if tok.lemma_.isalpha()\n",
    "        and tok.lemma_ not in all_stops\n",
    "        and tok.lemma_ != \"-PRON-\"\n",
    "    ]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396999d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Downloading cities from GeoNames for all of South America...\n",
      "‚úÖ Loaded 5000 cities from AR\n",
      "‚úÖ Loaded 5000 cities from BO\n",
      "‚úÖ Loaded 5000 cities from BR\n",
      "‚úÖ Loaded 5000 cities from CL\n",
      "‚úÖ Loaded 5000 cities from CO\n",
      "‚úÖ Loaded 5000 cities from EC\n",
      "‚úÖ Loaded 906 cities from GY\n",
      "‚úÖ Loaded 5000 cities from PY\n",
      "‚úÖ Loaded 5000 cities from PE\n",
      "‚úÖ Loaded 548 cities from SR\n",
      "‚úÖ Loaded 1077 cities from UY\n",
      "‚úÖ Loaded 5000 cities from VE\n",
      "‚úÖ Loaded 5000 cities from MX\n",
      "‚úÖ Loaded 5000 cities from GT\n",
      "‚úÖ Loaded 5000 cities from HN\n",
      "‚úÖ Loaded 4824 cities from SV\n",
      "‚úÖ Loaded 3039 cities from NI\n",
      "‚úÖ Loaded 5000 cities from CR\n",
      "‚úÖ Loaded 5000 cities from PA\n",
      "üìå Total unique cities in gazetteer: 48784\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Block 6: Build gazetteer using GeoNames API \n",
    "import requests\n",
    "import time\n",
    "\n",
    "geonames_username = \"alicjab\"  \n",
    "\n",
    "# ISO codes for all South American countries\n",
    "countries = [\n",
    "    \"AR\", \"BO\", \"BR\", \"CL\", \"CO\", \"EC\", \"GY\", \"PY\", \"PE\", \"SR\", \"UY\", \"VE\",  # South America\n",
    "    \"MX\", \"GT\", \"HN\", \"SV\", \"NI\", \"CR\", \"PA\"  # Central America / Mesoamerica\n",
    "]\n",
    "\n",
    "\n",
    "gazetteer = set()\n",
    "max_rows = 1000  # max per request (GeoNames limit)\n",
    "\n",
    "print(\"üåç Downloading cities from GeoNames for all of South America...\")\n",
    "\n",
    "for country_code in countries:\n",
    "    loaded = 0\n",
    "    try:\n",
    "        for start_row in range(0, 5000, max_rows):  # Optional paging: up to 5,000 per country\n",
    "            url = \"http://api.geonames.org/searchJSON\"\n",
    "            params = {\n",
    "                \"featureClass\": \"P\",      # populated places\n",
    "                \"country\": country_code,\n",
    "                \"maxRows\": max_rows,\n",
    "                \"startRow\": start_row,\n",
    "                \"orderby\": \"population\",  # most important first\n",
    "                \"username\": geonames_username\n",
    "            }\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            cities = data.get(\"geonames\", [])\n",
    "            if not cities:\n",
    "                break\n",
    "            city_names = [\n",
    "                entry[\"name\"].lower() for entry in cities if \"name\" in entry\n",
    "            ]\n",
    "            gazetteer.update(city_names)\n",
    "            loaded += len(city_names)\n",
    "            time.sleep(1)  # Respect GeoNames rate limits\n",
    "        print(f\"Loaded {loaded} cities from {country_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading cities for {country_code}: {e}\")\n",
    "\n",
    "print(f\"üìå Total unique cities in gazetteer: {len(gazetteer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0aabe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned versions saved\n"
     ]
    }
   ],
   "source": [
    "# Block 7: Apply cleaning and segmentation to raw text\n",
    "light_cleaned = clean_light(raw_text)\n",
    "sentences = segment_sentences(light_cleaned)\n",
    "\n",
    "# Optional tagging for geoparsing\n",
    "sentences_with_tags = [f\"[SENT {i+1}] {s}\" for i, s in enumerate(sentences)]\n",
    "\n",
    "# Heavy-cleaned version for downstream NLP\n",
    "heavy_cleaned = clean_heavy(light_cleaned)\n",
    "\n",
    "# Save outputs\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sentences_with_tags))\n",
    "\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_nlp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(heavy_cleaned)\n",
    "\n",
    "print(\"Cleaned versions saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41623c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 2476 tagged sentences\n"
     ]
    }
   ],
   "source": [
    "# Block 8: Load sentence list with IDs from saved file\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sentence_data = []\n",
    "for line in lines:\n",
    "    match = re.match(r\"\\[SENT (\\d+)\\] (.+)\", line.strip())\n",
    "    if match:\n",
    "        sent_id = int(match.group(1))\n",
    "        sent_text = match.group(2)\n",
    "        sentence_data.append((sent_id, sent_text))\n",
    "\n",
    "print(f\" Loaded {len(sentence_data)} tagged sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70be0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Running ensemble NER + gazetteer matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER + Gazetteer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2476/2476 [41:27<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: outputs/geoparsing_ner_ensemble.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 9: NER + Gazetteer with optional Stanza and progress bar (with short-name filtering)\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "# Toggle Stanza use (set to False to avoid slowdowns)\n",
    "USE_STANZA = False\n",
    "\n",
    "def extract_entities_spacy(text):\n",
    "    doc = nlp_spacy(text)\n",
    "    return [(ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "            for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "\n",
    "def extract_entities_stanza(text):\n",
    "    doc = stanza_pipeline(text)\n",
    "    results = []\n",
    "    for sent in doc.sentences:\n",
    "        for ent in sent.ents:\n",
    "            if ent.type in {\"GPE\", \"LOC\"}:\n",
    "                results.append((ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "    return results\n",
    "\n",
    "def match_gazetteer(text, known_places):\n",
    "    \"\"\"\n",
    "    gazetteer matching using word-boundary regex to reduce false positives.\n",
    "    Matches longer place names first to avoid substring collisions.\n",
    "    filters out short entries (‚â§3 chars) to reduce noise.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    matches = []\n",
    "\n",
    "    # Ignore very short place names (common source of noise)\n",
    "    known_places_sorted = sorted([p for p in known_places if len(p) > 3], key=len, reverse=True)\n",
    "\n",
    "    for place in known_places_sorted:\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(place.lower()))\n",
    "        for m in re.finditer(pattern, text_lower):\n",
    "            start, end = m.start(), m.end()\n",
    "            match_text = text[start:end]\n",
    "            matches.append((match_text, \"GAZETTEER\", start, end))\n",
    "    return matches\n",
    "\n",
    "def combine_ner_gazetteer(sentences, gazetteer):\n",
    "    results = []\n",
    "    for sent_id, text in tqdm(sentences, desc=\"NER + Gazetteer\"):\n",
    "        try:\n",
    "            ents_spacy = extract_entities_spacy(text)\n",
    "            ents_stanza = extract_entities_stanza(text) if USE_STANZA else []\n",
    "            ents_gazetteer = match_gazetteer(text, gazetteer)\n",
    "\n",
    "            all_ents = ents_spacy + ents_stanza + ents_gazetteer\n",
    "            seen = set()\n",
    "            for ent_text, label, start, end in all_ents:\n",
    "                norm = ent_text.lower()\n",
    "                if not any(fuzz.ratio(norm, s) > 90 for s in seen):\n",
    "                    seen.add(norm)\n",
    "                    results.append({\n",
    "                        \"sentence_id\": sent_id,\n",
    "                        \"entity\": ent_text,\n",
    "                        \"entity_norm\": norm,\n",
    "                        \"label\": label,\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end,\n",
    "                        \"sentence\": text\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\" Error in sentence {sent_id}: {e}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"üîé Running ensemble NER + gazetteer matching...\")\n",
    "entities_combined = combine_ner_gazetteer(sentence_data, gazetteer)\n",
    "\n",
    "df_combined = pd.DataFrame(entities_combined)\n",
    "df_combined.to_csv(\"outputs/geoparsing_ner_ensemble.csv\", index=False)\n",
    "print(\" Saved: outputs/geoparsing_ner_ensemble.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       233\n",
      "           1       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           1.00       270\n",
      "   macro avg       1.00      1.00      1.00       270\n",
      "weighted avg       1.00      1.00      1.00       270\n",
      "\n",
      "‚úÖ ML filtering complete\n"
     ]
    }
   ],
   "source": [
    "# Block 10: Train ML model to filter real geographic entities\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load ensemble NER output\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble.csv\")\n",
    "\n",
    "# Create features\n",
    "df[\"label_GPE\"] = (df[\"label\"] == \"GPE\").astype(int)\n",
    "df[\"label_LOC\"] = (df[\"label\"] == \"LOC\").astype(int)\n",
    "\n",
    "symbolic_keywords = [\"freedom\", \"struggle\", \"liberation\", \"future\", \"dream\", \"cause\", \"revolution\", \"hope\", \"people\"]\n",
    "df[\"symbolic_flagged\"] = df[\"sentence\"].str.contains(\"|\".join(symbolic_keywords), flags=re.IGNORECASE, na=False)\n",
    "\n",
    "expected_countries = [\"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\", \"venezuela\"]\n",
    "df[\"country_valid\"] = df[\"sentence\"].str.lower().apply(\n",
    "    lambda x: int(any(country in x for country in expected_countries))\n",
    ")\n",
    "\n",
    "df[\"fuzzy_score\"] = df.apply(\n",
    "    lambda row: fuzz.ratio(str(row[\"entity\"]).lower(), str(row[\"entity_norm\"]).lower()), axis=1\n",
    ")\n",
    "df[\"fuzzy_score_scaled\"] = df[\"fuzzy_score\"] / 100.0\n",
    "\n",
    "df[\"auto_label\"] = ((df[\"country_valid\"] == 1) & (~df[\"symbolic_flagged\"])).astype(int)\n",
    "\n",
    "# Train/test split\n",
    "features = df[[\"label_GPE\", \"label_LOC\", \"symbolic_flagged\", \"country_valid\", \"fuzzy_score_scaled\"]]\n",
    "target = df[\"auto_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, stratify=target, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(\" Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict on all\n",
    "X_all_scaled = scaler.transform(features)\n",
    "df[\"geo_confidence\"] = clf.predict_proba(X_all_scaled)[:, 1]\n",
    "df[\"filtered_out_ml\"] = df[\"geo_confidence\"] < 0.5\n",
    "\n",
    "# Save outputs\n",
    "df_filtered = df[~df[\"filtered_out_ml\"]].copy()\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_flagged_with_ml.csv\", index=False)\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\", index=False)\n",
    "\n",
    "print(\" ML filtering complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not load enriched data: [Errno 2] No such file or directory: 'outputs/geoparsing_final_enriched.csv'\n",
      "‚úÖ Saved: geoparsing_ensemble_final_ml_boosted.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 11: Boost confidence for small towns in South America\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_filtered.csv\")\n",
    "\n",
    "# Define target countries\n",
    "target_countries = [\n",
    "    \"argentina\", \"chile\", \"peru\", \"bolivia\", \"colombia\",\n",
    "    \"venezuela\", \"ecuador\", \"brazil\", \"uruguay\", \"paraguay\"\n",
    "]\n",
    "\n",
    "# Prepare for matching\n",
    "df[\"entity_norm_lower\"] = df[\"entity_norm\"].str.lower()\n",
    "\n",
    "# Try to load population-enriched data\n",
    "try:\n",
    "    enriched = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "    enriched[\"entity_norm_lower\"] = enriched[\"entity_norm\"].str.lower()\n",
    "\n",
    "    df = df.merge(\n",
    "        enriched[[\"entity_norm_lower\", \"country\", \"population\"]],\n",
    "        on=\"entity_norm_lower\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Mark small towns\n",
    "    df[\"boost_small_town\"] = (\n",
    "        df[\"population\"].fillna(0).lt(50000) &\n",
    "        df[\"country\"].str.lower().isin(target_countries)\n",
    "    )\n",
    "    print(f\" Boosted {df['boost_small_town'].sum()} small towns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Could not load enriched data: {e}\")\n",
    "    df[\"boost_small_town\"] = False\n",
    "\n",
    "# Apply confidence boost\n",
    "df[\"geo_confidence_boosted\"] = df[\"geo_confidence\"]\n",
    "df.loc[df[\"boost_small_town\"], \"geo_confidence_boosted\"] = 0.9\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\", index=False)\n",
    "print(\"Saved: geoparsing_ensemble_final_ml_boosted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b88ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ ML-learned vague terms: 0\n",
      "üìö WordNet vague terms: 1786\n",
      "‚ùå Removed 0 entities (ML or WordNet flagged)\n",
      "‚ö†Ô∏è 'country' column not found. Cannot apply region filter.\n",
      "‚úÖ Saved: geoparsing_ner_ensemble_filtered_southamerica.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Block 12: vague term filtering (ML + WordNet) + South America region restriction\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_ensemble_final_ml_boosted.csv\")\n",
    "\n",
    "#  Normalize entity name \n",
    "df[\"entity_norm\"] = df[\"entity_norm\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# === STEP 1: ML-learned vague terms ===\n",
    "term_stats = df.groupby(\"entity_norm\").agg({\n",
    "    \"geo_confidence_boosted\": \"mean\",\n",
    "    \"entity\": \"count\"\n",
    "}).rename(columns={\n",
    "    \"entity\": \"freq\",\n",
    "    \"geo_confidence_boosted\": \"avg_conf\"\n",
    "}).reset_index()\n",
    "\n",
    "learned_vague = term_stats[\n",
    "    (term_stats[\"freq\"] >= 3) &\n",
    "    (term_stats[\"avg_conf\"] < 0.35)\n",
    "][\"entity_norm\"].tolist()\n",
    "print(f\" ML-learned vague terms: {len(learned_vague)}\")\n",
    "\n",
    "# === STEP 2: WordNet vague terms ===\n",
    "location_synsets = [\n",
    "    wn.synset(\"location.n.01\"),\n",
    "    wn.synset(\"region.n.01\"),\n",
    "    wn.synset(\"area.n.01\"),\n",
    "    wn.synset(\"place.n.01\"),\n",
    "    wn.synset(\"territory.n.01\")\n",
    "]\n",
    "\n",
    "vague_terms_wordnet = set()\n",
    "for syn in location_synsets:\n",
    "    for hypo in syn.closure(lambda s: s.hyponyms()):\n",
    "        for lemma in hypo.lemmas():\n",
    "            vague_terms_wordnet.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "print(f\" WordNet vague terms: {len(vague_terms_wordnet)}\")\n",
    "\n",
    "# === STEP 3: Combined vague term filtering ===\n",
    "df[\"is_vague_combined\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"entity_norm\"] in learned_vague or\n",
    "        (row[\"label\"] == \"LOC\" and row[\"entity_norm\"] in vague_terms_wordnet)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_filtered = df[~df[\"is_vague_combined\"]].copy()\n",
    "print(f\" Removed {df['is_vague_combined'].sum()} entities (ML or WordNet flagged)\")\n",
    "\n",
    "# === STEP 4: South America region restriction ===\n",
    "sa_countries = {\n",
    "    \"argentina\", \"bolivia\", \"brazil\", \"chile\", \"colombia\",\n",
    "    \"ecuador\", \"guyana\", \"paraguay\", \"peru\", \"suriname\", \"uruguay\", \"venezuela\"\n",
    "}\n",
    "\n",
    "if \"country\" in df_filtered.columns:\n",
    "    df_filtered[\"country\"] = df_filtered[\"country\"].astype(str).str.lower()\n",
    "    df_filtered = df_filtered[df_filtered[\"country\"].isin(sa_countries)].copy()\n",
    "    print(f\"üåé After SA region filter: {len(df_filtered)} rows\")\n",
    "else:\n",
    "    print(\" 'country' column not found. Cannot apply region filter.\")\n",
    "\n",
    "# === Final Save ===\n",
    "df_filtered.to_csv(\"outputs/geoparsing_ner_ensemble_filtered_southamerica.csv\", index=False)\n",
    "print(\" Saved: geoparsing_ner_ensemble_filtered_southamerica.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('personnel_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_vehicle.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('weapons_carrier.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warplane.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('warship.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('tank.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('half_track.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('bomber.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('fighter.n.02') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('reconnaissance_plane.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('technical.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/alicja/micromamba/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('minivan.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "Scoring entities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:15<00:00, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Saved: outputs/geoparsing_scored_candidates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 13 : Contextual Scoring with enhanced inputs\n",
    "import spacy\n",
    "import re\n",
    "import dateparser.search\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn, framenet as fn\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === Load motion verbs ===\n",
    "motion_frames = ['Motion', 'Travel', 'Self_motion', 'Arriving', 'Departing']\n",
    "motion_verbs = set()\n",
    "for frame in motion_frames:\n",
    "    try:\n",
    "        for lu in fn.frame_by_name(frame).lexUnit.values():\n",
    "            if lu['name'].endswith('.v'):\n",
    "                motion_verbs.add(lu['name'].split('.')[0].lower())\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# === Load transport terms ===\n",
    "vehicle_syn = wn.synset('vehicle.n.01')\n",
    "transport_terms = set()\n",
    "for syn in vehicle_syn.closure(lambda s: s.hyponyms()):\n",
    "    for lemma in syn.lemmas():\n",
    "        transport_terms.add(lemma.name().lower().replace('_', ' '))\n",
    "\n",
    "# === Load filtered file after SA restriction ===\n",
    "df = pd.read_csv(\"outputs/geoparsing_ner_ensemble_filtered_southamerica.csv\")\n",
    "\n",
    "# === Load sentence map ===\n",
    "with open(\"outputs/cleaned_motorcycle_diaries_geoparsing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "sentence_map = {}\n",
    "for line in lines:\n",
    "    if line.strip().startswith(\"[SENT\"):\n",
    "        sid = int(line.split(\"]\")[0].split()[1])\n",
    "        sentence_map[sid] = line.split(\"]\")[1].strip()\n",
    "\n",
    "# === Scoring logic ===\n",
    "scored_rows = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring entities\"):\n",
    "    sid = row[\"sentence_id\"]\n",
    "    entity = row[\"entity\"]\n",
    "    norm = row[\"entity_norm\"]\n",
    "    label = row[\"label\"]\n",
    "    sentence = sentence_map.get(sid, \"\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    score = 0\n",
    "    entity_token = None\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"} and ent.text.lower().strip() == entity.lower().strip():\n",
    "            entity_token = ent.root\n",
    "            break\n",
    "\n",
    "    if not entity_token:\n",
    "        continue\n",
    "\n",
    "    sentence_lower = sentence.lower()\n",
    "    entity_lower = entity.lower()\n",
    "\n",
    "    if any(verb in sentence_lower for verb in motion_verbs):\n",
    "        score += 1\n",
    "    if any(term in sentence_lower for term in transport_terms):\n",
    "        score += 1\n",
    "    if re.search(r'\\b(in|to|at)\\s+' + re.escape(entity_lower) + r'\\b', sentence_lower):\n",
    "        score += 1\n",
    "    if re.search(r\"\\b\" + re.escape(entity_lower) + r\"['‚Äô]s\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bof\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if re.search(r\"\\bfor\\s+\" + re.escape(entity_lower) + r\"\\b\", sentence_lower):\n",
    "        score -= 1\n",
    "    if dateparser.search.search_dates(sentence):\n",
    "        score += 1\n",
    "\n",
    "    scored_rows.append({\n",
    "        \"sentence_id\": sid,\n",
    "        \"entity\": entity,\n",
    "        \"entity_norm\": norm,\n",
    "        \"label\": label,\n",
    "        \"sentence\": sentence,\n",
    "        \"score\": score,\n",
    "        \"latitude\": row.get(\"latitude\"),\n",
    "        \"longitude\": row.get(\"longitude\"),\n",
    "        \"country\": row.get(\"country\")\n",
    "    })\n",
    "\n",
    "scored_df = pd.DataFrame(scored_rows)\n",
    "scored_df = scored_df.sort_values(by=[\"score\", \"sentence_id\"], ascending=[False, True])\n",
    "scored_df.to_csv(\"outputs/geoparsing_scored_candidates.csv\", index=False)\n",
    "print(\"üìÑ Saved: outputs/geoparsing_scored_candidates.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ea256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Querying GeoNames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GeoNames queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [04:28<00:00,  6.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Querying Wikidata for missing coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wikidata queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:06<00:00,  6.15it/s]\n",
      "/var/folders/62/b5nr017s2w11vxfg0s2020wh0000gn/T/ipykernel_40370/1097318173.py:69: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"wikidata_lat\"])\n",
      "/var/folders/62/b5nr017s2w11vxfg0s2020wh0000gn/T/ipykernel_40370/1097318173.py:70: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"wikidata_lon\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß≠ Querying OpenStreetMap for unresolved locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OSM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:41<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 14: Enrich entities with coordinates via GeoNames ‚Üí Wikidata ‚Üí OSM\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load scored candidates ===\n",
    "df = pd.read_csv(\"outputs/geoparsing_scored_candidates.csv\")\n",
    "places = df[\"entity_norm\"].dropna().unique()\n",
    "geonames_username = \"alicjab\"\n",
    "\n",
    "geo_data = {}\n",
    "\n",
    "# === Step 1: GeoNames ===\n",
    "print(\"üåç Querying GeoNames...\")\n",
    "for place in tqdm(places, desc=\"GeoNames queries\"):\n",
    "    try:\n",
    "        params = {\"q\": place, \"maxRows\": 1, \"username\": geonames_username}\n",
    "        r = requests.get(\"http://api.geonames.org/searchJSON\", params=params, timeout=10).json()\n",
    "        if not r.get(\"geonames\"):\n",
    "            raise ValueError(\"No result\")\n",
    "        g = r[\"geonames\"][0]\n",
    "        geo_data[place] = {\n",
    "            \"latitude\": float(g[\"lat\"]),\n",
    "            \"longitude\": float(g[\"lng\"]),\n",
    "            \"country\": g.get(\"countryName\"),\n",
    "            \"population\": int(g.get(\"population\", 0))\n",
    "        }\n",
    "    except Exception:\n",
    "        geo_data[place] = {\"latitude\": None, \"longitude\": None, \"country\": None, \"population\": None}\n",
    "    time.sleep(1)  # GeoNames rate limit\n",
    "\n",
    "geo_df = pd.DataFrame.from_dict(geo_data, orient=\"index\")\n",
    "geo_df.index.name = \"entity_norm\"\n",
    "geo_df.reset_index(inplace=True)\n",
    "\n",
    "# === Merge GeoNames results ===\n",
    "df_enriched = df.merge(geo_df, on=\"entity_norm\", how=\"left\")\n",
    "\n",
    "# === Step 2: Wikidata fallback ===\n",
    "print(\"üîÅ Querying Wikidata for missing coordinates...\")\n",
    "if \"latitude\" not in df_enriched.columns:\n",
    "    df_enriched[\"latitude\"] = None\n",
    "    df_enriched[\"longitude\"] = None\n",
    "\n",
    "missing_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_wikidata_coords(place):\n",
    "    try:\n",
    "        search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"search\": place,\n",
    "            \"language\": \"en\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        r = requests.get(search_url, params=search_params, timeout=10).json()\n",
    "        if not r[\"search\"]:\n",
    "            return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "        qid = r[\"search\"][0][\"id\"]\n",
    "        entity = requests.get(f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\").json()\n",
    "        coords = entity[\"entities\"][qid][\"claims\"].get(\"P625\", [{}])[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {})\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": coords.get(\"latitude\"), \"wikidata_lon\": coords.get(\"longitude\")}\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"wikidata_lat\": None, \"wikidata_lon\": None}\n",
    "\n",
    "wikidata_results = pd.DataFrame([query_wikidata_coords(p) for p in tqdm(missing_places, desc=\"Wikidata queries\")])\n",
    "df_enriched = df_enriched.merge(wikidata_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"wikidata_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"wikidata_lon\"])\n",
    "\n",
    "# === Step 3: OSM fallback ===\n",
    "print(\"üß≠ Querying OpenStreetMap for unresolved locations...\")\n",
    "nominatim_places = df_enriched[df_enriched[\"latitude\"].isna()][\"entity_norm\"].dropna().unique()\n",
    "\n",
    "def query_osm(place):\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://nominatim.openstreetmap.org/search\",\n",
    "            params={\"q\": place, \"format\": \"json\", \"limit\": 1},\n",
    "            headers={\"User-Agent\": \"Geoparser/1.0\"},\n",
    "            timeout=10\n",
    "        ).json()\n",
    "        if not r:\n",
    "            return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "        return {\n",
    "            \"entity_norm\": place,\n",
    "            \"osm_lat\": float(r[0][\"lat\"]),\n",
    "            \"osm_lon\": float(r[0][\"lon\"])\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"entity_norm\": place, \"osm_lat\": None, \"osm_lon\": None}\n",
    "\n",
    "osm_results = pd.DataFrame([query_osm(p) for p in tqdm(nominatim_places, desc=\"OSM queries\")])\n",
    "df_enriched = df_enriched.merge(osm_results, on=\"entity_norm\", how=\"left\")\n",
    "df_enriched[\"latitude\"] = df_enriched[\"latitude\"].combine_first(df_enriched[\"osm_lat\"])\n",
    "df_enriched[\"longitude\"] = df_enriched[\"longitude\"].combine_first(df_enriched[\"osm_lon\"])\n",
    "\n",
    "\n",
    "df_enriched.to_csv(\"outputs/geoparsing_final_enriched.csv\", index=False)\n",
    "print(\"‚úÖ Final enriched file saved: geoparsing_final_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚Ü©Ô∏è Reweighting outliers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1387.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß≠ Final post-reweighting size: 108 rows\n",
      "üåé Filtering by continent...\n",
      "Loading formatted geocoded file...\n",
      "üåç Removed 18 non-South American entries\n",
      "‚úÖ Saved final cleaned + reweighted + region-filtered version: geoparsing_final_scored_clustered.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 15: Geographic Outlier Filtering using DBSCAN (corrected input + centroid-aware reweighting + continent filtering)\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_enriched.csv\")\n",
    "\n",
    "# Drop rows without coordinates\n",
    "df_geo = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "coords_rad = np.radians(df_geo[[\"latitude\", \"longitude\"]].values)\n",
    "\n",
    "# Cluster with DBSCAN using haversine distance (in radians)\n",
    "clustering = DBSCAN(eps=0.5, min_samples=2, metric='haversine')\n",
    "df_geo[\"geo_cluster\"] = clustering.fit_predict(coords_rad)\n",
    "\n",
    "# === Compute centroids of each valid cluster ===\n",
    "centroids = (\n",
    "    df_geo[df_geo[\"geo_cluster\"] != -1]\n",
    "    .groupby(\"geo_cluster\")[[\"latitude\", \"longitude\"]]\n",
    "    .mean()\n",
    "    .to_dict(\"index\")\n",
    ")\n",
    "\n",
    "# === Reweight scores for outliers ===\n",
    "outliers = df_geo[df_geo[\"geo_cluster\"] == -1].copy()\n",
    "non_outliers = df_geo[df_geo[\"geo_cluster\"] != -1].copy()\n",
    "\n",
    "reweighted = []\n",
    "for _, row in tqdm(outliers.iterrows(), total=len(outliers), desc=\"‚Ü©Ô∏è Reweighting outliers\"):\n",
    "    min_dist_km = float(\"inf\")\n",
    "    entity_point = (row[\"latitude\"], row[\"longitude\"])\n",
    "\n",
    "    for c in centroids.values():\n",
    "        centroid_point = (c[\"latitude\"], c[\"longitude\"])\n",
    "        dist = geodesic(entity_point, centroid_point).km\n",
    "        if dist < min_dist_km:\n",
    "            min_dist_km = dist\n",
    "\n",
    "    # If outlier is <500 km from any cluster, keep it with downgraded score\n",
    "    if min_dist_km < 500:\n",
    "        row[\"geo_cluster\"] = -2  # kept but marked as downgraded outlier\n",
    "        row[\"geo_score_adjusted\"] = row.get(\"score\", 0) - 1\n",
    "        reweighted.append(row)\n",
    "\n",
    "# Combine cleaned + downgraded outliers\n",
    "final_df = pd.concat([non_outliers, pd.DataFrame(reweighted)], ignore_index=True)\n",
    "print(f\" Final post-reweighting size: {len(final_df)} rows\")\n",
    "\n",
    "# === Filter by continent (keep only South America) ===\n",
    "def get_continent(lat, lon):\n",
    "    try:\n",
    "        results = rg.search((lat, lon), mode=1)\n",
    "        cc = results[0]['cc']\n",
    "        # ISO country codes in South America\n",
    "        south_america = {\n",
    "            'AR', 'BO', 'BR', 'CL', 'CO', 'EC', 'GY', 'PY', 'PE', 'SR', 'UY', 'VE'\n",
    "        }\n",
    "        return 'South America' if cc in south_america else 'Other'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "print(\"üåé Filtering by continent...\")\n",
    "final_df[\"continent\"] = final_df.apply(\n",
    "    lambda row: get_continent(row[\"latitude\"], row[\"longitude\"]), axis=1\n",
    ")\n",
    "\n",
    "before_filter = len(final_df)\n",
    "final_df = final_df[final_df[\"continent\"] == \"South America\"].copy()\n",
    "print(f\"üåç Removed {before_filter - len(final_df)} non-South American entries\")\n",
    "\n",
    "# Save final output\n",
    "final_df.to_csv(\"outputs/geoparsing_final_scored_clustered.csv\", index=False)\n",
    "print(\"Saved final cleaned + reweighted + region-filtered version: geoparsing_final_scored_clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e8424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "geo": "geo",
         "hovertemplate": "<b>%{hovertext}</b><br><br>entity=%{text}<br>latitude=%{lat}<br>longitude=%{lon}<br>cluster=%{marker.color}<extra></extra>",
         "hovertext": [
          "Antofagasta (cluster 0)",
          "Chile (cluster 0)",
          "Bogot√° (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Peru (cluster 0)",
          "Venezuela (cluster 0)",
          "Venezuela (cluster 0)",
          "Bogot√° (cluster 0)",
          "Colombia (cluster 0)",
          "Caracas (cluster 0)",
          "Chile (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Chile (cluster 0)",
          "San Carlos de Bariloche (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Peru (cluster 0)",
          "Chile (cluster 0)",
          "Iquique (cluster 0)",
          "Arica (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Santiago de Chile (cluster 0)",
          "Puno (cluster 0)",
          "Lima (cluster 0)",
          "Peru (cluster 0)",
          "peru (cluster 0)",
          "Argentina (cluster 0)",
          "Manaos (cluster 0)",
          "Peru (cluster 0)",
          "Bogot√° (cluster 0)",
          "Colombia (cluster 0)",
          "Venezuela (cluster 0)",
          "Colombia (cluster 0)",
          "Baquedano (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Valpara√≠so (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "Arica (cluster 0)",
          "Peru (cluster 0)",
          "Chile (cluster 0)",
          "Chuquicamata (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Andes (cluster 0)",
          "Chile (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Tarat√° (cluster 0)",
          "Peru (cluster 0)",
          "Pucallpa (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Lima (cluster 0)",
          "Peru (cluster 0)",
          "Peru (cluster 0)",
          "Lima (cluster 0)",
          "Chile (cluster 0)",
          "Peru (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)",
          "South America (cluster 0)",
          "Bolivia (cluster 0)",
          "Suqu√≠a (cluster 0)",
          "Peru (cluster 0)",
          "Quechua (cluster 0)",
          "Colombia (cluster 0)",
          "Colombia (cluster 0)",
          "Argentina (cluster 0)",
          "Argentina (cluster 0)",
          "Chile (cluster 0)",
          "Chile (cluster 0)"
         ],
         "lat": {
          "bdata": "i0YCvnmlN8BIwOjy5sI/wNTntLgQnRJA4YrVMI1/QcDhitUwjX9BwEjA6PLmwj/ASMDo8ubCP8DhitUwjX9BwDILSFbZehvA0KD8OPUAIEDQoPw49QAgQNTntLgQnRJA+kFdpFBmEEBhcZOsHgMlQEjA6PLmwj/AMgtIVtl6G8AyC0hW2XobwEjA6PLmwj/AuR11zxWRRMBIwOjy5sI/wOGK1TCNf0HAMgtIVtl6G8BIwOjy5sI/wAKyfALNNjTAsjEK3YB6MsAyC0hW2XobwOGK1TCNf0HASMDo8ubCP8BIwOjy5sI/wOGK1TCNf0HAT37mBwm4QMAAAAAAAAAuwPG8VGzMHyjAMgtIVtl6G8AyC0hW2XobwOGK1TCNf0HANoR0wpUNCcAyC0hW2XobwNTntLgQnRJA+kFdpFBmEEDQoPw49QAgQPpBXaRQZhBA4A55cBzJRsDhitUwjX9BwEjA6PLmwj/AL1XERN6FQMBIwOjy5sI/wEjA6PLmwj/ASMDo8ubCP8CyMQrdgHoywDILSFbZehvASMDo8ubCP8BeGnVjMFE2wEjA6PLmwj/A4YrVMI1/QcAFygEWVJ8WQEjA6PLmwj/A4YrVMI1/QcDhitUwjX9BwOGK1TCNf0HA4YrVMI1/QcBIwOjy5sI/wC7L12X4ZTHAMgtIVtl6G8DMUvwpn8MgwDILSFbZehvAMgtIVtl6G8AyC0hW2XobwOGK1TCNf0HA8bxUbMwfKMAyC0hW2XobwDILSFbZehvA8bxUbMwfKMBIwOjy5sI/wDILSFbZehvA4YrVMI1/QcDhitUwjX9BwEjA6PLmwj/ASMDo8ubCP8CxX8FHDgA1wPgikAGPDjHAG9O43AVjP8AyC0hW2XobwDjUlAlkZzvA+kFdpFBmEED6QV2kUGYQQOGK1TCNf0HA4YrVMI1/QcBIwOjy5sI/wEjA6PLmwj/A",
          "dtype": "f8"
         },
         "legendgroup": "",
         "lon": {
          "bdata": "i0YCvnmZUcArHQe5ZtRRwHP3OT5ahVLAPOGA8ec9UMA84YDx5z1QwCsdB7lm1FHAKx0HuWbUUcA84YDx5z1QwD4+ITvvwlLAiEWxgRmHUMCIRbGBGYdQwHP3OT5ahVLA32pC/yk6UsBPOcvRiLpQwCsdB7lm1FHAPj4hO+/CUsA+PiE778JSwCsdB7lm1FHAIPx6dNnTUcArHQe5ZtRRwDzhgPHnPVDAPj4hO+/CUsArHQe5ZtRRwA6z4frBiVHAQR1FjI2UUcA+PiE778JSwDzhgPHnPVDAKx0HuWbUUcArHQe5ZtRRwDzhgPHnPVDATivg+aCpUcAAAAAAAIBRwOBdeG9WQlPAPj4hO+/CUsA+PiE778JSwDzhgPHnPVDAzNfAscL9TcA+PiE778JSwHP3OT5ahVLA32pC/yk6UsCIRbGBGYdQwN9qQv8pOlLAV65cFGIEUsA84YDx5z1QwCsdB7lm1FHAvGnpwKjnUcArHQe5ZtRRwCsdB7lm1FHAKx0HuWbUUcBBHUWMjZRRwD4+ITvvwlLAKx0HuWbUUcCh7NIrijtRwCsdB7lm1FHAPOGA8ec9UMCtvfSbLvhSwCsdB7lm1FHAPOGA8ec9UMA84YDx5z1QwDzhgPHnPVDAPOGA8ec9UMArHQe5ZtRRwH5S7dNxf1HAPj4hO+/CUsCdTDfkeqJSwD4+ITvvwlLAPj4hO+/CUsA+PiE778JSwDzhgPHnPVDA4F14b1ZCU8A+PiE778JSwD4+ITvvwlLA4F14b1ZCU8ArHQe5ZtRRwD4+ITvvwlLAPOGA8ec9UMA84YDx5z1QwCsdB7lm1FHAKx0HuWbUUcDRAx+DFYBOwAj3FEpwP1DAq76p/ncMUMA+PiE778JSwD0q/u8IkVHA32pC/yk6UsDfakL/KTpSwDzhgPHnPVDAPOGA8ec9UMArHQe5ZtRRwCsdB7lm1FHA",
          "dtype": "f8"
         },
         "marker": {
          "color": {
           "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "",
         "showlegend": false,
         "text": [
          "Antofagasta",
          "Chile",
          "Bogot√°",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile",
          "Argentina",
          "Peru",
          "Venezuela",
          "Venezuela",
          "Bogot√°",
          "Colombia",
          "Caracas",
          "Chile",
          "Peru",
          "Peru",
          "Chile",
          "San Carlos de Bariloche",
          "Chile",
          "Argentina",
          "Peru",
          "Chile",
          "Iquique",
          "Arica",
          "Peru",
          "Argentina",
          "Chile",
          "Chile",
          "Argentina",
          "Santiago de Chile",
          "Puno",
          "Lima",
          "Peru",
          "peru",
          "Argentina",
          "Manaos",
          "Peru",
          "Bogot√°",
          "Colombia",
          "Venezuela",
          "Colombia",
          "Baquedano",
          "Argentina",
          "Chile",
          "Valpara√≠so",
          "Chile",
          "Chile",
          "Chile",
          "Arica",
          "Peru",
          "Chile",
          "Chuquicamata",
          "Chile",
          "Argentina",
          "Andes",
          "Chile",
          "Argentina",
          "Argentina",
          "Argentina",
          "Argentina",
          "Chile",
          "Tarat√°",
          "Peru",
          "Pucallpa",
          "Peru",
          "Peru",
          "Peru",
          "Argentina",
          "Lima",
          "Peru",
          "Peru",
          "Lima",
          "Chile",
          "Peru",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile",
          "South America",
          "Bolivia",
          "Suqu√≠a",
          "Peru",
          "Quechua",
          "Colombia",
          "Colombia",
          "Argentina",
          "Argentina",
          "Chile",
          "Chile"
         ],
         "type": "scattergeo"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "geo": {
         "center": {},
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "projection": {
          "type": "natural earth"
         }
        },
        "legend": {
         "title": {
          "text": "Cluster ID"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "üó∫Ô∏è Clustered Location Mentions from Text"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Block 16: Visualize clustered results using Plotly\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load clustered data\n",
    "df = pd.read_csv(\"outputs/geoparsing_final_scored_clustered.csv\")\n",
    "\n",
    "# Drop NaNs (should already be clean)\n",
    "df = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# Rename cluster column for consistency with Plotly\n",
    "df[\"cluster\"] = df[\"geo_cluster\"]\n",
    "\n",
    "# Create hover label\n",
    "df[\"hover\"] = df[\"entity\"] + \" (cluster \" + df[\"cluster\"].astype(str) + \")\"\n",
    "\n",
    "# Basic scatter geo map\n",
    "fig = px.scatter_geo(\n",
    "    df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    text=\"entity\",\n",
    "    hover_name=\"hover\",\n",
    "    color=\"cluster\",\n",
    "    title=\"Clustered Location Mentions from Text\",\n",
    "    projection=\"natural earth\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.update_layout(legend_title_text='Cluster ID')\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
